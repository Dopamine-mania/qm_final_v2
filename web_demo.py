#!/usr/bin/env python3
"""
ã€Šå¿ƒå¢ƒæµè½¬ã€‹Webæ¼”ç¤ºç•Œé¢
ä½¿ç”¨Gradioåˆ›å»ºç®€å•çš„ç½‘é¡µç•Œé¢
"""

import gradio as gr
import numpy as np
import json
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
import time
import os

# å¯¼å…¥æ ¸å¿ƒåŠŸèƒ½
try:
    from mood_flow_app import MoodFlowApp, TherapySession
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("æ­£åœ¨å°è¯•åŠ¨æ€å¯¼å…¥...")
    import importlib.util
    spec = importlib.util.spec_from_file_location("mood_flow_app", "mood_flow_app.py")
    mood_flow_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(mood_flow_module)
    MoodFlowApp = mood_flow_module.MoodFlowApp
    TherapySession = mood_flow_module.TherapySession

class WebDemo:
    def __init__(self):
        self.app = MoodFlowApp()
        self.current_session = None
        # è®¾ç½®matplotlibå­—ä½“ä¸ºè‹±æ–‡ï¼Œé¿å…ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
        plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False
        
    def process_voice_input(self, audio_file):
        """å¤„ç†è¯­éŸ³è¾“å…¥è½¬æ¢ä¸ºæ–‡å­—"""
        if audio_file is None:
            return "", "è¯·å½•åˆ¶æˆ–ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶"
        
        try:
            # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶å¤§å°å’Œæ ¼å¼
            file_path = Path(audio_file)
            if not file_path.exists() or file_path.stat().st_size == 0:
                return "", "âŒ éŸ³é¢‘æ–‡ä»¶æ— æ•ˆæˆ–ä¸ºç©º"
            
            # ç®€åŒ–ç‰ˆè¯­éŸ³å¤„ç†ï¼šç›®å‰ä½¿ç”¨ç¤ºä¾‹æ–‡æœ¬
            # åœ¨å®é™…éƒ¨ç½²æ—¶å¯ä»¥é›†æˆä¸“ä¸šçš„è¯­éŸ³è¯†åˆ«æœåŠ¡
            sample_texts = [
                "ä»Šå¤©å·¥ä½œå‹åŠ›å¾ˆå¤§ï¼Œèººåœ¨åºŠä¸Šç¿»æ¥è¦†å»ç¡ä¸ç€ï¼Œæ€»æ˜¯æƒ³ç€æ˜å¤©çš„ä¼šè®®",
                "æœ€è¿‘æ€»æ˜¯æ„Ÿåˆ°ç„¦è™‘ï¼Œæ™šä¸Šå¾ˆéš¾å…¥ç¡ï¼Œå³ä½¿ç¡ç€äº†ä¹Ÿå®¹æ˜“é†’",
                "å¿ƒæƒ…æœ‰äº›ä½è½ï¼Œæ„Ÿè§‰å¾ˆç–²æƒ«ä½†å°±æ˜¯ç¡ä¸ç€ï¼Œå¯¹ä»€ä¹ˆéƒ½æä¸èµ·å…´è¶£",
                "æœ‰ç‚¹å…´å¥‹ç¡ä¸ç€ï¼Œè„‘å­é‡Œæƒ³ç€å¾ˆå¤šäº‹æƒ…ï¼Œè¶Šæƒ³è¶Šæ¸…é†’",
                "èº«å¿ƒä¿±ç–²ï¼Œä½†èººä¸‹åå¤§è„‘è¿˜æ˜¯å¾ˆæ´»è·ƒï¼Œæ€»æ˜¯èƒ¡æ€ä¹±æƒ³"
            ]
            
            import random
            import hashlib
            
            # åŸºäºæ–‡ä»¶å†…å®¹ç”Ÿæˆä¸€è‡´çš„ç¤ºä¾‹æ–‡æœ¬
            with open(audio_file, 'rb') as f:
                file_hash = hashlib.md5(f.read()).hexdigest()
            
            # ä½¿ç”¨æ–‡ä»¶å“ˆå¸Œé€‰æ‹©ç¤ºä¾‹æ–‡æœ¬ï¼Œç¡®ä¿åŒä¸€æ–‡ä»¶è¿”å›ç›¸åŒç»“æœ
            text_index = int(file_hash[:8], 16) % len(sample_texts)
            selected_text = sample_texts[text_index]
            
            return selected_text, f"ğŸ¤ è¯­éŸ³å·²å¤„ç† (æ¼”ç¤ºæ¨¡å¼): {selected_text}"
                
        except Exception as e:
            return "", f"âŒ è¯­éŸ³å¤„ç†å‡ºé”™: {str(e)}"
    
    def process_input(self, text_input, audio_input, emotion_type=None, demo_mode=True, playback_mode="audio_only", progress=gr.Progress()):
        """å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶ç”Ÿæˆæ²»ç–—æ–¹æ¡ˆ"""
        # å¤„ç†è¯­éŸ³è¾“å…¥
        voice_text = ""
        voice_status = ""
        if audio_input is not None:
            voice_text, voice_status = self.process_voice_input(audio_input)
            if voice_text:
                text_input = voice_text if not text_input else f"{text_input} {voice_text}"
        
        if not text_input and emotion_type:
            # å¦‚æœæ²¡æœ‰æ–‡å­—è¾“å…¥ï¼Œä½¿ç”¨é¢„è®¾æƒ…ç»ª
            emotion_templates = {
                "ç„¦è™‘": "æˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œå¿ƒé‡Œæ€»æ˜¯ä¸å®‰ï¼Œæ‹…å¿ƒå¾ˆå¤šäº‹æƒ…",
                "å‹åŠ›": "å·¥ä½œå‹åŠ›å¤ªå¤§äº†ï¼Œæ„Ÿè§‰å–˜ä¸è¿‡æ°”æ¥",
                "å¤±çœ ": "èººåœ¨åºŠä¸Šç¿»æ¥è¦†å»å°±æ˜¯ç¡ä¸ç€",
                "æŠ‘éƒ": "å¿ƒæƒ…å¾ˆä½è½ï¼Œä»€ä¹ˆéƒ½ä¸æƒ³åš",
                "ç–²æƒ«": "èº«å¿ƒä¿±ç–²ï¼Œä½†å°±æ˜¯æ— æ³•å…¥ç¡"
            }
            text_input = emotion_templates.get(emotion_type, "æˆ‘ç¡ä¸ç€")
        
        if not text_input:
            error_msg = "è¯·é€šè¿‡ä»¥ä¸‹ä»»ä¸€æ–¹å¼æä¾›è¾“å…¥:\nâ€¢ åœ¨æ–‡å­—æ¡†ä¸­æè¿°æ‚¨çš„æ„Ÿå—\nâ€¢ å½•åˆ¶è¯­éŸ³æè¿°\nâ€¢ é€‰æ‹©é¢„è®¾æƒ…ç»ªç±»å‹"
            if voice_status:
                error_msg = f"{voice_status}\n\n{error_msg}"
            return None, None, None, None, error_msg
        
        try:
            # æ ¹æ®æ¨¡å¼è®¾ç½®æ—¶é•¿
            duration = 5 if demo_mode else 20
            progress(0.1, desc="Starting emotion analysis...")
            
            # æ ¹æ®æ’­æ”¾æ¨¡å¼å†³å®šæ˜¯å¦ç”Ÿæˆå®Œæ•´è§†é¢‘
            create_videos = (playback_mode == "ğŸµ+ğŸ¬ éŸ³ç”»ç»“åˆ")
            
            # è¿è¡Œæ²»ç–—ä¼šè¯
            session = self.app.run_therapy_session(text_input, duration=duration, create_full_videos=create_videos, progress_callback=progress)
            self.current_session = session
            
            progress(0.9, desc="Finalizing outputs...")
            
            # è¯»å–ç”Ÿæˆçš„æ–‡ä»¶
            report_image = session.music_file.replace("_therapy_music.wav", "_report.png")
            
            # æ ¹æ®æ’­æ”¾æ¨¡å¼å¤„ç†è¾“å‡º
            if playback_mode == "ğŸµ ä»…éŸ³ä¹":
                video_output = None
                combined_output = session.music_file
            else:  # ğŸµ+ğŸ¬ éŸ³ç”»ç»“åˆ
                # åˆ›å»ºéŸ³è§†é¢‘ç»“åˆç‰ˆæœ¬ (å½“å‰æ˜¾ç¤ºé¢„è§ˆå›¾)
                combined_output = session.music_file
                video_output = session.video_files[0] if session.video_files else None
            
            # ç”ŸæˆçŠ¶æ€ä¿¡æ¯
            mode_text = "Demo (5 min)" if demo_mode else "Full (20 min)"
            playback_text = "Audio Only" if playback_mode == "ğŸµ ä»…éŸ³ä¹" else "Audio + Video"
            status_parts = [f"âœ… Therapy plan generated! ({mode_text}, {playback_text})"]
            if voice_status:
                status_parts.append(f"\nğŸ¤ {voice_status}")
            
            status = f"""
{status_parts[0]}{status_parts[1] if len(status_parts) > 1 else ''}

ğŸ“Š Detected Emotion:
â€¢ Valence: {session.detected_emotion.valence:.2f}
â€¢ Arousal: {session.detected_emotion.arousal:.2f}

ğŸµ Music Therapy:
â€¢ Total Duration: {sum(s['duration'] for s in session.iso_stages)} minutes
â€¢ 3 Stages: {' â†’ '.join(s['stage'].value for s in session.iso_stages)}

ğŸ’¡ Usage Guide:
1. Find a quiet and comfortable environment
2. Dim the lights and relax your body
3. Put on headphones to listen to the music
4. Follow the visual guidance to adjust breathing

ğŸ“ Generated Files:
â€¢ ğŸµ Music: {Path(session.music_file).name}
â€¢ ğŸ¬ Videos: {len(session.video_files)} stage previews
"""
            
            progress(1.0, desc="Complete!")
            return combined_output, video_output, report_image, self.create_simple_visualization(), status
            
        except Exception as e:
            error_msg = f"âŒ å¤„ç†å‡ºé”™: {str(e)}"
            if voice_status:
                error_msg = f"{voice_status}\n\n{error_msg}"
            return None, None, None, None, error_msg
    
    def create_simple_visualization(self):
        """åˆ›å»ºç®€å•çš„å¯è§†åŒ–"""
        if not self.current_session:
            return None
        
        fig, ax = plt.subplots(figsize=(8, 6))
        
        # ç»˜åˆ¶æƒ…ç»ªè½¨è¿¹
        stages = ["å¼€å§‹"] + [s['stage'].value for s in self.current_session.iso_stages]
        valences = [self.current_session.detected_emotion.valence] + \
                   [s['emotion'].valence for s in self.current_session.iso_stages]
        arousals = [self.current_session.detected_emotion.arousal] + \
                   [s['emotion'].arousal for s in self.current_session.iso_stages]
        
        # åˆ›å»ºæ—¶é—´è½´
        times = [0]
        current_time = 0
        for stage in self.current_session.iso_stages:
            current_time += stage['duration']
            times.append(current_time)
        
        # ç»˜åˆ¶æ›²çº¿
        ax.plot(times, valences, 'b-o', linewidth=2, markersize=8, label='Valence')
        ax.plot(times, arousals, 'r-o', linewidth=2, markersize=8, label='Arousal')
        
        # æ·»åŠ é˜¶æ®µæ ‡æ³¨
        for i, (t, stage) in enumerate(zip(times[1:], stages[1:])):
            ax.axvline(x=t, color='gray', linestyle='--', alpha=0.3)
            # å°†ä¸­æ–‡é˜¶æ®µåè½¬æ¢ä¸ºè‹±æ–‡
            stage_en = stage
            if 'åŒæ­¥åŒ–' in stage:
                stage_en = 'Sync'
            elif 'å¼•å¯¼åŒ–' in stage:
                stage_en = 'Guide'
            elif 'å·©å›ºåŒ–' in stage:
                stage_en = 'Consolidate'
            ax.text(t-1, 0.9, stage_en, rotation=45, fontsize=10)
        
        ax.set_xlabel('Time (minutes)')
        ax.set_ylabel('Emotion Value')
        ax.set_title('ISO 3-Stage Emotion Guidance Trajectory')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_ylim(-1, 1)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        viz_path = Path("outputs/demo_sessions") / "current_visualization.png"
        plt.savefig(viz_path)
        plt.close()
        
        return str(viz_path)

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    demo = WebDemo()
    
    with gr.Blocks(title="å¿ƒå¢ƒæµè½¬ - ç¡çœ æ²»ç–—ç³»ç»Ÿ", theme=gr.themes.Soft()) as interface:
        gr.Markdown("""
        # ğŸŒ™ ã€Šå¿ƒå¢ƒæµè½¬ã€‹AIç¡çœ æ²»ç–—ç³»ç»Ÿ
        
        åŸºäºISOä¸‰é˜¶æ®µåŸåˆ™å’Œæƒ…ç»ªè¯†åˆ«æŠ€æœ¯ï¼Œä¸ºæ‚¨ç”Ÿæˆä¸ªæ€§åŒ–çš„éŸ³è§†é¢‘ç¡çœ æ²»ç–—æ–¹æ¡ˆã€‚
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“ è¾“å…¥æ‚¨çš„æ„Ÿå—")
                
                # é€‰é¡¹å¡ï¼šæ–‡å­—æˆ–è¯­éŸ³è¾“å…¥
                with gr.Tabs():
                    with gr.TabItem("ğŸ’¬ æ–‡å­—è¾“å…¥"):
                        text_input = gr.Textbox(
                            label="æè¿°æ‚¨çš„æ„Ÿå—",
                            placeholder="ä¾‹å¦‚ï¼šä»Šå¤©å·¥ä½œå‹åŠ›å¾ˆå¤§ï¼Œèººåœ¨åºŠä¸Šç¿»æ¥è¦†å»ç¡ä¸ç€...",
                            lines=4
                        )
                    
                    with gr.TabItem("ğŸ¤ è¯­éŸ³è¾“å…¥"):
                        audio_input = gr.Audio(
                            label="å½•åˆ¶æˆ–ä¸Šä¼ è¯­éŸ³",
                            type="filepath",
                            format="wav"
                        )
                        gr.Markdown("ğŸ’¡ **æç¤º**: å½•åˆ¶åè¯·ç‚¹å‡»'ç”Ÿæˆæ²»ç–—æ–¹æ¡ˆ'æŒ‰é’®")
                
                gr.Markdown("### ğŸ¯ æˆ–å¿«é€Ÿé€‰æ‹©æƒ…ç»ª:")
                emotion_buttons = gr.Radio(
                    choices=["ç„¦è™‘", "å‹åŠ›", "å¤±çœ ", "æŠ‘éƒ", "ç–²æƒ«"],
                    label="é¢„è®¾æƒ…ç»ª",
                    value=None
                )
                
                gr.Markdown("### âš™ï¸ ç³»ç»Ÿè®¾ç½®:")
                with gr.Row():
                    demo_mode_toggle = gr.Checkbox(
                        label="æ¼”ç¤ºæ¨¡å¼ (5åˆ†é’Ÿå¿«é€Ÿä½“éªŒ)",
                        value=True
                    )
                    
                playback_mode = gr.Radio(
                    choices=["ğŸµ ä»…éŸ³ä¹", "ğŸµ+ğŸ¬ éŸ³ç”»ç»“åˆ"],
                    label="æ’­æ”¾æ¨¡å¼",
                    value="ğŸµ ä»…éŸ³ä¹"
                )
                
                submit_btn = gr.Button("ğŸš€ ç”Ÿæˆæ²»ç–—æ–¹æ¡ˆ", variant="primary", size="lg")
                
                gr.Markdown("""
                ### ğŸ’¡ ä½¿ç”¨æŒ‡å—
                1. **è¾“å…¥æ–¹å¼**ï¼šé€‰æ‹©æ–‡å­—æˆ–è¯­éŸ³æè¿°
                2. **ç”Ÿæˆæ–¹æ¡ˆ**ï¼šç‚¹å‡»æŒ‰é’®å¼€å§‹å¤„ç†
                3. **è†å¬éŸ³ä¹**ï¼šæ’­æ”¾ä¸ªæ€§åŒ–æ²»ç–—éŸ³ä¹
                4. **è§‚çœ‹è§†é¢‘**ï¼šæŸ¥çœ‹é…å¥—çš„è§†è§‰å¼•å¯¼
                5. **æŸ¥çœ‹æŠ¥å‘Š**ï¼šäº†è§£æƒ…ç»ªåˆ†æå’Œæ²»ç–—è½¨è¿¹
                """)
                
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ¯ ä¸ªæ€§åŒ–æ²»ç–—æ–¹æ¡ˆ")
                
                status_output = gr.Textbox(
                    label="ğŸ“‹ å¤„ç†çŠ¶æ€",
                    lines=12,
                    interactive=False
                )
                
                with gr.Row():
                    with gr.Column():
                        audio_output = gr.Audio(
                            label="ğŸµ æ²»ç–—éŸ³ä¹ (20åˆ†é’Ÿä¸‰é˜¶æ®µ)",
                            type="filepath",
                            autoplay=False
                        )
                    
                    with gr.Column():
                        video_output = gr.Image(
                            label="ğŸ¬ è§†è§‰å¼•å¯¼é¢„è§ˆ",
                            type="filepath"
                        )
                
                with gr.Row():
                    report_output = gr.Image(
                        label="ğŸ“Š è¯¦ç»†åˆ†ææŠ¥å‘Š",
                        type="filepath"
                    )
                    
                    viz_output = gr.Image(
                        label="ğŸ“ˆ æƒ…ç»ªè½¨è¿¹å›¾",
                        type="filepath"
                    )
        
        # ç¤ºä¾‹
        gr.Markdown("### ğŸŒŸ ç¤ºä¾‹åœºæ™¯")
        gr.Examples(
            examples=[
                ["æœ€è¿‘æ€»æ˜¯å¤±çœ ï¼Œä¸€èººä¸‹å°±å¼€å§‹èƒ¡æ€ä¹±æƒ³ï¼Œè¶Šæƒ³è¶Šæ¸…é†’"],
                ["å·¥ä½œå‹åŠ›å¤ªå¤§äº†ï¼Œæ¯å¤©éƒ½å¾ˆç„¦è™‘ï¼Œæ™šä¸Šæ ¹æœ¬ç¡ä¸å¥½"],
                ["å¿ƒæƒ…å¾ˆä½è½ï¼Œå¯¹ä»€ä¹ˆéƒ½æä¸èµ·å…´è¶£ï¼Œæ™šä¸Šä¹Ÿç¡ä¸ç€"],
                ["ä»Šå¤©å‘ç”Ÿäº†ä¸€äº›çƒ¦å¿ƒäº‹ï¼Œç°åœ¨å¾ˆç”Ÿæ°”ï¼Œå®Œå…¨æ²¡æœ‰ç¡æ„"],
                ["ç™½å¤©å¤ªç´¯äº†ï¼Œä½†èººåœ¨åºŠä¸Šåè€Œç¡ä¸ç€ï¼Œå¤§è„‘è¿˜æ˜¯å¾ˆæ´»è·ƒ"]
            ],
            inputs=text_input
        )
        
        # ç»‘å®šäº‹ä»¶
        submit_btn.click(
            fn=demo.process_input,
            inputs=[text_input, audio_input, emotion_buttons, demo_mode_toggle, playback_mode],
            outputs=[audio_output, video_output, report_output, viz_output, status_output]
        )
        
    return interface

def main():
    """ä¸»å‡½æ•°"""
    print("å¯åŠ¨Webæ¼”ç¤ºç•Œé¢...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path("outputs/demo_sessions").mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    interface = create_interface()
    
    # å¯åŠ¨æœåŠ¡
    interface.launch(
        server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
        server_port=7860,
        share=True,  # åˆ›å»ºå…¬å…±é“¾æ¥
        inbrowser=True  # è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
    )

if __name__ == "__main__":
    main()