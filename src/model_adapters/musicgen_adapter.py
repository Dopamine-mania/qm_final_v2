#!/usr/bin/env python3
"""
MusicGenéŸ³ä¹ç”Ÿæˆé€‚é…å™¨ - é›†æˆMeta MusicGenæ¨¡å‹è¿›è¡Œé«˜è´¨é‡éŸ³ä¹ç”Ÿæˆ

ç†è®ºåŸºç¡€ï¼š
1. MusicGen (2023): "Simple and Controllable Music Generation"
   - Meta AI Research, https://arxiv.org/abs/2306.05284
   - 32kHz EnCodec tokenizerï¼Œ4ä¸ªcodebookï¼Œ50Hzé‡‡æ ·

2. Language Models for Music Medicine Generation (ISMIR 2024)
   - åŸºäºIsoåŸåˆ™çš„æ²»ç–—éŸ³ä¹ç”Ÿæˆ
   - ä½¿ç”¨LoRAå¾®è°ƒæŠ€æœ¯æ•´åˆæƒ…ç»ªæ ‡ç­¾

3. AudioCraft Toolkit Integration:
   - facebook/musicgen-melody: 1.5Bå‚æ•°ï¼Œæ”¯æŒæ—‹å¾‹å¼•å¯¼
   - facebook/musicgen-large: 3.3Bå‚æ•°ï¼Œæœ€é«˜è´¨é‡
   - æ”¯æŒé•¿åºåˆ—ç”Ÿæˆï¼ˆçª—å£æ»‘åŠ¨æŠ€æœ¯ï¼‰

ä½œè€…ï¼šå¿ƒå¢ƒæµè½¬å›¢é˜Ÿ
æ—¥æœŸï¼š2024
"""

import os
import sys
import time
import warnings
import numpy as np
from typing import Dict, Optional, Tuple, List, Union
from pathlib import Path
import logging

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class MusicGenAdapter:
    """
    MusicGenæ¨¡å‹é€‚é…å™¨ - ä¸ºç¡çœ æ²»ç–—ç”Ÿæˆé«˜è´¨é‡éŸ³ä¹
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. æ™ºèƒ½æ¨¡å‹é€‰æ‹©ï¼ˆæ ¹æ®å¯ç”¨GPUèµ„æºï¼‰
    2. æƒ…ç»ªåˆ°promptçš„æ˜ å°„
    3. æ²»ç–—éŸ³ä¹ç”Ÿæˆï¼ˆåŸºäºISOåŸåˆ™ï¼‰
    4. é•¿åºåˆ—ç”Ÿæˆä¼˜åŒ–
    5. è´¨é‡è¯„ä¼°å’Œåå¤„ç†
    """
    
    # æ²»ç–—éŸ³ä¹çš„BPMèŒƒå›´ï¼ˆåŸºäº2024å¹´éŸ³ä¹æ²»ç–—ç ”ç©¶ï¼‰
    THERAPEUTIC_BPM_RANGES = {
        'deep_sleep': (40, 60),      # æ·±åº¦ç¡çœ 
        'relaxation': (60, 80),      # æ”¾æ¾
        'meditation': (70, 90),      # å†¥æƒ³
        'stress_relief': (80, 100),  # å‹åŠ›ç¼“è§£
        'anxiety_reduction': (50, 70) # ç„¦è™‘ç¼“è§£
    }
    
    # åŸºäºFACEDæ•°æ®é›†çš„æƒ…ç»ªåˆ°éŸ³ä¹ç‰¹å¾æ˜ å°„
    EMOTION_TO_MUSICAL_FEATURES = {
        'anger': {
            'genre': 'rock, metal',
            'tempo': 'fast',
            'mood': 'aggressive, powerful',
            'instruments': 'electric guitar, drums',
            'key': 'minor'
        },
        'fear': {
            'genre': 'ambient, dark ambient',
            'tempo': 'slow',
            'mood': 'tense, mysterious',
            'instruments': 'strings, synthesizer',
            'key': 'minor'
        },
        'disgust': {
            'genre': 'experimental, industrial',
            'tempo': 'medium',
            'mood': 'harsh, dissonant',
            'instruments': 'synthesizer, noise',
            'key': 'atonal'
        },
        'sadness': {
            'genre': 'blues, ballad',
            'tempo': 'slow',
            'mood': 'melancholic, emotional',
            'instruments': 'piano, strings, acoustic guitar',
            'key': 'minor'
        },
        'amusement': {
            'genre': 'pop, jazz',
            'tempo': 'medium',
            'mood': 'playful, light',
            'instruments': 'piano, brass, percussion',
            'key': 'major'
        },
        'joy': {
            'genre': 'pop, dance',
            'tempo': 'fast',
            'mood': 'uplifting, energetic',
            'instruments': 'synthesizer, drums, bass',
            'key': 'major'
        },
        'inspiration': {
            'genre': 'orchestral, cinematic',
            'tempo': 'medium to fast',
            'mood': 'uplifting, motivational',
            'instruments': 'orchestra, piano, choir',
            'key': 'major'
        },
        'tenderness': {
            'genre': 'acoustic, folk',
            'tempo': 'slow',
            'mood': 'gentle, warm',
            'instruments': 'acoustic guitar, soft vocals, strings',
            'key': 'major'
        },
        'neutral': {
            'genre': 'ambient, instrumental',
            'tempo': 'medium',
            'mood': 'calm, balanced',
            'instruments': 'piano, soft pads',
            'key': 'major or minor'
        }
    }
    
    def __init__(self, 
                 model_size: str = "auto",
                 use_melody_conditioning: bool = True,
                 gpu_memory_gb: Optional[int] = None,
                 enable_long_generation: bool = True):
        """
        åˆå§‹åŒ–MusicGené€‚é…å™¨
        
        Args:
            model_size: æ¨¡å‹å¤§å° ("small", "medium", "large", "auto")
            use_melody_conditioning: æ˜¯å¦ä½¿ç”¨æ—‹å¾‹æ¡ä»¶ç”Ÿæˆ
            gpu_memory_gb: GPUæ˜¾å­˜å¤§å°ï¼ˆGBï¼‰ï¼Œç”¨äºè‡ªåŠ¨é€‰æ‹©æ¨¡å‹
            enable_long_generation: æ˜¯å¦å¯ç”¨é•¿åºåˆ—ç”Ÿæˆ
        """
        self.model_size = model_size
        self.use_melody_conditioning = use_melody_conditioning
        self.gpu_memory_gb = gpu_memory_gb
        self.enable_long_generation = enable_long_generation
        
        # æ¨¡å‹å’Œåº“åŠ è½½çŠ¶æ€
        self.model = None
        self.audiocraft_available = False
        self.sample_rate = 32000  # MusicGenæ ‡å‡†é‡‡æ ·ç‡
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._initialize_model()
        
    def _initialize_model(self):
        """åˆå§‹åŒ–MusicGenæ¨¡å‹"""
        try:
            # å°è¯•å¯¼å…¥audiocraft
            import torch
            import torchaudio
            from audiocraft.models import MusicGen
            from audiocraft.data.audio import audio_write
            
            self.audiocraft_available = True
            self.torch = torch
            self.torchaudio = torchaudio
            self.MusicGen = MusicGen
            self.audio_write = audio_write
            
            # æ£€æŸ¥GPUå¯ç”¨æ€§
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                logger.info(f"æ£€æµ‹åˆ° {gpu_count} ä¸ªGPUï¼Œæ˜¾å­˜: {gpu_memory:.1f}GB")
                
                if self.gpu_memory_gb is None:
                    self.gpu_memory_gb = gpu_memory
            else:
                logger.warning("æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼ï¼ˆæ€§èƒ½è¾ƒä½ï¼‰")
                self.gpu_memory_gb = 0
            
            # è‡ªåŠ¨é€‰æ‹©æ¨¡å‹å¤§å°
            model_name = self._select_optimal_model()
            
            logger.info(f"æ­£åœ¨åŠ è½½MusicGenæ¨¡å‹: {model_name}")
            self.model = self.MusicGen.get_pretrained(model_name)
            
            # è®¾ç½®é»˜è®¤ç”Ÿæˆå‚æ•°
            self._configure_generation_params()
            
            logger.info("âœ… MusicGenæ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼")
            
        except ImportError as e:
            logger.error(f"AudioCraftåº“æœªå®‰è£…: {e}")
            logger.error("è¯·è¿è¡Œ: pip install audiocraft")
            self.audiocraft_available = False
            
        except Exception as e:
            logger.error(f"MusicGenæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.audiocraft_available = False
    
    def _select_optimal_model(self) -> str:
        """æ ¹æ®GPUèµ„æºè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ¨¡å‹"""
        
        if self.model_size != "auto":
            # æ‰‹åŠ¨æŒ‡å®šæ¨¡å‹å¤§å°
            model_mapping = {
                "small": "facebook/musicgen-small",
                "medium": "facebook/musicgen-melody" if self.use_melody_conditioning else "facebook/musicgen-medium",
                "large": "facebook/musicgen-melody-large" if self.use_melody_conditioning else "facebook/musicgen-large"
            }
            return model_mapping.get(self.model_size, "facebook/musicgen-melody")
        
        # è‡ªåŠ¨é€‰æ‹©ï¼ˆåŸºäºGPUæ˜¾å­˜ï¼‰
        if self.gpu_memory_gb >= 40:
            # 40GB+ GPUï¼šä½¿ç”¨æœ€å¤§æ¨¡å‹
            model_name = "facebook/musicgen-melody-large" if self.use_melody_conditioning else "facebook/musicgen-large"
            logger.info(f"ğŸš€ ä½¿ç”¨å¤§å‹æ¨¡å‹ (3.3Bå‚æ•°): {model_name}")
            
        elif self.gpu_memory_gb >= 16:
            # 16-40GB GPUï¼šä½¿ç”¨ä¸­å‹æ¨¡å‹
            model_name = "facebook/musicgen-melody" if self.use_melody_conditioning else "facebook/musicgen-medium"
            logger.info(f"âš¡ ä½¿ç”¨ä¸­å‹æ¨¡å‹ (1.5Bå‚æ•°): {model_name}")
            
        elif self.gpu_memory_gb >= 8:
            # 8-16GB GPUï¼šä½¿ç”¨å°å‹æ¨¡å‹
            model_name = "facebook/musicgen-small"
            logger.info(f"ğŸ’» ä½¿ç”¨å°å‹æ¨¡å‹ (300Må‚æ•°): {model_name}")
            
        else:
            # <8GBæˆ–CPUï¼šä½¿ç”¨æœ€å°æ¨¡å‹
            model_name = "facebook/musicgen-small"
            logger.warning(f"âš ï¸ GPUæ˜¾å­˜ä¸è¶³ï¼Œä½¿ç”¨å°å‹æ¨¡å‹: {model_name}")
        
        return model_name
    
    def _configure_generation_params(self):
        """é…ç½®éŸ³ä¹ç”Ÿæˆå‚æ•°"""
        # é’ˆå¯¹æ²»ç–—éŸ³ä¹ä¼˜åŒ–çš„å‚æ•°ï¼ˆv2.0 - é«˜è´¨é‡ç‰ˆæœ¬ï¼‰
        self.model.set_generation_params(
            duration=10,        # é»˜è®¤10ç§’ï¼ˆå¯åŠ¨æ€è°ƒæ•´ï¼‰
            temperature=0.9,    # æé«˜åˆ›é€ æ€§å’ŒéŸ³ä¹è´¨é‡ (0.8â†’0.9)
            top_k=200,         # æ›´ä¸¥æ ¼çš„tokené€‰æ‹©ï¼Œæé«˜è´¨é‡ (250â†’200)
            top_p=0.0,         # ç¦ç”¨nucleusé‡‡æ ·
            cfg_coef=5.0       # å¤§å¹…å¢å¼ºæ–‡æœ¬æ¡ä»¶å½±å“ (3.0â†’5.0)
        )
        logger.info("ğŸ›ï¸ MusicGenå‚æ•°å·²ä¼˜åŒ–: temp=0.9, top_k=200, cfg_coef=5.0")
    
    def generate_therapeutic_music(self, 
                                 emotion_state: Dict,
                                 stage_info: Dict,
                                 duration_seconds: float = 60,
                                 bpm_target: Optional[int] = None) -> Tuple[np.ndarray, Dict]:
        """
        ç”Ÿæˆæ²»ç–—éŸ³ä¹
        
        Args:
            emotion_state: æƒ…ç»ªçŠ¶æ€ä¿¡æ¯ (valence, arousal, primary_emotion)
            stage_info: æ²»ç–—é˜¶æ®µä¿¡æ¯ (stage_name, therapy_goal)
            duration_seconds: éŸ³ä¹æ—¶é•¿ï¼ˆç§’ï¼‰
            bpm_target: ç›®æ ‡BPMï¼ˆå¯é€‰ï¼‰
            
        Returns:
            (audio_data, metadata): éŸ³é¢‘æ•°æ®å’Œå…ƒæ•°æ®
        """
        if not self.audiocraft_available:
            raise RuntimeError("MusicGenä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥audiocraftå®‰è£…")
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸµ [MusicGen v2.0] ç”Ÿæˆæ²»ç–—éŸ³ä¹:")
        logger.info(f"{'='*60}")
        
        # 1. æ„å»ºéŸ³ä¹prompt
        prompt = self._build_therapeutic_prompt(emotion_state, stage_info, bpm_target)
        logger.info(f"ğŸ“ ç”ŸæˆPrompt: {prompt}")
        
        # 2. è®¾ç½®ç”Ÿæˆå‚æ•°
        self.model.set_generation_params(duration=min(duration_seconds, 30))  # MusicGenå•æ¬¡æœ€å¤§30ç§’
        
        try:
            # 3. ç”ŸæˆéŸ³ä¹
            start_time = time.time()
            
            if duration_seconds <= 30:
                # çŸ­åºåˆ—ï¼šç›´æ¥ç”Ÿæˆ
                wav = self.model.generate([prompt])
                
                # æ£€æŸ¥ç”Ÿæˆç»“æœ
                if wav is None or len(wav) == 0:
                    raise RuntimeError("MusicGenç”Ÿæˆå¤±è´¥ï¼šè¿”å›ç©ºç»“æœ")
                
                # ç¡®ä¿è½¬æ¢ä¸ºnumpyæ•°ç»„
                if hasattr(wav[0], 'cpu'):
                    audio_data = wav[0].cpu().numpy().flatten()
                else:
                    audio_data = wav[0].flatten()
                
                # æ£€æŸ¥éŸ³é¢‘æ•°æ®æœ‰æ•ˆæ€§
                if len(audio_data) == 0:
                    raise RuntimeError("MusicGenç”Ÿæˆå¤±è´¥ï¼šéŸ³é¢‘æ•°æ®ä¸ºç©º")
                
            else:
                # é•¿åºåˆ—ï¼šä½¿ç”¨çª—å£æ»‘åŠ¨æŠ€æœ¯
                audio_data = self._generate_long_sequence(prompt, duration_seconds)
            
            generation_time = time.time() - start_time
            
            # 4. åå¤„ç†
            audio_data = self._post_process_audio(audio_data, stage_info)
            
            # 5. ç”Ÿæˆå…ƒæ•°æ®
            metadata = {
                'prompt': prompt,
                'emotion_state': emotion_state,
                'stage_info': stage_info,
                'duration': len(audio_data) / self.sample_rate,
                'sample_rate': self.sample_rate,
                'generation_time': generation_time,
                'model_used': getattr(getattr(self.model, 'cfg', None), 'name', 'musicgen_model'),
                'bpm_target': bpm_target
            }
            
            logger.info(f"â±ï¸ ç”Ÿæˆæ—¶é—´: {generation_time:.1f}ç§’")
            logger.info(f"ğŸ¼ éŸ³ä¹æ—¶é•¿: {len(audio_data) / self.sample_rate:.1f}ç§’")
            logger.info(f"ğŸ“Š é‡‡æ ·ç‡: {self.sample_rate}Hz")
            logger.info(f"{'='*60}")
            
            return audio_data, metadata
            
        except Exception as e:
            logger.error(f"éŸ³ä¹ç”Ÿæˆå¤±è´¥: {e}")
            # è¿”å›ç©ºéŸ³é¢‘ä½œä¸ºfallback
            fallback_audio = np.zeros(int(duration_seconds * self.sample_rate))
            return fallback_audio, {'error': str(e)}
    
    def _build_therapeutic_prompt(self, 
                                emotion_state: Dict, 
                                stage_info: Dict,
                                bpm_target: Optional[int] = None) -> str:
        """
        æ„å»ºæ²»ç–—éŸ³ä¹çš„prompt
        
        åŸºäºæƒ…ç»ªçŠ¶æ€ã€æ²»ç–—é˜¶æ®µå’ŒéŸ³ä¹æ²»ç–—ç†è®º
        """
        # è·å–ä¸»è¦æƒ…ç»ª
        primary_emotion = emotion_state.get('primary_emotion', 'neutral')
        valence = emotion_state.get('valence', 0.0)
        arousal = emotion_state.get('arousal', 0.0)
        
        # è·å–æ²»ç–—é˜¶æ®µ
        stage_name = stage_info.get('stage_name', 'unknown')
        
        # è°ƒè¯•ï¼šæ˜¾ç¤ºstage_infoå†…å®¹
        logger.info(f"ğŸ” Promptæ„å»º - stage_info: {stage_info}")
        logger.info(f"ğŸ” Promptæ„å»º - stage_name: {stage_name}")
        
        # åŸºç¡€éŸ³ä¹ç‰¹å¾
        emotion_features = self.EMOTION_TO_MUSICAL_FEATURES.get(primary_emotion, 
                                                              self.EMOTION_TO_MUSICAL_FEATURES['neutral'])
        
        # æ ¹æ®æ²»ç–—é˜¶æ®µè°ƒæ•´
        if 'åŒæ­¥åŒ–' in stage_name or 'Sync' in stage_name:
            # åŒæ­¥åŒ–ï¼šåŒ¹é…å½“å‰æƒ…ç»ªï¼Œä½†ç¨ä½œç¼“å’Œ
            mood_adjustment = "slightly calmer"
            tempo_adjustment = "moderate"
            therapeutic_goal = "establishing emotional connection"
            
        elif 'å¼•å¯¼åŒ–' in stage_name or 'Guide' in stage_name:
            # å¼•å¯¼åŒ–ï¼šé€æ¸è½¬å‘å¹³é™
            mood_adjustment = "gradually becoming peaceful"
            tempo_adjustment = "slowing down"
            therapeutic_goal = "transitioning to relaxation"
            
        elif 'å·©å›ºåŒ–' in stage_name or 'Consolidate' in stage_name:
            # å·©å›ºåŒ–ï¼šæ·±åº¦æ”¾æ¾ï¼Œå‡†å¤‡ç¡çœ 
            mood_adjustment = "deeply relaxing and sleep-inducing"
            tempo_adjustment = "very slow"
            therapeutic_goal = "deep sleep preparation"
            
        else:
            mood_adjustment = "therapeutic and calming"
            tempo_adjustment = "relaxed"
            therapeutic_goal = "general relaxation"
        
        # ç¡®å®šBPM
        if bpm_target:
            bpm_text = f"{bpm_target} BPM"
        else:
            # æ ¹æ®arousalè®¡ç®—BPMï¼ˆæ˜ å°„åˆ°æ²»ç–—èŒƒå›´ï¼‰
            if arousal > 0.5:
                bpm_range = self.THERAPEUTIC_BPM_RANGES['stress_relief']
            elif arousal > 0:
                bpm_range = self.THERAPEUTIC_BPM_RANGES['relaxation']
            else:
                bpm_range = self.THERAPEUTIC_BPM_RANGES['deep_sleep']
            
            bpm_target = int(bpm_range[0] + (bpm_range[1] - bpm_range[0]) * (arousal + 1) / 2)
            bpm_text = f"{bpm_target} BPM"
        
        # æ„å»ºpromptï¼ˆåŸºäº2024å¹´MusicGenæœ€ä½³å®è·µï¼‰
        prompt_parts = [
            # æ ¸å¿ƒæ²»ç–—ç›®æ ‡
            f"therapeutic {emotion_features['genre']} music for {therapeutic_goal}",
            
            # æƒ…ç»ªå’Œæ°›å›´
            f"{mood_adjustment}, {emotion_features['mood']}",
            
            # ä¹å™¨å’Œè´¨æ„Ÿ
            f"featuring {emotion_features['instruments']}",
            
            # èŠ‚å¥å’Œè°ƒæ€§
            f"{bpm_text}, {emotion_features['key']} key",
            
            # æ²»ç–—ç‰¹æ€§
            f"{tempo_adjustment}, sleep therapy, ambient pads, gentle transitions",
            
            # æŠ€æœ¯å“è´¨
            "high quality, professional, studio recording"
        ]
        
        prompt = ", ".join(prompt_parts)
        
        # é™åˆ¶prompté•¿åº¦ï¼ˆMusicGenæœ€ä½³å®è·µï¼š<200å­—ç¬¦ï¼‰
        if len(prompt) > 200:
            prompt = prompt[:197] + "..."
        
        return prompt
    
    def _generate_long_sequence(self, prompt: str, duration_seconds: float) -> np.ndarray:
        """
        ç”Ÿæˆé•¿åºåˆ—éŸ³ä¹ï¼ˆä½¿ç”¨çª—å£æ»‘åŠ¨æŠ€æœ¯ï¼‰
        
        åŸºäºMusicGenè®ºæ–‡ä¸­çš„é•¿åºåˆ—ç”Ÿæˆæ–¹æ³•ï¼š
        - 30ç§’çª—å£ï¼Œ10ç§’æ»‘åŠ¨
        - ä¿æŒ20ç§’ä¸Šä¸‹æ–‡
        """
        if not self.enable_long_generation:
            # å¦‚æœç¦ç”¨é•¿åºåˆ—ç”Ÿæˆï¼Œç›´æ¥ç”Ÿæˆ30ç§’
            wav = self.model.generate([prompt])
            return wav[0].cpu().numpy()
        
        logger.info(f"ğŸ”„ ä½¿ç”¨çª—å£æ»‘åŠ¨æŠ€æœ¯ç”Ÿæˆ {duration_seconds:.1f}ç§’éŸ³ä¹")
        
        # å‚æ•°è®¾ç½®
        window_duration = 30  # 30ç§’çª—å£
        slide_duration = 10   # 10ç§’æ»‘åŠ¨
        context_duration = 20 # 20ç§’ä¸Šä¸‹æ–‡
        
        # è®¡ç®—éœ€è¦çš„çª—å£æ•°
        num_windows = int(np.ceil((duration_seconds - window_duration) / slide_duration)) + 1
        
        # å­˜å‚¨å®Œæ•´éŸ³é¢‘
        full_audio = None
        context_audio = None
        
        for i in range(num_windows):
            logger.info(f"  ç”Ÿæˆçª—å£ {i+1}/{num_windows}")
            
            if context_audio is not None:
                # ä½¿ç”¨ä¸Šä¸‹æ–‡éŸ³é¢‘è¿›è¡Œæ¡ä»¶ç”Ÿæˆ
                # æ³¨æ„ï¼šè¿™éœ€è¦MusicGençš„continuationåŠŸèƒ½
                # å½“å‰ç®€åŒ–å®ç°ï¼šç›´æ¥ç”Ÿæˆæ–°ç‰‡æ®µ
                pass
            
            # ç”Ÿæˆå½“å‰çª—å£
            wav = self.model.generate([prompt])
            # ç¡®ä¿è½¬æ¢ä¸ºnumpyæ•°ç»„
            if hasattr(wav[0], 'cpu'):
                current_audio = wav[0].cpu().numpy().flatten()
            else:
                current_audio = wav[0].flatten()
            
            if full_audio is None:
                # ç¬¬ä¸€ä¸ªçª—å£ï¼šå®Œæ•´ä½¿ç”¨
                full_audio = current_audio
            else:
                # åç»­çª—å£ï¼šåªä½¿ç”¨æ»‘åŠ¨éƒ¨åˆ†ï¼Œå¹¶è¿›è¡Œäº¤å‰æ·¡åŒ–
                slide_samples = int(slide_duration * self.sample_rate)
                
                # ç®€å•æ‹¼æ¥ï¼ˆæœªæ¥å¯æ”¹è¿›ä¸ºäº¤å‰æ·¡åŒ–ï¼‰
                full_audio = np.concatenate([full_audio, current_audio[:slide_samples]])
            
            # æ›´æ–°ä¸Šä¸‹æ–‡ï¼ˆä¿ç•™æœ€å20ç§’ï¼‰
            context_samples = int(context_duration * self.sample_rate)
            if len(full_audio) > context_samples:
                context_audio = full_audio[-context_samples:]
            else:
                context_audio = full_audio
        
        # æˆªå–åˆ°ç›®æ ‡é•¿åº¦
        target_samples = int(duration_seconds * self.sample_rate)
        if len(full_audio) > target_samples:
            full_audio = full_audio[:target_samples]
        
        return full_audio
    
    def _post_process_audio(self, audio_data: np.ndarray, stage_info: Dict) -> np.ndarray:
        """
        éŸ³é¢‘åå¤„ç†
        
        æ ¹æ®æ²»ç–—é˜¶æ®µåº”ç”¨ç‰¹å®šçš„éŸ³é¢‘å¤„ç†
        """
        stage_name = stage_info.get('stage_name', '')
        
        # åº”ç”¨é˜¶æ®µç‰¹å®šçš„åå¤„ç†
        if 'åŒæ­¥åŒ–' in stage_name:
            # åŒæ­¥åŒ–é˜¶æ®µï¼šä¿æŒåŸæœ‰åŠ¨æ€
            pass
            
        elif 'å¼•å¯¼åŒ–' in stage_name:
            # å¼•å¯¼åŒ–é˜¶æ®µï¼šé€æ¸é™ä½éŸ³é‡
            fade_length = int(0.3 * len(audio_data))  # æœ€å30%æ·¡å‡º
            fade_curve = np.linspace(1.0, 0.8, fade_length)
            audio_data[-fade_length:] *= fade_curve
            
        elif 'å·©å›ºåŒ–' in stage_name:
            # å·©å›ºåŒ–é˜¶æ®µï¼šè½»å¾®é™ä½éŸ³é‡ï¼Œä¿æŒéŸ³è´¨
            audio_data *= 0.85  # æ¸©å’Œé™ä½éŸ³é‡ (0.7â†’0.85)
            
            # æ”¹è¿›çš„ä½é€šæ»¤æ³¢ï¼Œå‡å°‘éŸ³è´¨æŸå¤±
            audio_data = self._gentle_lowpass_filter(audio_data)
        
        # æ”¹è¿›çš„éŸ³é‡å½’ä¸€åŒ–ï¼Œä¿æŒåŠ¨æ€èŒƒå›´
        max_amplitude = np.max(np.abs(audio_data))
        if max_amplitude > 0:
            # ä½¿ç”¨æ›´æ¸©å’Œçš„å½’ä¸€åŒ–ï¼Œä¿æŒéŸ³è´¨
            target_level = 0.9 if max_amplitude > 0.9 else max_amplitude
            audio_data = audio_data / max_amplitude * target_level
        
        return audio_data
    
    def _simple_lowpass_filter(self, audio_data: np.ndarray, alpha: float = 0.1) -> np.ndarray:
        """ç®€å•çš„ä½é€šæ»¤æ³¢å™¨ï¼ˆä¸€é˜¶IIRï¼‰"""
        filtered = np.zeros_like(audio_data)
        filtered[0] = audio_data[0]
        
        for i in range(1, len(audio_data)):
            filtered[i] = alpha * audio_data[i] + (1 - alpha) * filtered[i-1]
        
        return filtered
    
    def _gentle_lowpass_filter(self, audio_data: np.ndarray) -> np.ndarray:
        """æ¸©å’Œçš„ä½é€šæ»¤æ³¢ï¼Œä¿æŒéŸ³è´¨"""
        # ä½¿ç”¨æ›´æ¸©å’Œçš„æ»¤æ³¢å‚æ•°ï¼Œå‡å°‘éŸ³è´¨æŸå¤±
        alpha = 0.05  # æ›´å°çš„alphaå€¼ï¼Œæ›´æ¸©å’Œçš„æ»¤æ³¢
        
        # åº”ç”¨åŒå‘æ»¤æ³¢ï¼Œå‡å°‘ç›¸ä½å¤±çœŸ
        forward_filtered = self._simple_lowpass_filter(audio_data, alpha)
        backward_filtered = self._simple_lowpass_filter(forward_filtered[::-1], alpha)
        result = backward_filtered[::-1]
        
        # æ··åˆåŸå§‹ä¿¡å·å’Œæ»¤æ³¢ä¿¡å·ï¼Œä¿æŒæ¸…æ™°åº¦
        mix_ratio = 0.7  # 70%æ»¤æ³¢ + 30%åŸå§‹
        return mix_ratio * result + (1 - mix_ratio) * audio_data
    
    def save_audio(self, audio_data: np.ndarray, file_path: str, metadata: Optional[Dict] = None):
        """
        ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ®
            file_path: ä¿å­˜è·¯å¾„
            metadata: å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
        """
        try:
            if self.audiocraft_available:
                # ä½¿ç”¨audiocraftçš„ä¿å­˜åŠŸèƒ½
                audio_tensor = self.torch.from_numpy(audio_data).unsqueeze(0)
                self.audio_write(
                    Path(file_path).stem,
                    audio_tensor,
                    self.sample_rate,
                    strategy="loudness"  # å“åº¦å½’ä¸€åŒ–
                )
                logger.info(f"âœ… éŸ³é¢‘å·²ä¿å­˜: {file_path}")
                
            else:
                # fallbackåˆ°numpyä¿å­˜
                # è¿™é‡Œéœ€è¦å…¶ä»–éŸ³é¢‘åº“æ”¯æŒ
                logger.warning("AudioCraftä¸å¯ç”¨ï¼ŒéŸ³é¢‘ä¿å­˜å¯èƒ½å¤±è´¥")
                
        except Exception as e:
            logger.error(f"éŸ³é¢‘ä¿å­˜å¤±è´¥: {e}")
    
    def is_available(self) -> bool:
        """æ£€æŸ¥MusicGenæ˜¯å¦å¯ç”¨"""
        return self.audiocraft_available and self.model is not None
    
    def get_model_info(self) -> Dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if not self.is_available():
            return {'status': 'unavailable'}
        
        return {
            'status': 'available',
            'model_name': getattr(self.model.cfg, 'name', 'unknown') if hasattr(self.model, 'cfg') else 'unknown',
            'sample_rate': self.sample_rate,
            'gpu_memory_gb': self.gpu_memory_gb,
            'use_melody_conditioning': self.use_melody_conditioning,
            'enable_long_generation': self.enable_long_generation
        }


# å·¥å‚å‡½æ•°ï¼Œä¾¿äºé›†æˆ
def create_musicgen_adapter(
    model_size: str = "auto",
    use_melody_conditioning: bool = True,
    gpu_memory_gb: Optional[int] = None
) -> MusicGenAdapter:
    """
    åˆ›å»ºMusicGené€‚é…å™¨å®ä¾‹
    
    Args:
        model_size: æ¨¡å‹å¤§å°é€‰æ‹©
        use_melody_conditioning: æ˜¯å¦ä½¿ç”¨æ—‹å¾‹æ¡ä»¶
        gpu_memory_gb: GPUæ˜¾å­˜å¤§å°ï¼ˆç”¨äºè‡ªåŠ¨é€‰æ‹©ï¼‰
        
    Returns:
        MusicGenAdapterå®ä¾‹
    """
    return MusicGenAdapter(
        model_size=model_size,
        use_melody_conditioning=use_melody_conditioning,
        gpu_memory_gb=gpu_memory_gb
    )