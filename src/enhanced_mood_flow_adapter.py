#!/usr/bin/env python3
"""
å¢å¼ºå‹å¿ƒå¢ƒæµè½¬é€‚é…å™¨ - å°†æ–°çš„ç†è®ºé©±åŠ¨æ¨¡å—é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿ

è®¾è®¡åŸåˆ™ï¼š
1. ä¿æŒå‘åå…¼å®¹æ€§
2. æ¸è¿›å¼å¢å¼º
3. å¯é…ç½®çš„æ¨¡å—é€‰æ‹©
4. ä¼˜é›…é™çº§

ä½œè€…ï¼šå¿ƒå¢ƒæµè½¬å›¢é˜Ÿ
æ—¥æœŸï¼š2024
"""

import os
import sys
import numpy as np
from typing import Dict, Optional, Any, Tuple
from pathlib import Path
import logging

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥å¢å¼ºæ¨¡å—
from src.emotion_recognition.enhanced_emotion_recognizer import (
    EnhancedEmotionRecognizer, DetailedEmotion, create_emotion_recognizer
)
from src.therapy_planning.enhanced_iso_planner import (
    EnhancedISOPlanner, create_iso_planner, TherapyStageConfig
)
from src.music_mapping.enhanced_music_mapper import (
    EnhancedMusicMapper, create_music_mapper, MusicProfile
)

# å¯¼å…¥SOTAæ¨¡å‹é€‚é…å™¨
try:
    from src.model_adapters.musicgen_adapter import (
        MusicGenAdapter, create_musicgen_adapter
    )
    from src.model_adapters.music_quality_evaluator import (
        MusicQualityEvaluator, create_music_quality_evaluator
    )
    SOTA_MODELS_AVAILABLE = True
except ImportError as e:
    logger.warning(f"SOTAæ¨¡å‹é€‚é…å™¨å¯¼å…¥å¤±è´¥: {e}")
    SOTA_MODELS_AVAILABLE = False

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class EnhancedMoodFlowAdapter:
    """
    å¢å¼ºå‹é€‚é…å™¨ - æ— ç¼é›†æˆæ–°æ¨¡å—åˆ°ç°æœ‰MoodFlowApp
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. å¢å¼ºçš„æƒ…ç»ªè¯†åˆ«ï¼ˆ9ç§ç»†ç²’åº¦æƒ…ç»ªï¼‰
    2. ç†è®ºé©±åŠ¨çš„ISOæ²»ç–—è§„åˆ’
    3. ç²¾å‡†çš„éŸ³ä¹ç‰¹å¾æ˜ å°„
    4. ä¿æŒä¸åŸç³»ç»Ÿçš„å®Œå…¨å…¼å®¹
    """
    
    def __init__(self, 
                 use_enhanced_emotion: bool = True,
                 use_enhanced_planning: bool = True,
                 use_enhanced_mapping: bool = True,
                 use_sota_music_generation: bool = False,
                 fallback_to_original: bool = True):
        """
        åˆå§‹åŒ–é€‚é…å™¨
        
        Args:
            use_enhanced_emotion: æ˜¯å¦ä½¿ç”¨å¢å¼ºæƒ…ç»ªè¯†åˆ«
            use_enhanced_planning: æ˜¯å¦ä½¿ç”¨å¢å¼ºæ²»ç–—è§„åˆ’
            use_enhanced_mapping: æ˜¯å¦ä½¿ç”¨å¢å¼ºéŸ³ä¹æ˜ å°„
            use_sota_music_generation: æ˜¯å¦ä½¿ç”¨SOTAéŸ³ä¹ç”Ÿæˆæ¨¡å‹ï¼ˆMusicGenï¼‰
            fallback_to_original: å‡ºé”™æ—¶æ˜¯å¦å›é€€åˆ°åŸå§‹å®ç°
        """
        self.use_enhanced_emotion = use_enhanced_emotion
        self.use_enhanced_planning = use_enhanced_planning
        self.use_enhanced_mapping = use_enhanced_mapping
        self.use_sota_music_generation = use_sota_music_generation and SOTA_MODELS_AVAILABLE
        self.fallback_to_original = fallback_to_original
        
        # åˆå§‹åŒ–å¢å¼ºæ¨¡å—
        self._init_enhanced_modules()
        
    def _init_enhanced_modules(self):
        """åˆå§‹åŒ–å¢å¼ºæ¨¡å—ï¼Œå¸¦é”™è¯¯å¤„ç†"""
        try:
            # æƒ…ç»ªè¯†åˆ«å™¨
            if self.use_enhanced_emotion:
                self.emotion_recognizer = create_emotion_recognizer(use_advanced=False)
                logger.info("âœ… å¢å¼ºæƒ…ç»ªè¯†åˆ«å™¨åˆå§‹åŒ–æˆåŠŸ")
            else:
                self.emotion_recognizer = None
                
            # ISOè§„åˆ’å™¨
            if self.use_enhanced_planning:
                self.iso_planner = create_iso_planner(enhanced=True)
                logger.info("âœ… å¢å¼ºISOè§„åˆ’å™¨åˆå§‹åŒ–æˆåŠŸ")
            else:
                self.iso_planner = None
                
            # éŸ³ä¹æ˜ å°„å™¨
            if self.use_enhanced_mapping:
                self.music_mapper = create_music_mapper(enhanced=True, sleep_optimized=True)
                logger.info("âœ… å¢å¼ºéŸ³ä¹æ˜ å°„å™¨åˆå§‹åŒ–æˆåŠŸ")
            else:
                self.music_mapper = None
                
            # SOTAéŸ³ä¹ç”Ÿæˆå™¨
            if self.use_sota_music_generation:
                try:
                    self.musicgen_adapter = create_musicgen_adapter(
                        model_size="auto",
                        use_melody_conditioning=True
                    )
                    self.music_quality_evaluator = create_music_quality_evaluator()
                    logger.info("âœ… MusicGenéŸ³ä¹ç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ")
                except Exception as e:
                    logger.error(f"MusicGenåˆå§‹åŒ–å¤±è´¥: {e}")
                    self.musicgen_adapter = None
                    self.music_quality_evaluator = None
                    if not self.fallback_to_original:
                        raise
            else:
                self.musicgen_adapter = None
                self.music_quality_evaluator = None
                
        except Exception as e:
            logger.error(f"âŒ å¢å¼ºæ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
            if not self.fallback_to_original:
                raise
    
    def analyze_emotion_enhanced(self, text: str, original_method=None) -> Any:
        """
        å¢å¼ºçš„æƒ…ç»ªåˆ†æ
        
        Args:
            text: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
            original_method: åŸå§‹çš„æƒ…ç»ªåˆ†ææ–¹æ³•ï¼ˆç”¨äºå›é€€ï¼‰
            
        Returns:
            EmotionStateå¯¹è±¡ï¼ˆå…¼å®¹åŸç³»ç»Ÿï¼‰
        """
        try:
            if self.use_enhanced_emotion and self.emotion_recognizer:
                # ä½¿ç”¨å¢å¼ºè¯†åˆ«å™¨
                detailed_emotion = self.emotion_recognizer.recognize(text)
                
                # è½¬æ¢ä¸ºåŸç³»ç»Ÿæ ¼å¼
                # åˆ›å»ºå…¼å®¹çš„EmotionStateå¯¹è±¡
                emotion_state = type('EmotionState', (), {
                    'valence': detailed_emotion.valence,
                    'arousal': detailed_emotion.arousal,
                    # æ·»åŠ å¢å¼ºä¿¡æ¯ä½œä¸ºé¢å¤–å±æ€§
                    '_detailed': detailed_emotion,
                    '_primary_emotion': detailed_emotion.primary_emotion,
                    '_confidence': detailed_emotion.confidence
                })()
                
                # è®°å½•è¯¦ç»†ä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆæ ‡è¯†ï¼‰
                print(f"\n{'='*60}")
                print(f"ğŸ§  [å¢å¼ºæƒ…ç»ªè¯†åˆ« v2.0] ç»†ç²’åº¦åˆ†æç»“æœ:")
                print(f"{'='*60}")
                print(f"ğŸ“ è¾“å…¥æ–‡æœ¬: {text[:50]}...")
                print(f"ğŸ¯ ä¸»è¦æƒ…ç»ª: {detailed_emotion.primary_emotion} ({self._emotion_to_chinese(detailed_emotion.primary_emotion)})")
                print(f"ğŸ“Š V-Aåæ ‡: Valence={detailed_emotion.valence:.2f}, Arousal={detailed_emotion.arousal:.2f}")
                print(f"ğŸ’¯ ç½®ä¿¡åº¦: {detailed_emotion.confidence:.1%}")
                print(f"ğŸ’ª å¼ºåº¦: {detailed_emotion.intensity:.1%}")
                if detailed_emotion.secondary_emotions:
                    print(f"ğŸ”„ æ¬¡è¦æƒ…ç»ª: {detailed_emotion.secondary_emotions}")
                print(f"{'='*60}\n")
                
                return emotion_state
                
        except Exception as e:
            logger.error(f"å¢å¼ºæƒ…ç»ªè¯†åˆ«å¤±è´¥: {e}")
            if self.fallback_to_original and original_method:
                logger.info("å›é€€åˆ°åŸå§‹æƒ…ç»ªè¯†åˆ«æ–¹æ³•")
                return original_method(text)
            raise
        
        # å¦‚æœæœªå¯ç”¨å¢å¼ºæˆ–æ— åŸå§‹æ–¹æ³•ï¼Œè¿”å›é»˜è®¤
        if original_method:
            return original_method(text)
        else:
            # è¿”å›é»˜è®¤æƒ…ç»ªçŠ¶æ€
            return type('EmotionState', (), {'valence': -0.5, 'arousal': 0.5})()
    
    def plan_therapy_stages_enhanced(self, current_emotion, target_emotion, 
                                   duration, original_method=None) -> list:
        """
        å¢å¼ºçš„æ²»ç–—é˜¶æ®µè§„åˆ’
        
        æ•´åˆISOåŸåˆ™å’ŒGrossæ¨¡å‹
        """
        try:
            if self.use_enhanced_planning and self.iso_planner:
                # ä½¿ç”¨å¢å¼ºè§„åˆ’å™¨
                stages = self.iso_planner.plan_stages(current_emotion, target_emotion, duration)
                
                # è®°å½•è§„åˆ’ä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆæ ‡è¯†ï¼‰
                print(f"\n{'='*60}")
                print(f"ğŸ“‹ [å¢å¼ºæ²»ç–—è§„åˆ’ v2.0] ISOåŸåˆ™+Grossæ¨¡å‹:")
                print(f"{'='*60}")
                for i, stage in enumerate(stages):
                    print(f"  é˜¶æ®µ{i+1}: {stage['stage'].value}")
                    print(f"    - æ—¶é•¿: {stage['duration']:.1f}åˆ†é’Ÿ")
                    print(f"    - ç›®æ ‡æƒ…ç»ª: V={stage['emotion'].valence:.2f}, A={stage['emotion'].arousal:.2f}")
                    if hasattr(stage['stage'], 'value') and 'åŒæ­¥åŒ–' in stage['stage'].value:
                        print(f"    - ç­–ç•¥: åŒ¹é…ç”¨æˆ·å½“å‰æƒ…ç»ªï¼Œå»ºç«‹ä¿¡ä»»")
                    elif hasattr(stage['stage'], 'value') and 'å¼•å¯¼åŒ–' in stage['stage'].value:
                        print(f"    - ç­–ç•¥: æ¸è¿›å¼è¿‡æ¸¡ï¼Œè®¤çŸ¥é‡è¯„")
                    elif hasattr(stage['stage'], 'value') and 'å·©å›ºåŒ–' in stage['stage'].value:
                        print(f"    - ç­–ç•¥: ç»´æŒä½å”¤é†’ï¼Œæ·±åŒ–æ”¾æ¾")
                print(f"{'='*60}\n")
                
                return stages
                
        except Exception as e:
            logger.error(f"å¢å¼ºæ²»ç–—è§„åˆ’å¤±è´¥: {e}")
            if self.fallback_to_original and original_method:
                logger.info("å›é€€åˆ°åŸå§‹è§„åˆ’æ–¹æ³•")
                return original_method.plan_stages(current_emotion, target_emotion, duration)
            raise
        
        # å¦‚æœæœªå¯ç”¨å¢å¼ºï¼Œä½¿ç”¨åŸå§‹æ–¹æ³•
        if original_method:
            return original_method.plan_stages(current_emotion, target_emotion, duration)
        else:
            # è¿”å›ç®€å•çš„é»˜è®¤è§„åˆ’
            return self._create_default_stages(duration)
    
    def get_music_parameters_enhanced(self, emotion_state, stage=None, 
                                    original_method=None) -> Dict:
        """
        å¢å¼ºçš„éŸ³ä¹å‚æ•°ç”Ÿæˆ
        
        åŸºäºç²¾å‡†çš„æƒ…ç»ª-éŸ³ä¹ç‰¹å¾æ˜ å°„
        """
        try:
            if self.use_enhanced_mapping and self.music_mapper:
                # æå–V-Aå€¼
                valence = emotion_state.valence
                arousal = emotion_state.arousal
                
                # è·å–é˜¶æ®µåç§°ï¼ˆå¦‚æœæœ‰ï¼‰
                stage_name = stage.get('stage').value if stage and 'stage' in stage else None
                
                # ä½¿ç”¨å¢å¼ºæ˜ å°„å™¨
                music_params = self.music_mapper.get_music_params(valence, arousal)
                
                # æ·»åŠ é˜¶æ®µç‰¹å®šè°ƒæ•´
                if stage_name:
                    music_params['stage'] = stage_name
                
                # è®°å½•æ˜ å°„ä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆæ ‡è¯†ï¼‰
                print(f"\n{'='*60}")
                print(f"ğŸµ [å¢å¼ºéŸ³ä¹æ˜ å°„ v2.0] ç²¾å‡†ç‰¹å¾ç”Ÿæˆ:")
                print(f"{'='*60}")
                print(f"  æƒ…ç»ªçŠ¶æ€: V={valence:.2f}, A={arousal:.2f}")
                print(f"  BPM: {music_params.get('bpm', 'N/A')} (åŸºäºArousalç›¸å…³æ€§0.88)")
                print(f"  è°ƒæ€§: {music_params.get('key', 'N/A')} (åŸºäºValenceç›¸å…³æ€§0.74)")
                print(f"  ä¹å™¨: {', '.join(music_params.get('instruments', [])[:3])}")
                print(f"  èŠ‚å¥å¤æ‚åº¦: {music_params.get('rhythm_pattern_complexity', 0):.2f}")
                if 'binaural_frequency' in music_params:
                    print(f"  åŒè€³èŠ‚æ‹: {music_params['binaural_frequency']}Hz (è¯±å¯¼è„‘ç”µæ³¢åŒæ­¥)")
                print(f"{'='*60}\n")
                
                return music_params
                
        except Exception as e:
            logger.error(f"å¢å¼ºéŸ³ä¹æ˜ å°„å¤±è´¥: {e}")
            if self.fallback_to_original and original_method:
                logger.info("å›é€€åˆ°åŸå§‹æ˜ å°„æ–¹æ³•")
                # åŸå§‹æ–¹æ³•å¯èƒ½åªè¿”å›BPM
                bpm = original_method.calc_bpm(emotion_state.arousal)
                return {'bpm': bpm}
            raise
        
        # å¦‚æœæœªå¯ç”¨å¢å¼ºï¼Œä½¿ç”¨ç®€å•æ˜ å°„
        if original_method:
            bpm = original_method.calc_bpm(emotion_state.arousal)
            return {'bpm': bpm}
        else:
            # è¿”å›é»˜è®¤å‚æ•°
            return {'bpm': 60}
    
    def _create_default_stages(self, duration: int) -> list:
        """åˆ›å»ºé»˜è®¤çš„æ²»ç–—é˜¶æ®µï¼ˆç”¨äºå›é€€ï¼‰"""
        return [
            {
                'stage': type('Stage', (), {'value': 'åŒæ­¥åŒ–'})(),
                'duration': duration * 0.25,
                'emotion': type('EmotionState', (), {'valence': -0.5, 'arousal': 0.5})()
            },
            {
                'stage': type('Stage', (), {'value': 'å¼•å¯¼åŒ–'})(),
                'duration': duration * 0.50,
                'emotion': type('EmotionState', (), {'valence': 0.0, 'arousal': 0.0})()
            },
            {
                'stage': type('Stage', (), {'value': 'å·©å›ºåŒ–'})(),
                'duration': duration * 0.25,
                'emotion': type('EmotionState', (), {'valence': 0.3, 'arousal': -0.8})()
            }
        ]
    
    def get_enhancement_status(self) -> Dict[str, bool]:
        """è·å–å¢å¼ºæ¨¡å—çš„çŠ¶æ€"""
        return {
            'emotion_recognition': self.use_enhanced_emotion and self.emotion_recognizer is not None,
            'therapy_planning': self.use_enhanced_planning and self.iso_planner is not None,
            'music_mapping': self.use_enhanced_mapping and self.music_mapper is not None,
            'sota_music_generation': self.use_sota_music_generation and self.musicgen_adapter is not None
        }
    
    def get_detailed_emotion_info(self, emotion_state) -> Optional[Dict]:
        """
        è·å–è¯¦ç»†çš„æƒ…ç»ªä¿¡æ¯ï¼ˆå¦‚æœä½¿ç”¨äº†å¢å¼ºè¯†åˆ«ï¼‰
        
        ç”¨äºåœ¨ç•Œé¢ä¸Šæ˜¾ç¤ºæ›´ä¸°å¯Œçš„ä¿¡æ¯
        """
        if hasattr(emotion_state, '_detailed'):
            detailed = emotion_state._detailed
            return {
                'primary_emotion': detailed.primary_emotion,
                'primary_emotion_cn': self._emotion_to_chinese(detailed.primary_emotion),
                'confidence': detailed.confidence,
                'intensity': detailed.intensity,
                'secondary_emotions': detailed.secondary_emotions
            }
        return None
    
    def _emotion_to_chinese(self, emotion: str) -> str:
        """å°†è‹±æ–‡æƒ…ç»ªè½¬æ¢ä¸ºä¸­æ–‡"""
        mapping = {
            'anger': 'æ„¤æ€’',
            'fear': 'ææƒ§/ç„¦è™‘',
            'disgust': 'åŒæ¶',
            'sadness': 'æ‚²ä¼¤',
            'amusement': 'æ„‰æ‚¦',
            'joy': 'å–œæ‚¦',
            'inspiration': 'çµæ„Ÿ/æ¿€åŠ±',
            'tenderness': 'æ¸©æŸ”',
            'neutral': 'ä¸­æ€§'
        }
        return mapping.get(emotion, emotion)
    
    def generate_sota_music(self, 
                          emotion_state, 
                          stage_info: Dict, 
                          duration_seconds: float = 60,
                          original_method=None) -> Tuple[Optional[np.ndarray], Dict]:
        """
        ä½¿ç”¨SOTAæ¨¡å‹ç”Ÿæˆé«˜è´¨é‡éŸ³ä¹
        
        Args:
            emotion_state: æƒ…ç»ªçŠ¶æ€
            stage_info: æ²»ç–—é˜¶æ®µä¿¡æ¯
            duration_seconds: éŸ³ä¹æ—¶é•¿ï¼ˆç§’ï¼‰
            original_method: åŸå§‹éŸ³ä¹ç”Ÿæˆæ–¹æ³•ï¼ˆå›é€€ç”¨ï¼‰
            
        Returns:
            (audio_data, metadata): éŸ³é¢‘æ•°æ®å’Œå…ƒæ•°æ®
        """
        if not self.use_sota_music_generation or not self.musicgen_adapter:
            # å¦‚æœæœªå¯ç”¨æˆ–ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸå§‹æ–¹æ³•
            if original_method:
                return original_method(duration_seconds, stage_info), {}
            else:
                return None, {'error': 'SOTA music generation not available'}
        
        try:
            # å‡†å¤‡æƒ…ç»ªçŠ¶æ€ä¿¡æ¯
            emotion_dict = {
                'valence': emotion_state.valence,
                'arousal': emotion_state.arousal,
                'primary_emotion': getattr(emotion_state, '_primary_emotion', 'neutral')
            }
            
            # è®¡ç®—ç›®æ ‡BPM
            bpm_target = None
            if hasattr(self, 'music_mapper') and self.music_mapper:
                music_params = self.music_mapper.get_music_params(
                    emotion_state.valence, emotion_state.arousal
                )
                bpm_target = music_params.get('bpm')
            
            # ä½¿ç”¨MusicGenç”ŸæˆéŸ³ä¹
            print(f"\n{'='*60}")
            print(f"ğŸ¼ [SOTAéŸ³ä¹ç”Ÿæˆ v1.0] MusicGené«˜è´¨é‡ç”Ÿæˆ:")
            print(f"{'='*60}")
            print(f"  æƒ…ç»ªçŠ¶æ€: V={emotion_dict['valence']:.2f}, A={emotion_dict['arousal']:.2f}")
            print(f"  ä¸»è¦æƒ…ç»ª: {emotion_dict['primary_emotion']}")
            print(f"  æ²»ç–—é˜¶æ®µ: {stage_info.get('stage', 'unknown')}")
            print(f"  ç›®æ ‡æ—¶é•¿: {duration_seconds}ç§’")
            if bpm_target:
                print(f"  ç›®æ ‡BPM: {bpm_target}")
            print(f"{'='*60}")
            
            audio_data, metadata = self.musicgen_adapter.generate_therapeutic_music(
                emotion_state=emotion_dict,
                stage_info=stage_info,
                duration_seconds=duration_seconds,
                bpm_target=bpm_target
            )
            
            # è´¨é‡è¯„ä¼°
            if self.music_quality_evaluator and audio_data is not None:
                quality_metrics = self.music_quality_evaluator.evaluate_music_quality(
                    audio_data, metadata, therapy_context=stage_info
                )
                metadata['quality_metrics'] = quality_metrics
                
                print(f"\nğŸ† è´¨é‡è¯„ä¼°ç»“æœ:")
                print(f"  æŠ€æœ¯è´¨é‡: {quality_metrics.technical_score:.2f}/1.0")
                print(f"  æ²»ç–—æ•ˆæœ: {quality_metrics.therapeutic_score:.2f}/1.0")
                print(f"  ç»¼åˆè¯„åˆ†: {quality_metrics.overall_score:.2f}/1.0")
                
                if quality_metrics.warnings:
                    print(f"  âš ï¸ è´¨é‡è­¦å‘Š: {len(quality_metrics.warnings)}ä¸ª")
                if quality_metrics.recommendations:
                    print(f"  ğŸ’¡ æ”¹è¿›å»ºè®®: {len(quality_metrics.recommendations)}ä¸ª")
                print(f"{'='*60}")
            
            return audio_data, metadata
            
        except Exception as e:
            logger.error(f"SOTAéŸ³ä¹ç”Ÿæˆå¤±è´¥: {e}")
            if self.fallback_to_original and original_method:
                logger.info("å›é€€åˆ°åŸå§‹éŸ³ä¹ç”Ÿæˆæ–¹æ³•")
                return original_method(duration_seconds, stage_info), {'fallback': True}
            else:
                return None, {'error': str(e)}


def integrate_enhanced_modules(mood_flow_app_instance, config: Optional[Dict] = None):
    """
    å°†å¢å¼ºæ¨¡å—é›†æˆåˆ°ç°æœ‰çš„MoodFlowAppå®ä¾‹
    
    è¿™æ˜¯ä¸€ä¸ªä¾¿æ·å‡½æ•°ï¼Œç”¨äºä¿®æ”¹ç°æœ‰å®ä¾‹ä»¥ä½¿ç”¨å¢å¼ºåŠŸèƒ½
    
    Args:
        mood_flow_app_instance: MoodFlowAppå®ä¾‹
        config: é…ç½®é€‰é¡¹
    """
    # é»˜è®¤é…ç½®
    if config is None:
        config = {
            'use_enhanced_emotion': True,
            'use_enhanced_planning': True,
            'use_enhanced_mapping': True,
            'fallback_to_original': True
        }
    
    # åˆ›å»ºé€‚é…å™¨
    adapter = EnhancedMoodFlowAdapter(**config)
    
    # ä¿å­˜åŸå§‹æ–¹æ³•
    original_analyze = mood_flow_app_instance.analyze_emotion_from_text
    original_plan = mood_flow_app_instance.plan_therapy_stages
    original_music_model = mood_flow_app_instance.music_model
    
    # æ›¿æ¢æ–¹æ³•ï¼ˆä½¿ç”¨é—­åŒ…ä¿æŒåŸå§‹æ–¹æ³•å¼•ç”¨ï¼‰
    def enhanced_analyze(text):
        return adapter.analyze_emotion_enhanced(text, lambda t: original_analyze(t))
    
    def enhanced_plan(current_emotion, duration=20):
        # ç›®æ ‡æƒ…ç»ªï¼šå¹³é™å…¥ç¡çŠ¶æ€ï¼ˆä¸åŸå§‹å®ç°ä¿æŒä¸€è‡´ï¼‰
        target_emotion = type('EmotionState', (), {'valence': 0.3, 'arousal': -0.8})()
        return adapter.plan_therapy_stages_enhanced(
            current_emotion, target_emotion, duration, mood_flow_app_instance.iso_model
        )
    
    # åº”ç”¨å¢å¼º
    mood_flow_app_instance.analyze_emotion_from_text = enhanced_analyze
    mood_flow_app_instance.plan_therapy_stages = enhanced_plan
    
    # æ·»åŠ å¢å¼ºçŠ¶æ€æŸ¥è¯¢æ–¹æ³•
    mood_flow_app_instance.get_enhancement_status = adapter.get_enhancement_status
    mood_flow_app_instance.get_detailed_emotion_info = adapter.get_detailed_emotion_info
    
    # æ·»åŠ éŸ³ä¹ç”Ÿæˆå¢å¼º
    if hasattr(mood_flow_app_instance, '_generate_simple_music'):
        original_generate = mood_flow_app_instance._generate_simple_music
        
        def enhanced_generate(duration_seconds, bpm, key, stage_index):
            # å¦‚æœå¯ç”¨äº†SOTAéŸ³ä¹ç”Ÿæˆï¼Œä½¿ç”¨MusicGen
            if adapter.use_sota_music_generation and adapter.musicgen_adapter:
                try:
                    # è·å–å½“å‰æƒ…ç»ªçŠ¶æ€å’Œé˜¶æ®µä¿¡æ¯
                    if hasattr(mood_flow_app_instance, 'current_session') and mood_flow_app_instance.current_session:
                        emotion = mood_flow_app_instance.current_session.iso_stages[stage_index]['emotion']
                        stage_info = {
                            'stage_name': mood_flow_app_instance.current_session.iso_stages[stage_index]['stage'].value,
                            'stage_index': stage_index,
                            'therapy_goal': 'sleep_therapy'
                        }
                        
                        # ä½¿ç”¨MusicGenç”ŸæˆéŸ³ä¹
                        audio_data, metadata = adapter.generate_sota_music(
                            emotion, stage_info, duration_seconds
                        )
                        
                        if audio_data is not None:
                            print(f"ğŸ¼ [SOTAç”Ÿæˆ] é˜¶æ®µ{stage_index+1}éŸ³ä¹ç”ŸæˆæˆåŠŸ: {len(audio_data)}æ ·æœ¬")
                            return audio_data
                        else:
                            print(f"âš ï¸ [SOTAç”Ÿæˆ] é˜¶æ®µ{stage_index+1}ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€æ–¹æ³•")
                    
                except Exception as e:
                    print(f"âš ï¸ [SOTAç”Ÿæˆ] å‡ºé”™ï¼Œå›é€€åˆ°åŸºç¡€æ–¹æ³•: {e}")
            
            # å›é€€åˆ°å¢å¼ºçš„åŸºç¡€æ–¹æ³•
            if hasattr(mood_flow_app_instance, 'current_session'):
                emotion = mood_flow_app_instance.current_session.iso_stages[stage_index]['emotion']
                params = adapter.get_music_parameters_enhanced(emotion, 
                                                              mood_flow_app_instance.current_session.iso_stages[stage_index],
                                                              original_music_model)
                # ä½¿ç”¨å¢å¼ºå‚æ•°
                bpm = params.get('bpm', bpm)
                key = params.get('key', key)
                if isinstance(key, str) and ' ' in key:
                    key = key.split()[0]  # æå–éŸ³ç¬¦éƒ¨åˆ†
            
            return original_generate(duration_seconds, bpm, key, stage_index)
        
        mood_flow_app_instance._generate_simple_music = enhanced_generate
    
    logger.info("âœ… å¢å¼ºæ¨¡å—é›†æˆå®Œæˆ")
    logger.info(f"å¢å¼ºçŠ¶æ€: {adapter.get_enhancement_status()}")
    
    return adapter


# é…ç½®é¢„è®¾
ENHANCEMENT_CONFIGS = {
    'full': {
        'use_enhanced_emotion': True,
        'use_enhanced_planning': True,
        'use_enhanced_mapping': True,
        'use_sota_music_generation': False,  # é»˜è®¤å…³é—­SOTAæ¨¡å‹
        'fallback_to_original': True
    },
    'full_with_sota': {
        'use_enhanced_emotion': True,
        'use_enhanced_planning': True,
        'use_enhanced_mapping': True,
        'use_sota_music_generation': True,   # å¯ç”¨SOTAéŸ³ä¹ç”Ÿæˆ
        'fallback_to_original': True
    },
    'sota_only': {
        'use_enhanced_emotion': False,
        'use_enhanced_planning': False,
        'use_enhanced_mapping': False,
        'use_sota_music_generation': True,   # ä»…SOTAéŸ³ä¹ç”Ÿæˆ
        'fallback_to_original': True
    },
    'emotion_only': {
        'use_enhanced_emotion': True,
        'use_enhanced_planning': False,
        'use_enhanced_mapping': False,
        'use_sota_music_generation': False,
        'fallback_to_original': True
    },
    'planning_only': {
        'use_enhanced_emotion': False,
        'use_enhanced_planning': True,
        'use_enhanced_mapping': False,
        'use_sota_music_generation': False,
        'fallback_to_original': True
    },
    'mapping_only': {
        'use_enhanced_emotion': False,
        'use_enhanced_planning': False,
        'use_enhanced_mapping': True,
        'use_sota_music_generation': False,
        'fallback_to_original': True
    },
    'disabled': {
        'use_enhanced_emotion': False,
        'use_enhanced_planning': False,
        'use_enhanced_mapping': False,
        'use_sota_music_generation': False,
        'fallback_to_original': True
    }
}