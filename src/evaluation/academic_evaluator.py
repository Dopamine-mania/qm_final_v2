"""
ã€Šå¿ƒå¢ƒæµè½¬ã€‹å­¦æœ¯è¯„ä¼°ç³»ç»Ÿ
Academic Evaluation System for Mood Transitions

æä¾›ç§‘å­¦ä¸¥è°¨çš„å­¦æœ¯è¯„ä¼°ä½“ç³»
- æ²»ç–—æ•ˆæœé‡åŒ–è¯„ä¼°
- å­¦æœ¯æŒ‡æ ‡è®¡ç®—
- ç§‘å­¦éªŒè¯æ–¹æ³•
- è®ºæ–‡æ”¯æ’‘æ•°æ®ç”Ÿæˆ
"""

import numpy as np
import pandas as pd
from scipy import stats
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
import json
import logging
from pathlib import Path

@dataclass
class EvaluationMetrics:
    """è¯„ä¼°æŒ‡æ ‡"""
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    auc_score: float
    mse: float
    correlation: float
    significance: float

@dataclass
class TherapyEffectiveness:
    """æ²»ç–—æ•ˆæœè¯„ä¼°"""
    emotion_improvement: float  # æƒ…ç»ªæ”¹å–„åº¦
    sleep_quality_improvement: float  # ç¡çœ è´¨é‡æ”¹å–„
    stress_reduction: float  # å‹åŠ›å‡å°‘
    user_satisfaction: float  # ç”¨æˆ·æ»¡æ„åº¦
    treatment_adherence: float  # æ²»ç–—ä¾ä»æ€§
    long_term_efficacy: float  # é•¿æœŸç–—æ•ˆ

@dataclass
class AcademicValidation:
    """å­¦æœ¯éªŒè¯ç»“æœ"""
    statistical_significance: bool
    effect_size: float
    confidence_interval: Tuple[float, float]
    p_value: float
    sample_size: int
    power_analysis: float

class AcademicEvaluator:
    """å­¦æœ¯è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.evaluation_history = []
        self.baseline_metrics = None
        
        # å­¦æœ¯æ ‡å‡†é˜ˆå€¼
        self.significance_threshold = 0.05
        self.effect_size_threshold = 0.5  # Cohen's d
        self.minimum_sample_size = 30
        
    def evaluate_emotion_recognition_accuracy(self, 
                                            predicted_emotions: List[Dict], 
                                            ground_truth_emotions: List[Dict]) -> EvaluationMetrics:
        """è¯„ä¼°æƒ…ç»ªè¯†åˆ«å‡†ç¡®æ€§"""
        
        # è½¬æ¢ä¸ºæ•°å€¼æ•°æ®
        pred_valence = [e['valence'] for e in predicted_emotions]
        pred_arousal = [e['arousal'] for e in predicted_emotions]
        true_valence = [e['valence'] for e in ground_truth_emotions]
        true_arousal = [e['arousal'] for e in ground_truth_emotions]
        
        # è®¡ç®—ç›¸å…³æ€§
        valence_corr = stats.pearsonr(pred_valence, true_valence)[0]
        arousal_corr = stats.pearsonr(pred_arousal, true_arousal)[0]
        overall_corr = (valence_corr + arousal_corr) / 2
        
        # è®¡ç®—MSE
        valence_mse = np.mean((np.array(pred_valence) - np.array(true_valence))**2)
        arousal_mse = np.mean((np.array(pred_arousal) - np.array(true_arousal))**2)
        overall_mse = (valence_mse + arousal_mse) / 2
        
        # åˆ†ç±»å‡†ç¡®æ€§ï¼ˆå››è±¡é™åˆ†ç±»ï¼‰
        pred_categories = [self._emotion_to_category(e) for e in predicted_emotions]
        true_categories = [self._emotion_to_category(e) for e in ground_truth_emotions]
        
        accuracy = accuracy_score(true_categories, pred_categories)
        precision = precision_score(true_categories, pred_categories, average='weighted')
        recall = recall_score(true_categories, pred_categories, average='weighted')
        f1 = f1_score(true_categories, pred_categories, average='weighted')
        
        # æ˜¾è‘—æ€§æ£€éªŒ
        _, p_value = stats.ttest_rel(pred_valence, true_valence)
        
        return EvaluationMetrics(
            accuracy=accuracy,
            precision=precision,
            recall=recall,
            f1_score=f1,
            auc_score=0.0,  # éœ€è¦ROCæ›²çº¿è®¡ç®—
            mse=overall_mse,
            correlation=overall_corr,
            significance=p_value
        )
    
    def _emotion_to_category(self, emotion: Dict) -> str:
        """å°†æƒ…ç»ªåæ ‡è½¬æ¢ä¸ºç±»åˆ«"""
        valence = emotion['valence']
        arousal = emotion['arousal']
        
        if valence > 0 and arousal > 0:
            return "high_positive"  # å…´å¥‹æ„‰æ‚¦
        elif valence > 0 and arousal <= 0:
            return "low_positive"   # å¹³é™æ»¡è¶³
        elif valence <= 0 and arousal > 0:
            return "high_negative"  # ç„¦è™‘ç´§å¼ 
        else:
            return "low_negative"   # æŠ‘éƒä½è½
    
    def evaluate_therapy_effectiveness(self, 
                                     pre_treatment_data: List[Dict],
                                     post_treatment_data: List[Dict],
                                     follow_up_data: Optional[List[Dict]] = None) -> TherapyEffectiveness:
        """è¯„ä¼°æ²»ç–—æ•ˆæœ"""
        
        # è®¡ç®—æƒ…ç»ªæ”¹å–„
        emotion_improvement = self._calculate_emotion_improvement(
            pre_treatment_data, post_treatment_data
        )
        
        # è®¡ç®—ç¡çœ è´¨é‡æ”¹å–„
        sleep_improvement = self._calculate_sleep_improvement(
            pre_treatment_data, post_treatment_data
        )
        
        # è®¡ç®—å‹åŠ›å‡å°‘
        stress_reduction = self._calculate_stress_reduction(
            pre_treatment_data, post_treatment_data
        )
        
        # ç”¨æˆ·æ»¡æ„åº¦ï¼ˆä»åé¦ˆæ•°æ®ä¸­æå–ï¼‰
        satisfaction_scores = [d.get('satisfaction', 3.0) for d in post_treatment_data]
        user_satisfaction = np.mean(satisfaction_scores) / 5.0  # æ ‡å‡†åŒ–åˆ°0-1
        
        # æ²»ç–—ä¾ä»æ€§
        completed_sessions = sum(1 for d in post_treatment_data if d.get('completed', False))
        adherence = completed_sessions / len(post_treatment_data) if post_treatment_data else 0
        
        # é•¿æœŸç–—æ•ˆï¼ˆå¦‚æœæœ‰éšè®¿æ•°æ®ï¼‰
        long_term_efficacy = 0.0
        if follow_up_data:
            long_term_efficacy = self._calculate_long_term_efficacy(
                pre_treatment_data, follow_up_data
            )
        
        return TherapyEffectiveness(
            emotion_improvement=emotion_improvement,
            sleep_quality_improvement=sleep_improvement,
            stress_reduction=stress_reduction,
            user_satisfaction=user_satisfaction,
            treatment_adherence=adherence,
            long_term_efficacy=long_term_efficacy
        )
    
    def _calculate_emotion_improvement(self, pre_data: List[Dict], post_data: List[Dict]) -> float:
        """è®¡ç®—æƒ…ç»ªæ”¹å–„åº¦"""
        # è®¡ç®—æ²»ç–—å‰åçš„æƒ…ç»ªè·ç¦»æ”¹å–„
        pre_emotions = [d.get('emotion_state', {}) for d in pre_data]
        post_emotions = [d.get('emotion_state', {}) for d in post_data]
        
        # ç›®æ ‡æƒ…ç»ªçŠ¶æ€ï¼ˆå¹³é™æ„‰æ‚¦ï¼švalence=0.3, arousal=-0.3ï¼‰
        target_emotion = {'valence': 0.3, 'arousal': -0.3}
        
        # è®¡ç®—æ²»ç–—å‰åä¸ç›®æ ‡çŠ¶æ€çš„è·ç¦»
        pre_distances = [
            np.sqrt((e.get('valence', 0) - target_emotion['valence'])**2 + 
                   (e.get('arousal', 0) - target_emotion['arousal'])**2)
            for e in pre_emotions
        ]
        
        post_distances = [
            np.sqrt((e.get('valence', 0) - target_emotion['valence'])**2 + 
                   (e.get('arousal', 0) - target_emotion['arousal'])**2)
            for e in post_emotions
        ]
        
        pre_avg_distance = np.mean(pre_distances) if pre_distances else 1.0
        post_avg_distance = np.mean(post_distances) if post_distances else 1.0
        
        # æ”¹å–„åº¦ = (æ²»ç–—å‰è·ç¦» - æ²»ç–—åè·ç¦») / æ²»ç–—å‰è·ç¦»
        improvement = (pre_avg_distance - post_avg_distance) / pre_avg_distance
        return max(0, min(1, improvement))  # é™åˆ¶åœ¨0-1èŒƒå›´
    
    def _calculate_sleep_improvement(self, pre_data: List[Dict], post_data: List[Dict]) -> float:
        """è®¡ç®—ç¡çœ è´¨é‡æ”¹å–„"""
        pre_sleep_scores = [d.get('sleep_quality', 3.0) for d in pre_data]
        post_sleep_scores = [d.get('sleep_quality', 3.0) for d in post_data]
        
        pre_avg = np.mean(pre_sleep_scores) if pre_sleep_scores else 3.0
        post_avg = np.mean(post_sleep_scores) if post_sleep_scores else 3.0
        
        # ç¡çœ è´¨é‡æ”¹å–„ï¼ˆå‡è®¾1-5åˆ†é‡è¡¨ï¼‰
        improvement = (post_avg - pre_avg) / 4.0  # æœ€å¤§æ”¹å–„ä¸º4åˆ†
        return max(0, min(1, improvement))
    
    def _calculate_stress_reduction(self, pre_data: List[Dict], post_data: List[Dict]) -> float:
        """è®¡ç®—å‹åŠ›å‡å°‘"""
        pre_stress_levels = [d.get('stress_level', 0.5) for d in pre_data]
        post_stress_levels = [d.get('stress_level', 0.5) for d in post_data]
        
        pre_avg = np.mean(pre_stress_levels) if pre_stress_levels else 0.5
        post_avg = np.mean(post_stress_levels) if post_stress_levels else 0.5
        
        # å‹åŠ›å‡å°‘åº¦
        reduction = (pre_avg - post_avg) / pre_avg if pre_avg > 0 else 0
        return max(0, min(1, reduction))
    
    def _calculate_long_term_efficacy(self, pre_data: List[Dict], follow_up_data: List[Dict]) -> float:
        """è®¡ç®—é•¿æœŸç–—æ•ˆ"""
        # ä¸çŸ­æœŸæ•ˆæœè®¡ç®—ç±»ä¼¼ï¼Œä½†ä½¿ç”¨éšè®¿æ•°æ®
        return self._calculate_emotion_improvement(pre_data, follow_up_data)
    
    def statistical_validation(self, 
                             control_group_data: List[Dict],
                             treatment_group_data: List[Dict],
                             outcome_measure: str = 'emotion_improvement') -> AcademicValidation:
        """ç»Ÿè®¡å­¦éªŒè¯"""
        
        # æå–ç»“æœå˜é‡
        control_outcomes = [d.get(outcome_measure, 0.0) for d in control_group_data]
        treatment_outcomes = [d.get(outcome_measure, 0.0) for d in treatment_group_data]
        
        # tæ£€éªŒ
        t_stat, p_value = stats.ttest_ind(treatment_outcomes, control_outcomes)
        
        # æ•ˆåº”é‡è®¡ç®— (Cohen's d)
        pooled_std = np.sqrt(((len(control_outcomes) - 1) * np.var(control_outcomes, ddof=1) + 
                             (len(treatment_outcomes) - 1) * np.var(treatment_outcomes, ddof=1)) / 
                            (len(control_outcomes) + len(treatment_outcomes) - 2))
        
        effect_size = (np.mean(treatment_outcomes) - np.mean(control_outcomes)) / pooled_std
        
        # ç½®ä¿¡åŒºé—´
        se = pooled_std * np.sqrt(1/len(control_outcomes) + 1/len(treatment_outcomes))
        df = len(control_outcomes) + len(treatment_outcomes) - 2
        t_critical = stats.t.ppf(0.975, df)  # 95% CI
        
        mean_diff = np.mean(treatment_outcomes) - np.mean(control_outcomes)
        ci_lower = mean_diff - t_critical * se
        ci_upper = mean_diff + t_critical * se
        
        # åŠŸæ•ˆåˆ†æ
        power = self._calculate_statistical_power(
            effect_size, len(control_outcomes) + len(treatment_outcomes), 0.05
        )
        
        return AcademicValidation(
            statistical_significance=p_value < self.significance_threshold,
            effect_size=effect_size,
            confidence_interval=(ci_lower, ci_upper),
            p_value=p_value,
            sample_size=len(control_outcomes) + len(treatment_outcomes),
            power_analysis=power
        )
    
    def _calculate_statistical_power(self, effect_size: float, sample_size: int, alpha: float) -> float:
        """è®¡ç®—ç»Ÿè®¡åŠŸæ•ˆ"""
        # ç®€åŒ–çš„åŠŸæ•ˆè®¡ç®—ï¼ˆå®é™…åº”ä½¿ç”¨ä¸“é—¨çš„ç»Ÿè®¡åŒ…ï¼‰
        z_alpha = stats.norm.ppf(1 - alpha/2)
        z_beta = effect_size * np.sqrt(sample_size/4) - z_alpha
        power = stats.norm.cdf(z_beta)
        return max(0, min(1, power))
    
    def generate_academic_report(self, 
                                evaluation_results: Dict[str, Any],
                                study_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå­¦æœ¯æŠ¥å‘Š"""
        
        report = {
            'study_information': {
                'title': 'ã€Šå¿ƒå¢ƒæµè½¬ã€‹ç¡çœ å¯¼å‘éŸ³è§†è§‰æ²»ç–—ç³»ç»Ÿæ•ˆæœè¯„ä¼°',
                'date': datetime.now().isoformat(),
                'sample_size': study_metadata.get('sample_size', 0),
                'study_duration': study_metadata.get('duration_weeks', 0),
                'methodology': 'éšæœºå¯¹ç…§è¯•éªŒ (RCT)'
            },
            
            'primary_outcomes': {},
            'secondary_outcomes': {},
            'statistical_analysis': {},
            'clinical_significance': {},
            'limitations': [],
            'conclusions': [],
            'recommendations': []
        }
        
        # å¡«å……ä¸»è¦ç»“æœ
        if 'therapy_effectiveness' in evaluation_results:
            effectiveness = evaluation_results['therapy_effectiveness']
            report['primary_outcomes'] = {
                'emotion_improvement': {
                    'value': effectiveness.emotion_improvement,
                    'interpretation': self._interpret_effect_size(effectiveness.emotion_improvement)
                },
                'sleep_quality_improvement': {
                    'value': effectiveness.sleep_quality_improvement,
                    'interpretation': self._interpret_effect_size(effectiveness.sleep_quality_improvement)
                }
            }
        
        # å¡«å……ç»Ÿè®¡åˆ†æ
        if 'statistical_validation' in evaluation_results:
            validation = evaluation_results['statistical_validation']
            report['statistical_analysis'] = {
                'significance': validation.statistical_significance,
                'p_value': validation.p_value,
                'effect_size': validation.effect_size,
                'confidence_interval': validation.confidence_interval,
                'statistical_power': validation.power_analysis
            }
        
        # ç”Ÿæˆç»“è®º
        report['conclusions'] = self._generate_conclusions(evaluation_results)
        
        # ç”Ÿæˆå»ºè®®
        report['recommendations'] = self._generate_recommendations(evaluation_results)
        
        return report
    
    def _interpret_effect_size(self, effect_size: float) -> str:
        """è§£é‡Šæ•ˆåº”é‡"""
        if effect_size < 0.2:
            return "å¾®å°æ•ˆåº”"
        elif effect_size < 0.5:
            return "å°æ•ˆåº”"
        elif effect_size < 0.8:
            return "ä¸­ç­‰æ•ˆåº”"
        else:
            return "å¤§æ•ˆåº”"
    
    def _generate_conclusions(self, evaluation_results: Dict) -> List[str]:
        """ç”Ÿæˆç ”ç©¶ç»“è®º"""
        conclusions = []
        
        # åŸºäºç»Ÿè®¡æ˜¾è‘—æ€§
        if evaluation_results.get('statistical_validation', {}).get('statistical_significance', False):
            conclusions.append("æ²»ç–—å¹²é¢„æ˜¾ç¤ºå‡ºç»Ÿè®¡å­¦æ˜¾è‘—çš„æ²»ç–—æ•ˆæœ")
        
        # åŸºäºæ•ˆåº”é‡
        effect_size = evaluation_results.get('statistical_validation', {}).get('effect_size', 0)
        if effect_size > 0.5:
            conclusions.append("æ²»ç–—æ•ˆæœå…·æœ‰ä¸´åºŠæ„ä¹‰")
        
        # åŸºäºç”¨æˆ·æ»¡æ„åº¦
        satisfaction = evaluation_results.get('therapy_effectiveness', {}).get('user_satisfaction', 0)
        if satisfaction > 0.7:
            conclusions.append("ç”¨æˆ·å¯¹æ²»ç–—ç³»ç»Ÿè¡¨ç°å‡ºé«˜æ»¡æ„åº¦")
        
        return conclusions
    
    def _generate_recommendations(self, evaluation_results: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åŸºäºä¾ä»æ€§
        adherence = evaluation_results.get('therapy_effectiveness', {}).get('treatment_adherence', 0)
        if adherence < 0.8:
            recommendations.append("éœ€è¦æ”¹è¿›æ²»ç–—ä¾ä»æ€§ç­–ç•¥")
        
        # åŸºäºé•¿æœŸæ•ˆæœ
        long_term = evaluation_results.get('therapy_effectiveness', {}).get('long_term_efficacy', 0)
        if long_term < 0.6:
            recommendations.append("éœ€è¦åŠ å¼ºé•¿æœŸæ•ˆæœç»´æŒæœºåˆ¶")
        
        # åŸºäºæ ·æœ¬é‡
        sample_size = evaluation_results.get('statistical_validation', {}).get('sample_size', 0)
        if sample_size < 100:
            recommendations.append("å»ºè®®æ‰©å¤§æ ·æœ¬é‡ä»¥æé«˜ç ”ç©¶åŠŸæ•ˆ")
        
        return recommendations
    
    def export_results_for_publication(self, 
                                     evaluation_results: Dict,
                                     output_path: str = "academic_results.json"):
        """å¯¼å‡ºå­¦æœ¯å‘è¡¨ç”¨ç»“æœ"""
        
        publication_data = {
            'metadata': {
                'system_name': 'ã€Šå¿ƒå¢ƒæµè½¬ã€‹',
                'evaluation_date': datetime.now().isoformat(),
                'academic_level': 'ç¡•å£«å­¦ä½è®ºæ–‡',
                'field': 'äººå·¥æ™ºèƒ½ + å¿ƒç†å¥åº·'
            },
            
            'methodology': {
                'approach': 'å¤šæ¨¡æ€æ·±åº¦å­¦ä¹  + éŸ³ä¹æ²»ç–—ç†è®º',
                'technical_framework': 'ISOä¸‰é˜¶æ®µæ²»ç–—åŸåˆ™',
                'emotion_model': 'Valence-ArousaläºŒç»´æƒ…ç»ªæ¨¡å‹',
                'evaluation_metrics': 'å¤šç»´åº¦é‡åŒ–è¯„ä¼°ä½“ç³»'
            },
            
            'results': evaluation_results,
            
            'academic_contributions': [
                'é¦–åˆ›ç¡çœ å¯¼å‘çš„å¤šæ¨¡æ€æ²»ç–—æ–¹æ³•',
                'å°†éŸ³ä¹æ²»ç–—ç†è®ºä¸AIæŠ€æœ¯æ·±åº¦èåˆ', 
                'å»ºç«‹äº†æƒ…ç»ªè¯†åˆ«åˆ°æ²»ç–—å†…å®¹ç”Ÿæˆçš„å®Œæ•´é“¾è·¯',
                'æä¾›äº†é‡åŒ–çš„æ²»ç–—æ•ˆæœè¯„ä¼°ä½“ç³»'
            ],
            
            'innovation_points': [
                'å¤šæ„Ÿå®˜ååŒæ²»ç–—æœºåˆ¶',
                'ä¸ªæ€§åŒ–æƒ…ç»ªè½¨è¿¹è§„åˆ’',
                'å®æ—¶æ²»ç–—æ•ˆæœç›‘æµ‹',
                'ç§‘å­¦ç†è®ºæŒ‡å¯¼çš„AIåº”ç”¨'
            ]
        }
        
        # ä¿å­˜ç»“æœ
        output_file = Path(output_path)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(publication_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"å­¦æœ¯è¯„ä¼°ç»“æœå·²å¯¼å‡ºè‡³: {output_file}")
        
        return publication_data

if __name__ == "__main__":
    # æµ‹è¯•å­¦æœ¯è¯„ä¼°å™¨
    evaluator = AcademicEvaluator()
    
    # æ¨¡æ‹Ÿæ•°æ®
    predicted_emotions = [
        {'valence': 0.2, 'arousal': -0.1},
        {'valence': 0.1, 'arousal': -0.2},
        {'valence': 0.3, 'arousal': -0.3}
    ]
    
    ground_truth_emotions = [
        {'valence': 0.25, 'arousal': -0.05},
        {'valence': 0.15, 'arousal': -0.15},
        {'valence': 0.28, 'arousal': -0.25}
    ]
    
    # è¯„ä¼°æƒ…ç»ªè¯†åˆ«å‡†ç¡®æ€§
    metrics = evaluator.evaluate_emotion_recognition_accuracy(
        predicted_emotions, ground_truth_emotions
    )
    
    print("ğŸ“Š æƒ…ç»ªè¯†åˆ«è¯„ä¼°ç»“æœ:")
    print(f"   å‡†ç¡®ç‡: {metrics.accuracy:.3f}")
    print(f"   ç›¸å…³æ€§: {metrics.correlation:.3f}")
    print(f"   MSE: {metrics.mse:.3f}")
    
    # ç”Ÿæˆå­¦æœ¯æŠ¥å‘Š
    evaluation_results = {
        'emotion_recognition_metrics': metrics,
        'therapy_effectiveness': TherapyEffectiveness(
            emotion_improvement=0.65,
            sleep_quality_improvement=0.58,
            stress_reduction=0.72,
            user_satisfaction=0.84,
            treatment_adherence=0.76,
            long_term_efficacy=0.61
        )
    }
    
    report = evaluator.generate_academic_report(
        evaluation_results,
        {'sample_size': 50, 'duration_weeks': 4}
    )
    
    print("\nğŸ“ å­¦æœ¯è¯„ä¼°æŠ¥å‘Šå·²ç”Ÿæˆ")
    print(f"ç ”ç©¶ç»“è®ºæ•°é‡: {len(report['conclusions'])}")
    print(f"æ”¹è¿›å»ºè®®æ•°é‡: {len(report['recommendations'])}")