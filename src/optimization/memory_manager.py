"""
ã€Šå¿ƒå¢ƒæµè½¬ã€‹æ˜¾å­˜ä¼˜åŒ–ç®¡ç†å™¨
GPU Memory Optimization Manager for Mood Transitions System

ä¸“ä¸ºJupyterHubç¯å¢ƒè®¾è®¡çš„æ™ºèƒ½æ˜¾å­˜ç®¡ç†ç³»ç»Ÿ
- åŠ¨æ€æ˜¾å­˜åˆ†é…å’Œé‡Šæ”¾
- CPU-GPUæ··åˆè®¡ç®—
- æ¨¡å‹åˆ†ç‰‡å’Œæµæ°´çº¿ä¼˜åŒ–
- å®æ—¶æ€§èƒ½ç›‘æ§
"""

import torch
import psutil
import gc
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import threading
import time
from contextlib import contextmanager
import logging

@dataclass
class MemoryConfig:
    """æ˜¾å­˜é…ç½®"""
    max_gpu_memory_gb: float = 70.0  # æœ€å¤§GPUæ˜¾å­˜ä½¿ç”¨(GB)
    cpu_offload_threshold: float = 0.8  # CPUåˆ†æµé˜ˆå€¼
    mixed_precision: bool = True  # æ··åˆç²¾åº¦
    gradient_checkpointing: bool = True  # æ¢¯åº¦æ£€æŸ¥ç‚¹
    model_parallel: bool = False  # æ¨¡å‹å¹¶è¡Œ
    cache_size_mb: int = 2048  # ç¼“å­˜å¤§å°(MB)

class GPUMemoryMonitor:
    """GPUæ˜¾å­˜ç›‘æ§å™¨"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.total_memory = self._get_total_memory()
        self.monitoring = False
        self.memory_log = []
        
    def _get_total_memory(self) -> float:
        """è·å–æ€»æ˜¾å­˜"""
        if torch.cuda.is_available():
            return torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
        return 0.0
    
    def get_current_usage(self) -> Dict[str, float]:
        """è·å–å½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
        if not torch.cuda.is_available():
            return {"allocated": 0.0, "cached": 0.0, "free": 0.0, "total": 0.0}
            
        allocated = torch.cuda.memory_allocated() / (1024**3)
        cached = torch.cuda.memory_reserved() / (1024**3)
        free = self.total_memory - cached
        
        return {
            "allocated": allocated,
            "cached": cached, 
            "free": free,
            "total": self.total_memory,
            "utilization": cached / self.total_memory if self.total_memory > 0 else 0.0
        }
    
    def start_monitoring(self, interval: float = 1.0):
        """å¼€å§‹æ˜¾å­˜ç›‘æ§"""
        if self.monitoring:
            return
            
        self.monitoring = True
        self.memory_log = []
        
        def monitor_loop():
            while self.monitoring:
                usage = self.get_current_usage()
                usage['timestamp'] = time.time()
                self.memory_log.append(usage)
                time.sleep(interval)
                
        self.monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
        self.monitor_thread.start()
    
    def stop_monitoring(self) -> List[Dict]:
        """åœæ­¢ç›‘æ§å¹¶è¿”å›æ—¥å¿—"""
        self.monitoring = False
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join()
        return self.memory_log

class MemoryOptimizer:
    """æ˜¾å­˜ä¼˜åŒ–å™¨"""
    
    def __init__(self, config: MemoryConfig):
        self.config = config
        self.monitor = GPUMemoryMonitor()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.cpu_models = {}  # CPUä¸Šçš„æ¨¡å‹
        self.gpu_models = {}  # GPUä¸Šçš„æ¨¡å‹
        self.model_metadata = {}  # æ¨¡å‹å…ƒæ•°æ®
        
        # è®¾ç½®æ··åˆç²¾åº¦
        if config.mixed_precision and torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
    
    @contextmanager
    def memory_management(self, model_name: str):
        """æ˜¾å­˜ç®¡ç†ä¸Šä¸‹æ–‡"""
        initial_usage = self.monitor.get_current_usage()
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡Šæ”¾æ˜¾å­˜
            if initial_usage['utilization'] > self.config.cpu_offload_threshold:
                self._optimize_memory()
            
            yield
            
        finally:
            # æ¸…ç†æ˜¾å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
    
    def load_model_optimized(self, model_class, model_name: str, 
                           model_config: Dict, force_cpu: bool = False) -> torch.nn.Module:
        """ä¼˜åŒ–åŠ è½½æ¨¡å‹"""
        
        # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        current_usage = self.monitor.get_current_usage()
        estimated_model_size = self._estimate_model_size(model_config)
        
        # å†³å®šåŠ è½½ä½ç½®
        if (force_cpu or 
            current_usage['free'] < estimated_model_size or
            current_usage['utilization'] > self.config.cpu_offload_threshold):
            
            device = torch.device("cpu")
            print(f"ğŸ”„ å°†æ¨¡å‹ {model_name} åŠ è½½åˆ°CPU (æ˜¾å­˜ä¸è¶³)")
        else:
            device = self.device
            print(f"âš¡ å°†æ¨¡å‹ {model_name} åŠ è½½åˆ°GPU")
        
        # åŠ è½½æ¨¡å‹
        with self.memory_management(model_name):
            if self.config.mixed_precision and device.type == "cuda":
                model = model_class(**model_config).half()  # FP16
            else:
                model = model_class(**model_config)
            
            model = model.to(device)
            model.eval()
            
            # è®°å½•æ¨¡å‹ä¿¡æ¯
            self.model_metadata[model_name] = {
                'device': device,
                'precision': 'fp16' if self.config.mixed_precision else 'fp32',
                'size_gb': estimated_model_size,
                'load_time': time.time()
            }
            
            if device.type == "cpu":
                self.cpu_models[model_name] = model
            else:
                self.gpu_models[model_name] = model
        
        return model
    
    def _estimate_model_size(self, model_config: Dict) -> float:
        """ä¼°ç®—æ¨¡å‹å¤§å° (GB)"""
        # åŸºäºæ¨¡å‹é…ç½®ä¼°ç®—å¤§å°
        if 'hidden_size' in model_config and 'num_layers' in model_config:
            hidden_size = model_config['hidden_size']
            num_layers = model_config['num_layers']
            # ç®€åŒ–ä¼°ç®—ï¼šå‚æ•°æ•°é‡ * 4å­—èŠ‚(fp32) æˆ– 2å­—èŠ‚(fp16)
            param_count = hidden_size * hidden_size * num_layers * 4  # ç®€åŒ–ä¼°ç®—
            bytes_per_param = 2 if self.config.mixed_precision else 4
            return (param_count * bytes_per_param) / (1024**3)
        
        # é»˜è®¤ä¼°ç®—
        return 1.0  # 1GB default
    
    def _optimize_memory(self):
        """ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨"""
        print("ğŸ§¹ å¼€å§‹æ˜¾å­˜ä¼˜åŒ–...")
        
        # 1. æ¸…ç†ç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        # 2. ç§»åŠ¨æ¨¡å‹åˆ°CPU
        current_usage = self.monitor.get_current_usage()
        if current_usage['utilization'] > self.config.cpu_offload_threshold:
            self._offload_models_to_cpu()
        
        # 3. å†æ¬¡æ¸…ç†
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        new_usage = self.monitor.get_current_usage()
        print(f"âœ… æ˜¾å­˜ä¼˜åŒ–å®Œæˆ: {current_usage['utilization']:.1%} â†’ {new_usage['utilization']:.1%}")
    
    def _offload_models_to_cpu(self):
        """å°†æ¨¡å‹åˆ†æµåˆ°CPU"""
        # æŒ‰åŠ è½½æ—¶é—´æ’åºï¼Œä¼˜å…ˆç§»åŠ¨è¾ƒæ—©åŠ è½½çš„æ¨¡å‹
        models_by_time = sorted(
            self.model_metadata.items(),
            key=lambda x: x[1]['load_time']
        )
        
        for model_name, metadata in models_by_time:
            if metadata['device'].type == "cuda" and model_name in self.gpu_models:
                print(f"ğŸ“¤ å°†æ¨¡å‹ {model_name} ç§»åŠ¨åˆ°CPU")
                
                model = self.gpu_models.pop(model_name)
                model = model.cpu()
                self.cpu_models[model_name] = model
                
                # æ›´æ–°å…ƒæ•°æ®
                self.model_metadata[model_name]['device'] = torch.device("cpu")
                
                # æ£€æŸ¥æ˜¯å¦å·²è¶³å¤Ÿ
                current_usage = self.monitor.get_current_usage()
                if current_usage['utilization'] < self.config.cpu_offload_threshold:
                    break
    
    def get_model(self, model_name: str) -> Optional[torch.nn.Module]:
        """è·å–æ¨¡å‹ï¼ˆè‡ªåŠ¨å¤„ç†è®¾å¤‡ä½ç½®ï¼‰"""
        if model_name in self.gpu_models:
            return self.gpu_models[model_name]
        elif model_name in self.cpu_models:
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥ç§»å›GPU
            current_usage = self.monitor.get_current_usage()
            model_size = self.model_metadata[model_name]['size_gb']
            
            if (current_usage['free'] > model_size and 
                current_usage['utilization'] < 0.6):  # ä½äº60%ä½¿ç”¨ç‡æ—¶ç§»å›GPU
                
                print(f"ğŸ“¥ å°†æ¨¡å‹ {model_name} ç§»å›GPU")
                model = self.cpu_models.pop(model_name)
                model = model.to(self.device)
                self.gpu_models[model_name] = model
                
                # æ›´æ–°å…ƒæ•°æ®
                self.model_metadata[model_name]['device'] = self.device
                
                return model
            else:
                return self.cpu_models[model_name]
        
        return None
    
    def get_optimization_stats(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        current_usage = self.monitor.get_current_usage()
        
        return {
            'memory_usage': current_usage,
            'models_on_gpu': len(self.gpu_models),
            'models_on_cpu': len(self.cpu_models),
            'total_models': len(self.model_metadata),
            'optimization_config': {
                'mixed_precision': self.config.mixed_precision,
                'cpu_offload_threshold': self.config.cpu_offload_threshold,
                'max_gpu_memory_gb': self.config.max_gpu_memory_gb
            },
            'model_distribution': {
                name: meta['device'].type 
                for name, meta in self.model_metadata.items()
            }
        }

class HardwareProfiler:
    """ç¡¬ä»¶æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.gpu_available = torch.cuda.is_available()
        self.device_info = self._get_device_info()
    
    def _get_device_info(self) -> Dict[str, Any]:
        """è·å–è®¾å¤‡ä¿¡æ¯"""
        info = {
            'cpu_count': psutil.cpu_count(),
            'cpu_freq': psutil.cpu_freq()._asdict() if psutil.cpu_freq() else {},
            'memory_total_gb': psutil.virtual_memory().total / (1024**3),
            'gpu_available': self.gpu_available
        }
        
        if self.gpu_available:
            gpu_props = torch.cuda.get_device_properties(0)
            info['gpu_info'] = {
                'name': gpu_props.name,
                'total_memory_gb': gpu_props.total_memory / (1024**3),
                'compute_capability': f"{gpu_props.major}.{gpu_props.minor}",
                'multiprocessor_count': gpu_props.multi_processor_count
            }
        
        return info
    
    def benchmark_inference(self, model: torch.nn.Module, 
                          input_shape: Tuple, num_runs: int = 10) -> Dict[str, float]:
        """æ¨ç†æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        device = next(model.parameters()).device
        
        # é¢„çƒ­
        dummy_input = torch.randn(input_shape).to(device)
        for _ in range(3):
            with torch.no_grad():
                _ = model(dummy_input)
        
        # æµ‹è¯•
        torch.cuda.synchronize() if device.type == "cuda" else None
        start_time = time.time()
        
        for _ in range(num_runs):
            with torch.no_grad():
                _ = model(dummy_input)
        
        torch.cuda.synchronize() if device.type == "cuda" else None
        end_time = time.time()
        
        avg_time = (end_time - start_time) / num_runs
        throughput = 1.0 / avg_time
        
        return {
            'avg_inference_time_ms': avg_time * 1000,
            'throughput_fps': throughput,
            'device': str(device),
            'input_shape': input_shape
        }
    
    def get_optimal_config(self) -> MemoryConfig:
        """è·å–æœ€ä¼˜é…ç½®"""
        gpu_memory = self.device_info.get('gpu_info', {}).get('total_memory_gb', 0)
        cpu_memory = self.device_info['memory_total_gb']
        
        if gpu_memory >= 80:  # A100ç­‰é«˜ç«¯GPU
            return MemoryConfig(
                max_gpu_memory_gb=70.0,
                cpu_offload_threshold=0.85,
                mixed_precision=True,
                model_parallel=True
            )
        elif gpu_memory >= 40:  # A40ç­‰ä¸­é«˜ç«¯GPU
            return MemoryConfig(
                max_gpu_memory_gb=35.0,
                cpu_offload_threshold=0.8,
                mixed_precision=True,
                model_parallel=False
            )
        else:  # ä½ç«¯GPUæˆ–CPU
            return MemoryConfig(
                max_gpu_memory_gb=min(gpu_memory * 0.8, 16.0),
                cpu_offload_threshold=0.6,
                mixed_precision=True,
                model_parallel=False
            )

# å…¨å±€ä¼˜åŒ–å™¨å®ä¾‹
_global_optimizer = None

def get_memory_optimizer(config: Optional[MemoryConfig] = None) -> MemoryOptimizer:
    """è·å–å…¨å±€æ˜¾å­˜ä¼˜åŒ–å™¨"""
    global _global_optimizer
    
    if _global_optimizer is None:
        if config is None:
            profiler = HardwareProfiler()
            config = profiler.get_optimal_config()
        
        _global_optimizer = MemoryOptimizer(config)
        print(f"ğŸš€ æ˜¾å­˜ä¼˜åŒ–å™¨å·²åˆå§‹åŒ–: {config.max_gpu_memory_gb}GB GPUæ˜¾å­˜é™åˆ¶")
    
    return _global_optimizer

def optimize_for_jupyterhub():
    """JupyterHubç¯å¢ƒä¼˜åŒ–"""
    print("ğŸ”§ æ­£åœ¨ä¸ºJupyterHubç¯å¢ƒè¿›è¡Œä¼˜åŒ–...")
    
    # è·å–ç¡¬ä»¶ä¿¡æ¯
    profiler = HardwareProfiler()
    device_info = profiler.device_info
    
    print(f"ğŸ’» CPUæ ¸å¿ƒæ•°: {device_info['cpu_count']}")
    print(f"ğŸ’¾ ç³»ç»Ÿå†…å­˜: {device_info['memory_total_gb']:.1f}GB")
    
    if device_info['gpu_available']:
        gpu_info = device_info['gpu_info']
        print(f"ğŸ® GPU: {gpu_info['name']}")
        print(f"ğŸ“Š GPUæ˜¾å­˜: {gpu_info['total_memory_gb']:.1f}GB")
        
        # è·å–ä¼˜åŒ–é…ç½®
        config = profiler.get_optimal_config()
        optimizer = get_memory_optimizer(config)
        
        print(f"âš™ï¸  ä¼˜åŒ–ç­–ç•¥:")
        print(f"   â€¢ æœ€å¤§GPUæ˜¾å­˜ä½¿ç”¨: {config.max_gpu_memory_gb}GB")
        print(f"   â€¢ CPUåˆ†æµé˜ˆå€¼: {config.cpu_offload_threshold:.0%}")
        print(f"   â€¢ æ··åˆç²¾åº¦: {'âœ…' if config.mixed_precision else 'âŒ'}")
        print(f"   â€¢ æ¨¡å‹å¹¶è¡Œ: {'âœ…' if config.model_parallel else 'âŒ'}")
        
        return optimizer
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
        return None

if __name__ == "__main__":
    # æµ‹è¯•æ˜¾å­˜ä¼˜åŒ–å™¨
    optimizer = optimize_for_jupyterhub()
    
    if optimizer:
        # å¼€å§‹ç›‘æ§
        optimizer.monitor.start_monitoring()
        
        # æ¨¡æ‹ŸåŠ è½½æ¨¡å‹
        time.sleep(2)
        
        # åœæ­¢ç›‘æ§å¹¶æ˜¾ç¤ºç»“æœ
        logs = optimizer.monitor.stop_monitoring()
        stats = optimizer.get_optimization_stats()
        
        print(f"\nğŸ“Š ä¼˜åŒ–ç»Ÿè®¡:")
        print(f"å½“å‰æ˜¾å­˜ä½¿ç”¨: {stats['memory_usage']['utilization']:.1%}")
        print(f"GPUæ¨¡å‹æ•°: {stats['models_on_gpu']}")
        print(f"CPUæ¨¡å‹æ•°: {stats['models_on_cpu']}")