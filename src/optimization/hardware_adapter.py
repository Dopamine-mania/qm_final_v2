"""
ã€Šå¿ƒå¢ƒæµè½¬ã€‹ç¡¬ä»¶é€‚é…å™¨
Hardware Adapter for Mood Transitions System

æ™ºèƒ½ç¡¬ä»¶é€‚é…å’Œæ€§èƒ½ä¼˜åŒ–ç³»ç»Ÿ
- è‡ªåŠ¨æ£€æµ‹ç¡¬ä»¶é…ç½®
- åŠ¨æ€è°ƒæ•´è®¡ç®—ç­–ç•¥
- å¤šè®¾å¤‡ååŒè®¡ç®—
- å®æ—¶æ€§èƒ½è°ƒä¼˜
"""

import torch
import torch.nn as nn
import psutil
import platform
import subprocess
import json
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, asdict
from enum import Enum
import time
import threading
from concurrent.futures import ThreadPoolExecutor
import logging

class DeviceType(Enum):
    """è®¾å¤‡ç±»å‹"""
    CPU = "cpu"
    CUDA_GPU = "cuda"
    MPS = "mps"  # Apple Silicon
    INTEL_GPU = "intel_gpu"

class ComputeStrategy(Enum):
    """è®¡ç®—ç­–ç•¥"""
    GPU_ONLY = "gpu_only"
    CPU_ONLY = "cpu_only"
    HYBRID = "hybrid"
    DISTRIBUTED = "distributed"
    ADAPTIVE = "adaptive"

@dataclass
class HardwareSpec:
    """ç¡¬ä»¶è§„æ ¼"""
    device_type: DeviceType
    device_name: str
    compute_capability: Optional[str] = None
    memory_gb: float = 0.0
    core_count: int = 0
    frequency_ghz: Optional[float] = None
    bandwidth_gbps: Optional[float] = None
    power_watts: Optional[int] = None

@dataclass
class PerformanceProfile:
    """æ€§èƒ½é…ç½®æ–‡ä»¶"""
    batch_size: int = 1
    sequence_length: int = 512
    precision: str = "fp32"  # fp32, fp16, int8
    compute_strategy: ComputeStrategy = ComputeStrategy.ADAPTIVE
    memory_optimization: bool = True
    parallel_workers: int = 1
    cache_enabled: bool = True

class HardwareDetector:
    """ç¡¬ä»¶æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.detected_devices = []
        self.system_info = self._get_system_info()
        self._detect_all_devices()
    
    def _get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        return {
            'platform': platform.platform(),
            'processor': platform.processor(),
            'architecture': platform.architecture(),
            'python_version': platform.python_version(),
            'cpu_count': psutil.cpu_count(),
            'memory_total_gb': psutil.virtual_memory().total / (1024**3)
        }
    
    def _detect_all_devices(self):
        """æ£€æµ‹æ‰€æœ‰å¯ç”¨è®¾å¤‡"""
        # æ£€æµ‹CPU
        self._detect_cpu()
        
        # æ£€æµ‹CUDA GPU
        if torch.cuda.is_available():
            self._detect_cuda_gpus()
        
        # æ£€æµ‹Apple Silicon MPS
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            self._detect_mps()
        
        # æ£€æµ‹Intel GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self._detect_intel_gpu()
    
    def _detect_cpu(self):
        """æ£€æµ‹CPU"""
        cpu_info = psutil.cpu_freq()
        
        spec = HardwareSpec(
            device_type=DeviceType.CPU,
            device_name=platform.processor() or "CPU",
            memory_gb=self.system_info['memory_total_gb'],
            core_count=self.system_info['cpu_count'],
            frequency_ghz=cpu_info.max / 1000 if cpu_info else None
        )
        
        self.detected_devices.append(spec)
    
    def _detect_cuda_gpus(self):
        """æ£€æµ‹CUDA GPU"""
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            
            spec = HardwareSpec(
                device_type=DeviceType.CUDA_GPU,
                device_name=props.name,
                compute_capability=f"{props.major}.{props.minor}",
                memory_gb=props.total_memory / (1024**3),
                core_count=props.multi_processor_count,
                bandwidth_gbps=self._estimate_memory_bandwidth(props)
            )
            
            self.detected_devices.append(spec)
    
    def _detect_mps(self):
        """æ£€æµ‹Apple Silicon MPS"""
        # MPS (Metal Performance Shaders) for Apple Silicon
        spec = HardwareSpec(
            device_type=DeviceType.MPS,
            device_name="Apple Silicon GPU",
            memory_gb=self.system_info['memory_total_gb'],  # å…±äº«å†…å­˜
            core_count=self._get_apple_gpu_cores()
        )
        
        self.detected_devices.append(spec)
    
    def _detect_intel_gpu(self):
        """æ£€æµ‹Intel GPU"""
        try:
            # å°è¯•æ£€æµ‹Intel GPUï¼ˆå¦‚æœæœ‰ç›¸å…³åº“ï¼‰
            pass
        except:
            pass
    
    def _estimate_memory_bandwidth(self, props) -> Optional[float]:
        """ä¼°ç®—æ˜¾å­˜å¸¦å®½"""
        # åŸºäºGPUæ¶æ„çš„ç®€åŒ–ä¼°ç®—
        memory_type_bandwidth = {
            "GDDR6": 672,  # GB/s
            "GDDR6X": 936,
            "HBM2": 900,
            "HBM3": 2400
        }
        
        # ç®€åŒ–ä¼°ç®—ï¼Œå®é™…éœ€è¦æ›´è¯¦ç»†çš„GPUä¿¡æ¯
        if "A100" in props.name:
            return 1555  # A100 HBM2
        elif "V100" in props.name:
            return 900   # V100 HBM2
        elif "RTX" in props.name:
            return 672   # å¤§å¤šæ•°RTXä½¿ç”¨GDDR6
        else:
            return 400   # ä¿å®ˆä¼°ç®—
    
    def _get_apple_gpu_cores(self) -> int:
        """è·å–Apple GPUæ ¸å¿ƒæ•°"""
        try:
            result = subprocess.run(['system_profiler', 'SPDisplaysDataType'], 
                                  capture_output=True, text=True)
            # è§£æè¾“å‡ºè·å–GPUæ ¸å¿ƒæ•°ï¼ˆç®€åŒ–å®ç°ï¼‰
            return 8  # é»˜è®¤å€¼
        except:
            return 8
    
    def get_best_device(self) -> HardwareSpec:
        """è·å–æœ€ä½³è®¡ç®—è®¾å¤‡"""
        # ä¼˜å…ˆçº§ï¼šCUDA GPU > MPS > CPU
        cuda_devices = [d for d in self.detected_devices if d.device_type == DeviceType.CUDA_GPU]
        if cuda_devices:
            return max(cuda_devices, key=lambda x: x.memory_gb)
        
        mps_devices = [d for d in self.detected_devices if d.device_type == DeviceType.MPS]
        if mps_devices:
            return mps_devices[0]
        
        cpu_devices = [d for d in self.detected_devices if d.device_type == DeviceType.CPU]
        return cpu_devices[0] if cpu_devices else None

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self, hardware_spec: HardwareSpec):
        self.hardware_spec = hardware_spec
        self.performance_history = []
        self.current_profile = self._generate_default_profile()
    
    def _generate_default_profile(self) -> PerformanceProfile:
        """ç”Ÿæˆé»˜è®¤æ€§èƒ½é…ç½®"""
        if self.hardware_spec.device_type == DeviceType.CUDA_GPU:
            # GPUä¼˜åŒ–é…ç½®
            if self.hardware_spec.memory_gb >= 40:  # é«˜ç«¯GPU
                return PerformanceProfile(
                    batch_size=8,
                    sequence_length=1024,
                    precision="fp16",
                    compute_strategy=ComputeStrategy.GPU_ONLY,
                    parallel_workers=4
                )
            elif self.hardware_spec.memory_gb >= 16:  # ä¸­ç«¯GPU
                return PerformanceProfile(
                    batch_size=4,
                    sequence_length=512,
                    precision="fp16",
                    compute_strategy=ComputeStrategy.HYBRID,
                    parallel_workers=2
                )
            else:  # ä½ç«¯GPU
                return PerformanceProfile(
                    batch_size=2,
                    sequence_length=256,
                    precision="fp16",
                    compute_strategy=ComputeStrategy.HYBRID,
                    parallel_workers=1
                )
        
        elif self.hardware_spec.device_type == DeviceType.MPS:
            # Apple Siliconä¼˜åŒ–é…ç½®
            return PerformanceProfile(
                batch_size=4,
                sequence_length=512,
                precision="fp16",
                compute_strategy=ComputeStrategy.GPU_ONLY,
                parallel_workers=2
            )
        
        else:  # CPU
            return PerformanceProfile(
                batch_size=1,
                sequence_length=256,
                precision="fp32",
                compute_strategy=ComputeStrategy.CPU_ONLY,
                parallel_workers=min(4, self.hardware_spec.core_count)
            )
    
    def optimize_for_task(self, task_type: str, estimated_load: str) -> PerformanceProfile:
        """é’ˆå¯¹ç‰¹å®šä»»åŠ¡ä¼˜åŒ–"""
        base_profile = self.current_profile
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´
        if task_type == "emotion_recognition":
            # æƒ…ç»ªè¯†åˆ«ï¼šä½å»¶è¿Ÿä¼˜å…ˆ
            base_profile.batch_size = 1
            base_profile.sequence_length = min(512, base_profile.sequence_length)
            
        elif task_type == "music_generation":
            # éŸ³ä¹ç”Ÿæˆï¼šä¸­ç­‰å»¶è¿Ÿï¼Œè¾ƒé«˜ååé‡
            if estimated_load == "high":
                base_profile.batch_size = max(1, base_profile.batch_size // 2)
            
        elif task_type == "video_generation":
            # è§†é¢‘ç”Ÿæˆï¼šé«˜è®¡ç®—éœ€æ±‚
            if self.hardware_spec.memory_gb < 20:
                base_profile.batch_size = 1
                base_profile.compute_strategy = ComputeStrategy.HYBRID
        
        # æ ¹æ®è´Ÿè½½è°ƒæ•´
        if estimated_load == "high":
            base_profile.precision = "fp16"
            base_profile.memory_optimization = True
        elif estimated_load == "low":
            base_profile.precision = "fp32"
        
        return base_profile
    
    def benchmark_configuration(self, model: nn.Module, 
                              input_shape: Tuple, 
                              profile: PerformanceProfile) -> Dict[str, float]:
        """åŸºå‡†æµ‹è¯•é…ç½®"""
        device = torch.device(self.hardware_spec.device_type.value)
        model = model.to(device)
        
        # è®¾ç½®ç²¾åº¦
        if profile.precision == "fp16" and device.type in ["cuda", "mps"]:
            model = model.half()
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        test_input = torch.randn(profile.batch_size, *input_shape[1:])
        test_input = test_input.to(device)
        
        if profile.precision == "fp16" and device.type in ["cuda", "mps"]:
            test_input = test_input.half()
        
        # é¢„çƒ­
        for _ in range(3):
            with torch.no_grad():
                _ = model(test_input)
        
        # è®¡æ—¶æµ‹è¯•
        num_runs = 10
        start_time = time.time()
        
        for _ in range(num_runs):
            with torch.no_grad():
                output = model(test_input)
        
        if device.type == "cuda":
            torch.cuda.synchronize()
        
        end_time = time.time()
        
        avg_time = (end_time - start_time) / num_runs
        throughput = profile.batch_size / avg_time
        
        # å†…å­˜ä½¿ç”¨
        if device.type == "cuda":
            memory_used = torch.cuda.max_memory_allocated() / (1024**3)
        else:
            memory_used = 0.0
        
        return {
            'avg_inference_time_ms': avg_time * 1000,
            'throughput_samples_per_sec': throughput,
            'memory_used_gb': memory_used,
            'device': str(device),
            'precision': profile.precision,
            'batch_size': profile.batch_size
        }
    
    def adaptive_optimization(self, performance_feedback: Dict[str, float]):
        """è‡ªé€‚åº”ä¼˜åŒ–"""
        self.performance_history.append(performance_feedback)
        
        # åˆ†ææ€§èƒ½è¶‹åŠ¿
        if len(self.performance_history) >= 3:
            recent_performance = self.performance_history[-3:]
            avg_latency = sum(p.get('avg_inference_time_ms', 0) for p in recent_performance) / 3
            
            # å¦‚æœå»¶è¿Ÿè¿‡é«˜ï¼Œé™ä½å¤æ‚åº¦
            if avg_latency > 1000:  # 1ç§’
                self.current_profile.batch_size = max(1, self.current_profile.batch_size - 1)
                self.current_profile.sequence_length = max(128, self.current_profile.sequence_length - 128)
            
            # å¦‚æœå»¶è¿Ÿè¾ƒä½ä¸”å†…å­˜å……è¶³ï¼Œå¯ä»¥æé«˜æ€§èƒ½
            elif avg_latency < 200 and all(p.get('memory_used_gb', 0) < self.hardware_spec.memory_gb * 0.7 
                                         for p in recent_performance):
                self.current_profile.batch_size = min(8, self.current_profile.batch_size + 1)

class HardwareAdapter:
    """ç¡¬ä»¶é€‚é…å™¨ä¸»ç±»"""
    
    def __init__(self):
        self.detector = HardwareDetector()
        self.best_device = self.detector.get_best_device()
        self.optimizer = PerformanceOptimizer(self.best_device) if self.best_device else None
        self.device_pool = self._create_device_pool()
        
        print(f"ğŸ”§ ç¡¬ä»¶é€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ® æœ€ä½³è®¡ç®—è®¾å¤‡: {self.best_device.device_name}")
        print(f"ğŸ“Š è®¾å¤‡å†…å­˜: {self.best_device.memory_gb:.1f}GB")
    
    def _create_device_pool(self) -> Dict[str, torch.device]:
        """åˆ›å»ºè®¾å¤‡æ± """
        pool = {}
        
        for spec in self.detector.detected_devices:
            device_key = f"{spec.device_type.value}_{spec.device_name}"
            pool[device_key] = torch.device(spec.device_type.value)
        
        return pool
    
    def get_optimal_device(self, task_type: str = "general") -> torch.device:
        """è·å–ä»»åŠ¡æœ€ä¼˜è®¾å¤‡"""
        if task_type == "emotion_recognition":
            # æƒ…ç»ªè¯†åˆ«ä¼˜å…ˆä½¿ç”¨GPUåŠ é€Ÿ
            cuda_devices = [d for d in self.detector.detected_devices 
                          if d.device_type == DeviceType.CUDA_GPU]
            if cuda_devices:
                return torch.device("cuda")
        
        return torch.device(self.best_device.device_type.value)
    
    def optimize_model_for_hardware(self, model: nn.Module, 
                                  task_type: str = "general",
                                  target_latency_ms: float = 500) -> Tuple[nn.Module, PerformanceProfile]:
        """ä¸ºç¡¬ä»¶ä¼˜åŒ–æ¨¡å‹"""
        if not self.optimizer:
            return model, PerformanceProfile()
        
        # ä¼°ç®—è´Ÿè½½
        estimated_load = "high" if target_latency_ms < 200 else "medium"
        
        # è·å–ä¼˜åŒ–é…ç½®
        profile = self.optimizer.optimize_for_task(task_type, estimated_load)
        
        # åº”ç”¨ä¼˜åŒ–
        device = self.get_optimal_device(task_type)
        model = model.to(device)
        
        # ç²¾åº¦ä¼˜åŒ–
        if profile.precision == "fp16" and device.type in ["cuda", "mps"]:
            model = model.half()
        
        # ç¼–è¯‘ä¼˜åŒ–ï¼ˆPyTorch 2.0+ï¼‰
        if hasattr(torch, 'compile') and device.type == "cuda":
            try:
                model = torch.compile(model)
            except:
                pass  # ç¼–è¯‘å¤±è´¥æ—¶ç»§ç»­ä½¿ç”¨åŸæ¨¡å‹
        
        return model, profile
    
    def get_hardware_summary(self) -> Dict[str, Any]:
        """è·å–ç¡¬ä»¶æ‘˜è¦"""
        return {
            'system_info': self.detector.system_info,
            'detected_devices': [asdict(spec) for spec in self.detector.detected_devices],
            'best_device': asdict(self.best_device) if self.best_device else None,
            'current_profile': asdict(self.optimizer.current_profile) if self.optimizer else None,
            'device_pool': list(self.device_pool.keys())
        }
    
    def monitor_performance(self, interval: float = 5.0) -> threading.Thread:
        """å¯åŠ¨æ€§èƒ½ç›‘æ§"""
        def monitor_loop():
            while True:
                # æ”¶é›†æ€§èƒ½æ•°æ®
                if torch.cuda.is_available():
                    gpu_util = torch.cuda.utilization()
                    gpu_memory = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated()
                else:
                    gpu_util = 0
                    gpu_memory = 0
                
                cpu_util = psutil.cpu_percent()
                memory_util = psutil.virtual_memory().percent
                
                performance_data = {
                    'timestamp': time.time(),
                    'gpu_utilization': gpu_util,
                    'gpu_memory_utilization': gpu_memory,
                    'cpu_utilization': cpu_util,
                    'memory_utilization': memory_util
                }
                
                # è‡ªé€‚åº”ä¼˜åŒ–
                if self.optimizer:
                    self.optimizer.adaptive_optimization(performance_data)
                
                time.sleep(interval)
        
        monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
        monitor_thread.start()
        return monitor_thread

# å…¨å±€é€‚é…å™¨å®ä¾‹
_global_adapter = None

def get_hardware_adapter() -> HardwareAdapter:
    """è·å–å…¨å±€ç¡¬ä»¶é€‚é…å™¨"""
    global _global_adapter
    
    if _global_adapter is None:
        _global_adapter = HardwareAdapter()
    
    return _global_adapter

def auto_configure_for_jupyterhub() -> Dict[str, Any]:
    """JupyterHubç¯å¢ƒè‡ªåŠ¨é…ç½®"""
    print("ğŸ”§ æ­£åœ¨ä¸ºJupyterHubç¯å¢ƒè¿›è¡Œè‡ªåŠ¨é…ç½®...")
    
    adapter = get_hardware_adapter()
    summary = adapter.get_hardware_summary()
    
    # ç”Ÿæˆæ¨èé…ç½®
    recommendations = {
        'compute_strategy': 'hybrid' if summary['best_device']['device_type'] != 'cpu' else 'cpu_only',
        'batch_size': 4 if summary['best_device']['memory_gb'] > 16 else 2,
        'precision': 'fp16' if summary['best_device']['device_type'] in ['cuda', 'mps'] else 'fp32',
        'parallel_workers': min(4, summary['system_info']['cpu_count']),
        'memory_optimization': True
    }
    
    print(f"âœ… è‡ªåŠ¨é…ç½®å®Œæˆ:")
    print(f"   ğŸ® æœ€ä½³è®¾å¤‡: {summary['best_device']['device_name']}")
    print(f"   ğŸ“Š æ¨èæ‰¹å¤§å°: {recommendations['batch_size']}")
    print(f"   ğŸ”¢ æ¨èç²¾åº¦: {recommendations['precision']}")
    print(f"   ğŸ§µ å¹¶è¡Œå·¥ä½œçº¿ç¨‹: {recommendations['parallel_workers']}")
    
    return {
        'hardware_summary': summary,
        'recommendations': recommendations
    }

if __name__ == "__main__":
    # æµ‹è¯•ç¡¬ä»¶é€‚é…å™¨
    config = auto_configure_for_jupyterhub()
    print(f"\nğŸ“‹ é…ç½®æ‘˜è¦:")
    print(json.dumps(config, indent=2, ensure_ascii=False))