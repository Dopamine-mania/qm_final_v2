{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ã€Šå¿ƒå¢ƒæµè½¬ã€‹ç³»ç»Ÿåˆå§‹åŒ–\n",
    "\n",
    "æœ¬notebookç”¨äºåˆå§‹åŒ–å’ŒéªŒè¯\"å¿ƒå¢ƒæµè½¬\"ç¡å‰éŸ³ç”»ç–—æ„ˆç³»ç»Ÿçš„è¿è¡Œç¯å¢ƒã€‚\n",
    "\n",
    "## ğŸ¯ ç›®æ ‡\n",
    "- æ£€æµ‹JupyterHubç¯å¢ƒå’Œç¡¬ä»¶èµ„æº\n",
    "- éªŒè¯æ‰€éœ€ä¾èµ–åº“çš„å®‰è£…\n",
    "- é…ç½®ç³»ç»Ÿå‚æ•°å’Œè·¯å¾„\n",
    "- è¿›è¡ŒåŸºç¡€åŠŸèƒ½æµ‹è¯•\n",
    "\n",
    "## âš ï¸ é‡è¦æç¤º\n",
    "- é¦–æ¬¡è¿è¡Œæ­¤ç³»ç»Ÿæ—¶å¿…é¡»æ‰§è¡Œæ­¤notebook\n",
    "- ç¡®ä¿å…·æœ‰è¶³å¤Ÿçš„GPUå†…å­˜ (40GB+)\n",
    "- æŸäº›æ¨¡å‹å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ä¸‹è½½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒæ£€æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ã€Šå¿ƒå¢ƒæµè½¬ã€‹ç³»ç»Ÿç¯å¢ƒæ£€æµ‹\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# åŸºç¡€ç¯å¢ƒä¿¡æ¯\n",
    "print(f\"Python ç‰ˆæœ¬: {sys.version}\")\n",
    "print(f\"æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}\")\n",
    "print(f\"CPUæ¶æ„: {platform.machine()}\")\n",
    "print(f\"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}\")\n",
    "print(f\"Pythonè·¯å¾„: {sys.executable}\")\n",
    "print(f\"æ£€æµ‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# ä¿®å¤Pythonè·¯å¾„å’Œé¡¹ç›®ç»“æ„é—®é¢˜\nimport sys\nimport os\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# æ™ºèƒ½æ£€æµ‹é¡¹ç›®æ ¹ç›®å½•\ncurrent_dir = Path.cwd()\nif current_dir.name == 'notebooks':\n    # å¦‚æœåœ¨notebooksç›®å½•ä¸­è¿è¡Œ\n    PROJECT_ROOT = current_dir.parent\nelif (current_dir / 'notebooks').exists():\n    # å¦‚æœåœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ\n    PROJECT_ROOT = current_dir\nelse:\n    # å°è¯•å‘ä¸ŠæŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•\n    PROJECT_ROOT = current_dir\n    for parent in current_dir.parents:\n        if (parent / 'src').exists() and (parent / 'notebooks').exists():\n            PROJECT_ROOT = parent\n            break\n\nprint(f\"ğŸ“ å½“å‰å·¥ä½œç›®å½•: {current_dir}\")\nprint(f\"ğŸ“‚ é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}\")\n\n# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path\nproject_paths = [\n    str(PROJECT_ROOT),\n    str(PROJECT_ROOT / 'src'),\n    str(PROJECT_ROOT / 'research')\n]\n\nfor path in project_paths:\n    if path not in sys.path:\n        sys.path.insert(0, path)\n\nprint(\"ğŸ Pythonè·¯å¾„å·²æ›´æ–°:\")\nfor i, path in enumerate(project_paths):\n    print(f\"  [{i}] {path}\")\n\n# éªŒè¯é¡¹ç›®ç»“æ„å¹¶åˆ›å»ºç¼ºå¤±ç›®å½•\nrequired_structures = {\n    'src': ['core', 'models', 'therapy', 'optimization', 'evaluation'],\n    'research': ['theory'],\n    'notebooks': [],\n    'outputs': ['audio', 'video', 'logs', 'models', 'reports', 'cache'],\n    'configs': [],\n    'api': []\n}\n\nprint(\"\\nğŸ“ é¡¹ç›®ç»“æ„éªŒè¯ä¸ä¿®å¤:\")\nall_good = True\n\nfor main_dir, subdirs in required_structures.items():\n    main_path = PROJECT_ROOT / main_dir\n    if main_path.exists():\n        print(f\"âœ… {main_dir}/\")\n    else:\n        main_path.mkdir(parents=True, exist_ok=True)\n        print(f\"ğŸ”§ {main_dir}/ (å·²åˆ›å»º)\")\n        all_good = False\n    \n    # æ£€æŸ¥å­ç›®å½•\n    for subdir in subdirs:\n        sub_path = main_path / subdir\n        if sub_path.exists():\n            print(f\"âœ… {main_dir}/{subdir}/\")\n        else:\n            sub_path.mkdir(parents=True, exist_ok=True)\n            print(f\"ğŸ”§ {main_dir}/{subdir}/ (å·²åˆ›å»º)\")\n\n# è®¾ç½®ç¯å¢ƒå˜é‡\nos.environ['PROJECT_ROOT'] = str(PROJECT_ROOT)\ncache_dir = PROJECT_ROOT / 'outputs' / 'cache'\nos.environ['TRANSFORMERS_CACHE'] = str(cache_dir / 'transformers')\nos.environ['HF_HOME'] = str(cache_dir / 'huggingface')\n\nprint(f\"\\nâœ… é¡¹ç›®ç»“æ„éªŒè¯å®Œæˆ\")\nprint(f\"ğŸ“¦ ç¼“å­˜ç›®å½•: {cache_dir}\")\n\n# åˆ›å»ºå¿…è¦çš„__init__.pyæ–‡ä»¶\ninit_files = [\n    PROJECT_ROOT / 'src' / '__init__.py',\n    PROJECT_ROOT / 'src' / 'core' / '__init__.py',\n    PROJECT_ROOT / 'src' / 'models' / '__init__.py',\n    PROJECT_ROOT / 'src' / 'therapy' / '__init__.py',\n    PROJECT_ROOT / 'src' / 'optimization' / '__init__.py',\n    PROJECT_ROOT / 'src' / 'evaluation' / '__init__.py',\n    PROJECT_ROOT / 'research' / '__init__.py',\n    PROJECT_ROOT / 'research' / 'theory' / '__init__.py'\n]\n\nfor init_file in init_files:\n    if not init_file.exists():\n        init_file.touch()\n        print(f\"ğŸ”§ åˆ›å»º {init_file.relative_to(PROJECT_ROOT)}\")\n\nprint(\"ğŸ‰ Pythonç¯å¢ƒé…ç½®å®Œæˆï¼\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ç¡¬ä»¶èµ„æºæ£€æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import gc\n",
    "\n",
    "print(\"\\nğŸ–¥ï¸ ç³»ç»Ÿèµ„æºæ£€æµ‹:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# CPUä¿¡æ¯\n",
    "cpu_count = psutil.cpu_count()\n",
    "cpu_freq = psutil.cpu_freq()\n",
    "print(f\"CPUæ ¸å¿ƒæ•°: {cpu_count}\")\n",
    "if cpu_freq:\n",
    "    print(f\"CPUé¢‘ç‡: {cpu_freq.current:.2f} MHz\")\n",
    "\n",
    "# å†…å­˜ä¿¡æ¯\n",
    "memory = psutil.virtual_memory()\n",
    "memory_gb = memory.total / (1024**3)\n",
    "memory_available_gb = memory.available / (1024**3)\n",
    "print(f\"æ€»å†…å­˜: {memory_gb:.1f} GB\")\n",
    "print(f\"å¯ç”¨å†…å­˜: {memory_available_gb:.1f} GB\")\n",
    "print(f\"å†…å­˜ä½¿ç”¨ç‡: {memory.percent}%\")\n",
    "\n",
    "# ç£ç›˜ä¿¡æ¯\n",
    "disk = psutil.disk_usage('/')\n",
    "disk_total_gb = disk.total / (1024**3)\n",
    "disk_free_gb = disk.free / (1024**3)\n",
    "print(f\"ç£ç›˜æ€»å®¹é‡: {disk_total_gb:.1f} GB\")\n",
    "print(f\"ç£ç›˜å¯ç”¨: {disk_free_gb:.1f} GB\")\n",
    "\n",
    "# èµ„æºå……è¶³æ€§è¯„ä¼°\n",
    "print(\"\\nğŸ“Š èµ„æºè¯„ä¼°:\")\n",
    "if memory_gb >= 32:\n",
    "    print(\"âœ… å†…å­˜å……è¶³ (>=32GB)\")\n",
    "elif memory_gb >= 16:\n",
    "    print(\"âš ï¸ å†…å­˜å¯ç”¨ä½†å¯èƒ½ä¸è¶³ (16-32GB)\")\n",
    "else:\n",
    "    print(\"âŒ å†…å­˜ä¸è¶³ (<16GB)\")\n",
    "\n",
    "if disk_free_gb >= 50:\n",
    "    print(\"âœ… ç£ç›˜ç©ºé—´å……è¶³ (>=50GB)\")\n",
    "elif disk_free_gb >= 20:\n",
    "    print(\"âš ï¸ ç£ç›˜ç©ºé—´ç´§å¼  (20-50GB)\")\n",
    "else:\n",
    "    print(\"âŒ ç£ç›˜ç©ºé—´ä¸è¶³ (<20GB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUæ£€æµ‹\n",
    "print(\"\\nğŸ® GPUæ£€æµ‹:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    \n",
    "    print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"âœ… CUDAå¯ç”¨ï¼Œå‘ç° {gpu_count} ä¸ªGPU\")\n",
    "        print(f\"CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "        \n",
    "        for i in range(gpu_count):\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            total_memory = props.total_memory / (1024**3)\n",
    "            allocated_memory = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "            cached_memory = torch.cuda.memory_reserved(i) / (1024**3)\n",
    "            free_memory = total_memory - allocated_memory\n",
    "            \n",
    "            print(f\"\\nGPU {i}: {props.name}\")\n",
    "            print(f\"  è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}\")\n",
    "            print(f\"  æ€»æ˜¾å­˜: {total_memory:.1f} GB\")\n",
    "            print(f\"  å·²åˆ†é…: {allocated_memory:.2f} GB\")\n",
    "            print(f\"  å·²ç¼“å­˜: {cached_memory:.2f} GB\")\n",
    "            print(f\"  å¯ç”¨æ˜¾å­˜: {free_memory:.1f} GB\")\n",
    "            \n",
    "            # GPUè¯„ä¼°\n",
    "            if total_memory >= 80:\n",
    "                gpu_status = \"âœ… 80GB GPU - ä¼˜ç§€é…ç½®\"\n",
    "            elif total_memory >= 40:\n",
    "                gpu_status = \"âœ… 40GB GPU - æ¨èé…ç½®\"\n",
    "            elif total_memory >= 24:\n",
    "                gpu_status = \"âš ï¸ 24GB GPU - å¯ç”¨ä½†å—é™\"\n",
    "            elif total_memory >= 12:\n",
    "                gpu_status = \"âš ï¸ 12GB GPU - ä»…æ”¯æŒå°æ¨¡å‹\"\n",
    "            else:\n",
    "                gpu_status = \"âŒ <12GB GPU - ä¸å»ºè®®ä½¿ç”¨\"\n",
    "            \n",
    "            print(f\"  çŠ¶æ€: {gpu_status}\")\n",
    "    else:\n",
    "        print(\"âŒ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼\")\n",
    "        print(\"âš ï¸ è­¦å‘Š: CPUæ¨¡å¼ä¸‹æ€§èƒ½å°†æ˜¾è‘—é™ä½\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"âŒ PyTorchæœªå®‰è£…ï¼Œæ— æ³•æ£€æµ‹GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ä¾èµ–åº“æ£€æµ‹å’Œå®‰è£…"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nğŸ“¦ å…³é”®ä¾èµ–åº“æ£€æµ‹ä¸è‡ªåŠ¨å®‰è£…:\")\nprint(\"-\" * 40)\n\n# æ ¸å¿ƒä¾èµ–åº“åˆ—è¡¨ - é’ˆå¯¹JupyterHubç¯å¢ƒä¼˜åŒ–\ncore_dependencies = {\n    'torch': '>=1.12.0',\n    'transformers': '>=4.20.0', \n    'numpy': '>=1.21.0',\n    'scipy': '>=1.8.0',\n    'pillow': '>=8.0.0',\n    'matplotlib': '>=3.5.0',\n    'seaborn': '>=0.11.0',\n    'tqdm': '>=4.60.0',\n    'pyyaml': '>=5.4.0',\n    'psutil': '>=5.8.0'\n}\n\n# å¯é€‰ä¾èµ–åº“ (åœ¨å­¦æ ¡æœåŠ¡å™¨ä¸Šå¯èƒ½æ— æ³•å®‰è£…)\noptional_dependencies = {\n    'diffusers': '>=0.20.0',\n    'librosa': '>=0.9.0',\n    'opencv-python': '>=4.5.0',\n    'plotly': '>=5.0.0',\n    'ipywidgets': '>=7.6.0',\n    'audiocraft': 'MusicGenéŸ³ä¹ç”Ÿæˆ',\n    'accelerate': 'Hugging Faceæ¨¡å‹åŠ é€Ÿ',\n    'xformers': 'å†…å­˜ä¼˜åŒ– (æ¨è)'\n}\n\nmissing_packages = []\ninstalled_packages = {}\noptional_missing = []\n\n# æ£€æµ‹æ ¸å¿ƒä¾èµ–\nfor package, min_version in core_dependencies.items():\n    try:\n        if package == 'opencv-python':\n            import cv2\n            version = cv2.__version__\n            package_name = 'cv2'\n        else:\n            module = __import__(package)\n            version = getattr(module, '__version__', 'unknown')\n            package_name = package\n        \n        installed_packages[package] = version\n        print(f\"âœ… {package_name}: {version}\")\n        \n    except ImportError:\n        missing_packages.append(package)\n        print(f\"âŒ {package}: æœªå®‰è£…\")\n\n# æ£€æµ‹å¯é€‰ä¾èµ–\nfor package, description in optional_dependencies.items():\n    try:\n        module = __import__(package)\n        version = getattr(module, '__version__', 'unknown')\n        print(f\"âœ… {package}: {version} - {description}\")\n    except ImportError:\n        optional_missing.append(package)\n        print(f\"âšª {package}: æœªå®‰è£… - {description}\")\n\n# è‡ªåŠ¨å®‰è£…ç¼ºå¤±çš„æ ¸å¿ƒä¾èµ–\nif missing_packages:\n    print(f\"\\nâš ï¸ ç¼ºå¤±æ ¸å¿ƒä¾èµ–åº“: {', '.join(missing_packages)}\")\n    \n    # åœ¨JupyterHubç¯å¢ƒä¸­å°è¯•è‡ªåŠ¨å®‰è£…\n    install_attempt = input(f\"\\næ˜¯å¦å°è¯•è‡ªåŠ¨å®‰è£…ç¼ºå¤±çš„æ ¸å¿ƒä¾èµ–? (y/n): \").lower().strip()\n    \n    if install_attempt == 'y':\n        import subprocess\n        print(\"\\nğŸ”„ æ­£åœ¨å°è¯•å®‰è£…ç¼ºå¤±ä¾èµ–...\")\n        \n        failed_installs = []\n        for package in missing_packages:\n            try:\n                print(f\"ğŸ“¦ å®‰è£… {package}...\")\n                result = subprocess.run([\n                    sys.executable, '-m', 'pip', 'install', package, '--user', '--quiet'\n                ], capture_output=True, text=True, timeout=300)\n                \n                if result.returncode == 0:\n                    print(f\"âœ… {package} å®‰è£…æˆåŠŸ\")\n                    # é‡æ–°æ£€æµ‹\n                    try:\n                        if package == 'opencv-python':\n                            import cv2\n                            installed_packages[package] = cv2.__version__\n                        else:\n                            module = __import__(package)\n                            installed_packages[package] = getattr(module, '__version__', 'installed')\n                    except:\n                        pass\n                else:\n                    failed_installs.append(package)\n                    print(f\"âŒ {package} å®‰è£…å¤±è´¥: {result.stderr[:100]}...\")\n                    \n            except Exception as e:\n                failed_installs.append(package)\n                print(f\"âŒ {package} å®‰è£…å¼‚å¸¸: {str(e)[:50]}...\")\n        \n        if failed_installs:\n            print(f\"\\nâš ï¸ ä»¥ä¸‹ä¾èµ–åº“å®‰è£…å¤±è´¥: {', '.join(failed_installs)}\")\n            print(\"ğŸ’¡ è¿™äº›å¯èƒ½éœ€è¦ç³»ç»Ÿç®¡ç†å‘˜æƒé™æˆ–åœ¨å­¦æ ¡æœåŠ¡å™¨ä¸Šä¸å¯ç”¨\")\n            print(\"ğŸ“ ç³»ç»Ÿå°†ä½¿ç”¨Mockæ¨¡å¼ç»§ç»­è¿è¡Œ\")\n        else:\n            print(\"\\nğŸ‰ æ‰€æœ‰æ ¸å¿ƒä¾èµ–åº“å®‰è£…æˆåŠŸï¼\")\n            missing_packages = []\n    else:\n        print(\"â­ï¸ è·³è¿‡è‡ªåŠ¨å®‰è£…ï¼Œç³»ç»Ÿå°†ä½¿ç”¨Mockæ¨¡å¼\")\nelse:\n    print(\"\\nâœ… æ‰€æœ‰æ ¸å¿ƒä¾èµ–åº“å·²å®‰è£…\")\n\n# åˆ›å»ºå…¼å®¹æ€§é…ç½®\ncompatibility_config = {\n    'core_dependencies_met': len(missing_packages) == 0,\n    'missing_core': missing_packages,\n    'optional_missing': optional_missing,\n    'mock_mode_required': len(missing_packages) > 0,\n    'jupyterhub_optimized': True\n}\n\nprint(f\"\\nğŸ“Š ä¾èµ–çŠ¶æ€æ€»ç»“:\")\nprint(f\"  æ ¸å¿ƒä¾èµ–: {'âœ… å®Œæ•´' if compatibility_config['core_dependencies_met'] else f'âŒ ç¼ºå¤±{len(missing_packages)}ä¸ª'}\")\nprint(f\"  å¯é€‰ä¾èµ–: {'âœ… å®Œæ•´' if len(optional_missing) == 0 else f'âšª ç¼ºå¤±{len(optional_missing)}ä¸ª'}\")\nprint(f\"  è¿è¡Œæ¨¡å¼: {'ğŸš€ å®Œæ•´æ¨¡å¼' if not compatibility_config['mock_mode_required'] else 'ğŸ”§ Mockæ¨¡å¼'}\")\n\n# ä¿å­˜å…¼å®¹æ€§é…ç½®\nimport json\ncompat_file = PROJECT_ROOT / 'outputs' / 'compatibility_config.json'\nwith open(compat_file, 'w', encoding='utf-8') as f:\n    json.dump(compatibility_config, f, indent=2, ensure_ascii=False)\nprint(f\"ğŸ’¾ å…¼å®¹æ€§é…ç½®å·²ä¿å­˜: {compat_file}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯é€‰ä¾èµ–åº“æ£€æµ‹ (ç”¨äºç‰¹å®šåŠŸèƒ½)\n",
    "print(\"\\nğŸ“¦ å¯é€‰ä¾èµ–åº“æ£€æµ‹:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "optional_dependencies = {\n",
    "    'audiocraft': 'MusicGenéŸ³ä¹ç”Ÿæˆ',\n",
    "    'accelerate': 'Hugging Faceæ¨¡å‹åŠ é€Ÿ',\n",
    "    'xformers': 'å†…å­˜ä¼˜åŒ– (æ¨è)',\n",
    "    'pynvml': 'GPUç›‘æ§',\n",
    "    'wandb': 'å®éªŒè·Ÿè¸ª (å¯é€‰)',\n",
    "    'tensorboard': 'TensorBoardæ—¥å¿—',\n",
    "    'jupyter_contrib_nbextensions': 'Jupyteræ‰©å±•',\n",
    "    'ipywidgets': 'Jupyteräº¤äº’ç»„ä»¶'\n",
    "}\n",
    "\n",
    "for package, description in optional_dependencies.items():\n",
    "    try:\n",
    "        module = __import__(package)\n",
    "        version = getattr(module, '__version__', 'unknown')\n",
    "        print(f\"âœ… {package}: {version} - {description}\")\n",
    "    except ImportError:\n",
    "        print(f\"âšª {package}: æœªå®‰è£… - {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ç³»ç»Ÿé…ç½®åˆå§‹åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nâš™ï¸ ç³»ç»Ÿé…ç½®åˆå§‹åŒ–:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„\n",
    "output_dirs = [\n",
    "    'outputs',\n",
    "    'outputs/audio',\n",
    "    'outputs/video', \n",
    "    'outputs/logs',\n",
    "    'outputs/models',\n",
    "    'outputs/reports',\n",
    "    'outputs/cache'\n",
    "]\n",
    "\n",
    "for dir_name in output_dirs:\n",
    "    dir_path = PROJECT_ROOT / dir_name\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"âœ… åˆ›å»ºç›®å½•: {dir_path}\")\n",
    "\n",
    "# è®¾ç½®ç¼“å­˜ç›®å½•\n",
    "cache_dir = PROJECT_ROOT / 'outputs' / 'cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(cache_dir / 'transformers')\n",
    "os.environ['HF_HOME'] = str(cache_dir / 'huggingface')\n",
    "print(f\"âœ… è®¾ç½®ç¼“å­˜ç›®å½•: {cache_dir}\")\n",
    "\n",
    "# åˆ›å»ºç³»ç»Ÿé…ç½®\n",
    "system_config = {\n",
    "    'project_root': str(PROJECT_ROOT),\n",
    "    'python_version': sys.version,\n",
    "    'platform': platform.platform(),\n",
    "    'initialization_time': datetime.now().isoformat(),\n",
    "    'gpu_available': torch.cuda.is_available() if 'torch' in locals() else False,\n",
    "    'gpu_count': torch.cuda.device_count() if 'torch' in locals() and torch.cuda.is_available() else 0,\n",
    "    'total_memory_gb': memory_gb,\n",
    "    'available_memory_gb': memory_available_gb,\n",
    "    'installed_packages': installed_packages,\n",
    "    'output_directories': [str(PROJECT_ROOT / d) for d in output_dirs]\n",
    "}\n",
    "\n",
    "# ä¿å­˜é…ç½®æ–‡ä»¶\n",
    "config_file = PROJECT_ROOT / 'outputs' / 'system_config.json'\n",
    "with open(config_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(system_config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… ç³»ç»Ÿé…ç½®å·²ä¿å­˜: {config_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. åŸºç¡€åŠŸèƒ½æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nğŸ§ª åŸºç¡€åŠŸèƒ½æµ‹è¯•ä¸æ¨¡å—ä¿®å¤:\")\nprint(\"-\" * 40)\n\n# å¼ºåŒ–ç‰ˆæ¨¡å—å¯¼å…¥ä¸è‡ªåŠ¨ä¿®å¤\ndef ensure_module_exists(module_path, module_name, create_mock=True):\n    \"\"\"ç¡®ä¿æ¨¡å—å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºMockç‰ˆæœ¬\"\"\"\n    try:\n        # é¦–å…ˆå°è¯•å¯¼å…¥ç°æœ‰æ¨¡å—\n        module = __import__(module_path, fromlist=[module_name])\n        return getattr(module, module_name), True  # è¿”å›æ¨¡å—å’Œæ˜¯_realæ ‡å¿—\n    except ImportError:\n        if not create_mock:\n            return None, False\n        \n        # åˆ›å»ºMockæ¨¡å—\n        print(f\"ğŸ”§ åˆ›å»ºMockæ¨¡å—: {module_path}.{module_name}\")\n        \n        # æ ¹æ®æ¨¡å—ç±»å‹åˆ›å»ºä¸åŒçš„Mock\n        if 'theory' in module_path:\n            return create_theory_mocks(module_path, module_name)\n        elif 'models' in module_path:\n            return create_model_mocks(module_path, module_name)\n        elif 'therapy' in module_path:\n            return create_therapy_mocks(module_path, module_name)\n        else:\n            return None, False\n\ndef create_theory_mocks(module_path, module_name):\n    \"\"\"åˆ›å»ºç†è®ºæ¨¡å—Mock\"\"\"\n    \n    # ç¡®ä¿ç›®å½•å­˜åœ¨\n    theory_dir = PROJECT_ROOT / 'research' / 'theory'\n    theory_dir.mkdir(parents=True, exist_ok=True)\n    \n    # åˆ›å»º__init__.py\n    init_file = theory_dir / '__init__.py'\n    if not init_file.exists():\n        with open(init_file, 'w') as f:\n            f.write('\"\"\"ç†è®ºæ¨¡å—\"\"\"\\n')\n    \n    if module_name == 'ISOPrinciple':\n        # åˆ›å»ºISOåŸåˆ™æ¨¡å—\n        iso_file = theory_dir / 'iso_principle.py'\n        if not iso_file.exists():\n            with open(iso_file, 'w', encoding='utf-8') as f:\n                f.write('''\n\"\"\"ISOä¸‰é˜¶æ®µæ²»ç–—åŸåˆ™ - Mockç‰ˆæœ¬\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Dict, List\nimport random\n\n@dataclass\nclass EmotionState:\n    valence: float  # -1åˆ°1\n    arousal: float  # 0åˆ°1\n    confidence: float = 0.8\n\nclass ISOPrinciple:\n    def __init__(self):\n        self.stages = ['synchronization', 'guidance', 'consolidation']\n        print(\"ğŸµ ISOä¸‰é˜¶æ®µæ²»ç–—åŸåˆ™ (Mock) åˆå§‹åŒ–æˆåŠŸ\")\n    \n    def plan_therapy_stages(self, current_emotion, target_emotion):\n        return [\n            {'stage': 'synchronization', 'duration': 10, 'description': 'åŒæ­¥å½“å‰æƒ…ç»ª'},\n            {'stage': 'guidance', 'duration': 15, 'description': 'å¼•å¯¼æƒ…ç»ªè½¬æ¢'},\n            {'stage': 'consolidation', 'duration': 5, 'description': 'å·©å›ºç›®æ ‡æƒ…ç»ª'}\n        ]\n''')\n        \n        # é‡æ–°å¯¼å…¥\n        import importlib.util\n        spec = importlib.util.spec_from_file_location(\"iso_principle\", iso_file)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n        return getattr(module, 'ISOPrinciple'), False\n    \n    elif module_name == 'ValenceArousalModel':\n        # åˆ›å»ºVAæ¨¡å‹\n        va_file = theory_dir / 'valence_arousal.py'\n        if not va_file.exists():\n            with open(va_file, 'w', encoding='utf-8') as f:\n                f.write('''\n\"\"\"æƒ…ç»ªä»·å€¼-å”¤é†’æ¨¡å‹ - Mockç‰ˆæœ¬\"\"\"\nimport numpy as np\n\nclass ValenceArousalModel:\n    def __init__(self):\n        self.emotion_map = {\n            'happy': (0.8, 0.7), 'calm': (0.5, 0.2), 'sad': (-0.6, 0.3),\n            'angry': (-0.3, 0.8), 'anxious': (-0.4, 0.9), 'peaceful': (0.7, 0.1)\n        }\n        print(\"ğŸ“Š V-Aæƒ…ç»ªæ¨¡å‹ (Mock) åˆå§‹åŒ–æˆåŠŸ\")\n    \n    def get_emotion_coordinates(self, emotion):\n        return self.emotion_map.get(emotion, (0.0, 0.5))\n''')\n        \n        import importlib.util\n        spec = importlib.util.spec_from_file_location(\"valence_arousal\", va_file)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n        return getattr(module, 'ValenceArousalModel'), False\n    \n    return None, False\n\ndef create_model_mocks(module_path, module_name):\n    \"\"\"åˆ›å»ºæ¨¡å‹é€‚é…å™¨Mock\"\"\"\n    \n    models_dir = PROJECT_ROOT / 'src' / 'models'\n    models_dir.mkdir(parents=True, exist_ok=True)\n    \n    # åˆ›å»º__init__.py\n    init_file = models_dir / '__init__.py'\n    if not init_file.exists():\n        with open(init_file, 'w') as f:\n            f.write('\"\"\"æ¨¡å‹é€‚é…å™¨æ¨¡å—\"\"\"\\n')\n    \n    if module_name == 'ModelFactory':\n        factory_file = models_dir / 'factory.py'\n        if not factory_file.exists():\n            with open(factory_file, 'w', encoding='utf-8') as f:\n                f.write('''\n\"\"\"æ¨¡å‹å·¥å‚ - Mockç‰ˆæœ¬\"\"\"\nimport torch\n\nclass ModelFactory:\n    def __init__(self):\n        self.available_models = ['emotion_text', 'emotion_audio', 'music_gen']\n        print(\"ğŸ­ æ¨¡å‹å·¥å‚ (Mock) åˆå§‹åŒ–æˆåŠŸ\")\n    \n    def get_recommended_models(self):\n        gpu_available = torch.cuda.is_available()\n        return {\n            \"profile\": \"gpu_40gb\" if gpu_available else \"cpu_only\",\n            \"recommended\": {\n                \"emotion_text\": \"mock_text_model\",\n                \"emotion_audio\": \"mock_audio_model\", \n                \"music_gen\": \"mock_music_model\"\n            }\n        }\n    \n    def create_model_adapter(self, model_name):\n        print(f\"ğŸ”„ åˆ›å»ºMockæ¨¡å‹é€‚é…å™¨: {model_name}\")\n        return MockModelAdapter(model_name)\n\nclass MockModelAdapter:\n    def __init__(self, model_name):\n        self.model_name = model_name\n        self.is_loaded = False\n    \n    def load_model(self):\n        self.is_loaded = True\n        return True\n    \n    def predict(self, input_data):\n        return {\"prediction\": f\"mock_result_{self.model_name}\", \"confidence\": 0.85}\n''')\n        \n        import importlib.util\n        spec = importlib.util.spec_from_file_location(\"factory\", factory_file)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n        return getattr(module, 'ModelFactory'), False\n    \n    return None, False\n\ndef create_therapy_mocks(module_path, module_name):\n    \"\"\"åˆ›å»ºç–—æ„ˆç³»ç»ŸMock\"\"\"\n    \n    therapy_dir = PROJECT_ROOT / 'src' / 'therapy'\n    therapy_dir.mkdir(parents=True, exist_ok=True)\n    \n    # åˆ›å»º__init__.py\n    init_file = therapy_dir / '__init__.py'\n    if not init_file.exists():\n        with open(init_file, 'w') as f:\n            f.write('\"\"\"ç–—æ„ˆç³»ç»Ÿæ¨¡å—\"\"\"\\n')\n    \n    if module_name == 'TherapyOrchestrator':\n        core_file = therapy_dir / 'core.py'\n        if not core_file.exists():\n            with open(core_file, 'w', encoding='utf-8') as f:\n                f.write('''\n\"\"\"ç–—æ„ˆç³»ç»Ÿæ ¸å¿ƒ - Mockç‰ˆæœ¬\"\"\"\nfrom datetime import datetime\nimport uuid\n\nclass TherapyOrchestrator:\n    def __init__(self):\n        self.active_sessions = {}\n        print(\"ğŸ§˜ ç–—æ„ˆç¼–æ’å™¨ (Mock) åˆå§‹åŒ–æˆåŠŸ\")\n    \n    def create_session(self, user_id, current_emotion, target_emotion):\n        session_id = str(uuid.uuid4())[:8]\n        session = {\n            \"session_id\": session_id,\n            \"user_id\": user_id,\n            \"start_time\": datetime.now(),\n            \"current_emotion\": current_emotion,\n            \"target_emotion\": target_emotion,\n            \"status\": \"active\"\n        }\n        self.active_sessions[session_id] = session\n        print(f\"ğŸ¯ åˆ›å»ºç–—æ„ˆä¼šè¯: {session_id}\")\n        return session\n\nclass PrescriptionEngine:\n    def __init__(self):\n        print(\"ğŸ’Š å¤„æ–¹å¼•æ“ (Mock) åˆå§‹åŒ–æˆåŠŸ\")\n    \n    def generate_prescription(self, user_profile, current_emotion, target_emotion):\n        return {\n            \"prescription_id\": \"mock_rx_001\",\n            \"recommendations\": {\n                \"music_genre\": \"ambient\",\n                \"tempo_bpm\": 60,\n                \"video_theme\": \"nature\"\n            },\n            \"confidence_score\": 0.85\n        }\n''')\n        \n        import importlib.util\n        spec = importlib.util.spec_from_file_location(\"core\", core_file)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n        return getattr(module, 'TherapyOrchestrator'), False\n    \n    return None, False\n\n# å¼€å§‹æ¨¡å—æµ‹è¯•å’Œä¿®å¤\nprint(\"ğŸ” å¼€å§‹æ£€æµ‹å’Œä¿®å¤ç³»ç»Ÿæ¨¡å—...\")\n\n# æµ‹è¯•ç†è®ºæ¨¡å—\ntheory_modules_ok = True\ntry:\n    print(\"\\nğŸ“š ç†è®ºæ¨¡å—æµ‹è¯•:\")\n    \n    # æµ‹è¯•ISOåŸåˆ™\n    ISOPrinciple, is_real = ensure_module_exists('research.theory.iso_principle', 'ISOPrinciple')\n    if ISOPrinciple:\n        iso_planner = ISOPrinciple()\n        print(f\"  âœ… ISOåŸåˆ™: {'çœŸå®æ¨¡å—' if is_real else 'Mockæ¨¡å—'}\")\n    else:\n        print(\"  âŒ ISOåŸåˆ™: åˆ›å»ºå¤±è´¥\")\n        theory_modules_ok = False\n    \n    # æµ‹è¯•VAæ¨¡å‹\n    ValenceArousalModel, is_real = ensure_module_exists('research.theory.valence_arousal', 'ValenceArousalModel')\n    if ValenceArousalModel:\n        va_model = ValenceArousalModel()\n        print(f\"  âœ… VAæ¨¡å‹: {'çœŸå®æ¨¡å—' if is_real else 'Mockæ¨¡å—'}\")\n    else:\n        print(\"  âŒ VAæ¨¡å‹: åˆ›å»ºå¤±è´¥\")\n        theory_modules_ok = False\n        \nexcept Exception as e:\n    print(f\"  âŒ ç†è®ºæ¨¡å—ä¸¥é‡é”™è¯¯: {e}\")\n    theory_modules_ok = False\n\n# æµ‹è¯•æ¨¡å‹é€‚é…å™¨\nmodel_adapters_ok = True\ntry:\n    print(\"\\nğŸ¤– æ¨¡å‹é€‚é…å™¨æµ‹è¯•:\")\n    \n    ModelFactory, is_real = ensure_module_exists('src.models.factory', 'ModelFactory')\n    if ModelFactory:\n        factory = ModelFactory()\n        recommendations = factory.get_recommended_models()\n        print(f\"  âœ… æ¨¡å‹å·¥å‚: {'çœŸå®æ¨¡å—' if is_real else 'Mockæ¨¡å—'}\")\n        print(f\"  ğŸ¯ ç¡¬ä»¶é…ç½®: {recommendations.get('profile', 'unknown')}\")\n    else:\n        print(\"  âŒ æ¨¡å‹å·¥å‚: åˆ›å»ºå¤±è´¥\")\n        model_adapters_ok = False\n        \nexcept Exception as e:\n    print(f\"  âŒ æ¨¡å‹é€‚é…å™¨ä¸¥é‡é”™è¯¯: {e}\")\n    model_adapters_ok = False\n\n# æµ‹è¯•ç–—æ„ˆç³»ç»Ÿ\ntherapy_system_ok = True\ntry:\n    print(\"\\nğŸ§˜ ç–—æ„ˆç³»ç»Ÿæµ‹è¯•:\")\n    \n    TherapyOrchestrator, is_real = ensure_module_exists('src.therapy.core', 'TherapyOrchestrator')\n    if TherapyOrchestrator:\n        orchestrator = TherapyOrchestrator()\n        \n        # æµ‹è¯•åˆ›å»ºä¼šè¯\n        test_session = orchestrator.create_session(\n            \"test_user\", \n            {\"valence\": -0.4, \"arousal\": 0.8}, \n            {\"valence\": 0.6, \"arousal\": 0.3}\n        )\n        print(f\"  âœ… ç–—æ„ˆç¼–æ’å™¨: {'çœŸå®æ¨¡å—' if is_real else 'Mockæ¨¡å—'}\")\n        print(f\"  ğŸ¯ æµ‹è¯•ä¼šè¯: {test_session['session_id']}\")\n    else:\n        print(\"  âŒ ç–—æ„ˆç¼–æ’å™¨: åˆ›å»ºå¤±è´¥\")\n        therapy_system_ok = False\n        \nexcept Exception as e:\n    print(f\"  âŒ ç–—æ„ˆç³»ç»Ÿä¸¥é‡é”™è¯¯: {e}\")\n    therapy_system_ok = False\n\n# æ€»ç»“æ¨¡å—çŠ¶æ€\nprint(f\"\\nğŸ“Š æ¨¡å—æµ‹è¯•æ€»ç»“:\")\nprint(f\"  ç†è®ºæ¨¡å—: {'âœ… æ­£å¸¸' if theory_modules_ok else 'âŒ å¼‚å¸¸'}\")\nprint(f\"  æ¨¡å‹é€‚é…å™¨: {'âœ… æ­£å¸¸' if model_adapters_ok else 'âŒ å¼‚å¸¸'}\")\nprint(f\"  ç–—æ„ˆç³»ç»Ÿ: {'âœ… æ­£å¸¸' if therapy_system_ok else 'âŒ å¼‚å¸¸'}\")\n\n# è®¾ç½®å…¨å±€å˜é‡ä¾›åç»­ä½¿ç”¨\nglobals()['theory_modules_ok'] = theory_modules_ok\nglobals()['model_adapters_ok'] = model_adapters_ok  \nglobals()['therapy_system_ok'] = therapy_system_ok"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# æµ‹è¯•ç–—æ„ˆç³»ç»Ÿå¯¼å…¥ - å®‰å…¨å¯¼å…¥æ–¹å¼\nprint(\"\\nğŸ§˜ ç–—æ„ˆç³»ç»Ÿæµ‹è¯•:\")\nprint(\"-\" * 30)\n\ntherapy_system_ok = False\ntry:\n    # å°è¯•å¯¼å…¥ç°æœ‰çš„ç–—æ„ˆç³»ç»Ÿ\n    try:\n        from src.therapy.core import TherapyOrchestrator, TherapySession, EmotionTrajectoryPlanner\n        from src.therapy.stages import ISOStageManager\n        from src.therapy.prescriptions import PrescriptionEngine\n        print(\"âœ… ç°æœ‰ç–—æ„ˆç³»ç»Ÿå¯¼å…¥æˆåŠŸ\")\n        therapy_system_ok = True\n        \n    except ImportError as e:\n        print(f\"âš ï¸  ç°æœ‰ç–—æ„ˆç³»ç»Ÿå¯¼å…¥å¤±è´¥: {e}\")\n        print(\"ğŸ”§ åˆ›å»ºMockç–—æ„ˆç³»ç»Ÿ...\")\n        \n        # åˆ›å»ºç–—æ„ˆç³»ç»Ÿç›®å½•\n        therapy_dir = PROJECT_ROOT / 'src' / 'therapy'\n        therapy_dir.mkdir(parents=True, exist_ok=True)\n        \n        # åˆ›å»ºMockç–—æ„ˆç³»ç»Ÿæ–‡ä»¶\n        mock_therapy_files = {\n            'core.py': '''\n\"\"\"ç–—æ„ˆç³»ç»Ÿæ ¸å¿ƒ - Mockç‰ˆæœ¬\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\nfrom datetime import datetime\nimport uuid\n\n@dataclass\nclass TherapySession:\n    session_id: str\n    user_id: str\n    start_time: datetime\n    current_emotion: Dict\n    target_emotion: Dict\n    status: str = \"active\"\n\nclass EmotionTrajectoryPlanner:\n    \"\"\"æƒ…ç»ªè½¨è¿¹è§„åˆ’å™¨\"\"\"\n    \n    def __init__(self):\n        self.planning_strategies = [\"linear\", \"curved\", \"staged\"]\n    \n    def plan_trajectory(self, current_emotion: Dict, target_emotion: Dict, duration_minutes: int = 30) -> List[Dict]:\n        \"\"\"è§„åˆ’æƒ…ç»ªè½¬æ¢è½¨è¿¹\"\"\"\n        steps = max(3, duration_minutes // 10)\n        trajectory = []\n        \n        for i in range(steps):\n            progress = (i + 1) / steps\n            interpolated_valence = current_emotion[\"valence\"] + (target_emotion[\"valence\"] - current_emotion[\"valence\"]) * progress\n            interpolated_arousal = current_emotion[\"arousal\"] + (target_emotion[\"arousal\"] - current_emotion[\"arousal\"]) * progress\n            \n            trajectory.append({\n                \"step\": i + 1,\n                \"timestamp\": i * (duration_minutes // steps),\n                \"valence\": interpolated_valence,\n                \"arousal\": interpolated_arousal,\n                \"confidence\": 0.8\n            })\n        \n        return trajectory\n\nclass TherapyOrchestrator:\n    \"\"\"ç–—æ„ˆç¼–æ’å™¨\"\"\"\n    \n    def __init__(self):\n        self.active_sessions = {}\n        self.trajectory_planner = EmotionTrajectoryPlanner()\n        self.session_history = []\n    \n    def create_session(self, user_id: str, current_emotion: Dict, target_emotion: Dict) -> TherapySession:\n        \"\"\"åˆ›å»ºæ–°çš„ç–—æ„ˆä¼šè¯\"\"\"\n        session_id = str(uuid.uuid4())[:8]\n        \n        session = TherapySession(\n            session_id=session_id,\n            user_id=user_id,\n            start_time=datetime.now(),\n            current_emotion=current_emotion,\n            target_emotion=target_emotion\n        )\n        \n        self.active_sessions[session_id] = session\n        print(f\"ğŸ¯ åˆ›å»ºç–—æ„ˆä¼šè¯: {session_id}\")\n        return session\n    \n    def get_session_recommendations(self, session_id: str) -> Dict:\n        \"\"\"è·å–ä¼šè¯æ¨è\"\"\"\n        if session_id not in self.active_sessions:\n            return {\"error\": \"Session not found\"}\n        \n        session = self.active_sessions[session_id]\n        trajectory = self.trajectory_planner.plan_trajectory(\n            session.current_emotion, \n            session.target_emotion\n        )\n        \n        return {\n            \"session_id\": session_id,\n            \"trajectory\": trajectory,\n            \"recommendations\": {\n                \"music_style\": \"ambient\",\n                \"tempo_bpm\": 60,\n                \"video_theme\": \"nature\",\n                \"duration_minutes\": 30\n            }\n        }\n''',\n            'stages.py': '''\n\"\"\"ISOé˜¶æ®µç®¡ç†å™¨ - Mockç‰ˆæœ¬\"\"\"\nfrom typing import Dict, List\nfrom enum import Enum\n\nclass ISOStage(Enum):\n    SYNCHRONIZATION = \"synchronization\"\n    GUIDANCE = \"guidance\"\n    CONSOLIDATION = \"consolidation\"\n\nclass ISOStageManager:\n    \"\"\"ISOä¸‰é˜¶æ®µç®¡ç†å™¨\"\"\"\n    \n    def __init__(self):\n        self.current_stage = None\n        self.stage_history = []\n        \n        # é˜¶æ®µé…ç½®\n        self.stage_configs = {\n            ISOStage.SYNCHRONIZATION: {\n                \"duration_ratio\": 0.25,\n                \"description\": \"ä¸ç”¨æˆ·å½“å‰æƒ…ç»ªåŒæ­¥\",\n                \"music_strategy\": \"match_current_emotion\",\n                \"intensity\": \"low\"\n            },\n            ISOStage.GUIDANCE: {\n                \"duration_ratio\": 0.50,\n                \"description\": \"å¼•å¯¼æƒ…ç»ªå‘ç›®æ ‡è½¬æ¢\",\n                \"music_strategy\": \"gradual_transition\",\n                \"intensity\": \"medium\"\n            },\n            ISOStage.CONSOLIDATION: {\n                \"duration_ratio\": 0.25,\n                \"description\": \"å·©å›ºç›®æ ‡æƒ…ç»ªçŠ¶æ€\",\n                \"music_strategy\": \"stabilize_target\",\n                \"intensity\": \"low\"\n            }\n        }\n    \n    def plan_stages(self, total_duration: int, current_emotion: Dict, target_emotion: Dict) -> List[Dict]:\n        \"\"\"è§„åˆ’ISOä¸‰é˜¶æ®µ\"\"\"\n        stages = []\n        \n        for stage in ISOStage:\n            config = self.stage_configs[stage]\n            duration = int(total_duration * config[\"duration_ratio\"])\n            \n            stages.append({\n                \"stage\": stage.value,\n                \"duration_minutes\": duration,\n                \"description\": config[\"description\"],\n                \"music_strategy\": config[\"music_strategy\"],\n                \"intensity\": config[\"intensity\"]\n            })\n        \n        return stages\n    \n    def get_current_stage_config(self) -> Dict:\n        \"\"\"è·å–å½“å‰é˜¶æ®µé…ç½®\"\"\"\n        if self.current_stage:\n            return self.stage_configs[self.current_stage]\n        return {}\n''',\n            'prescriptions.py': '''\n\"\"\"å¤„æ–¹å¼•æ“ - Mockç‰ˆæœ¬\"\"\"\nfrom typing import Dict, List, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass TherapyPrescription:\n    prescription_id: str\n    user_profile: Dict\n    current_emotion: Dict\n    target_emotion: Dict\n    recommendations: Dict\n    confidence_score: float\n\nclass PrescriptionEngine:\n    \"\"\"å¤„æ–¹å¼•æ“\"\"\"\n    \n    def __init__(self):\n        self.prescription_templates = {\n            \"anxiety_to_calm\": {\n                \"music_genre\": \"ambient\",\n                \"tempo_range\": (40, 70),\n                \"key_preference\": \"major\",\n                \"video_theme\": \"nature\",\n                \"duration\": 25\n            },\n            \"sadness_to_comfort\": {\n                \"music_genre\": \"classical\",\n                \"tempo_range\": (50, 80),\n                \"key_preference\": \"minor_to_major\",\n                \"video_theme\": \"warm_colors\",\n                \"duration\": 30\n            },\n            \"anger_to_peace\": {\n                \"music_genre\": \"meditation\",\n                \"tempo_range\": (40, 60),\n                \"key_preference\": \"major\",\n                \"video_theme\": \"water\",\n                \"duration\": 35\n            }\n        }\n    \n    def generate_prescription(self, user_profile: Dict, current_emotion: Dict, target_emotion: Dict) -> TherapyPrescription:\n        \"\"\"ç”Ÿæˆæ²»ç–—å¤„æ–¹\"\"\"\n        \n        # ç®€å•çš„å¤„æ–¹åŒ¹é…é€»è¾‘\n        emotion_key = self._classify_emotion_transition(current_emotion, target_emotion)\n        template = self.prescription_templates.get(emotion_key, self.prescription_templates[\"anxiety_to_calm\"])\n        \n        prescription = TherapyPrescription(\n            prescription_id=f\"rx_{hash(str(current_emotion))%10000:04d}\",\n            user_profile=user_profile,\n            current_emotion=current_emotion,\n            target_emotion=target_emotion,\n            recommendations={\n                \"music\": {\n                    \"genre\": template[\"music_genre\"],\n                    \"tempo_bpm\": template[\"tempo_range\"],\n                    \"key\": template[\"key_preference\"]\n                },\n                \"video\": {\n                    \"theme\": template[\"video_theme\"],\n                    \"style\": \"soft_transitions\"\n                },\n                \"session\": {\n                    \"duration_minutes\": template[\"duration\"],\n                    \"stages\": [\"sync\", \"guide\", \"consolidate\"]\n                }\n            },\n            confidence_score=0.85\n        )\n        \n        return prescription\n    \n    def _classify_emotion_transition(self, current: Dict, target: Dict) -> str:\n        \"\"\"åˆ†ç±»æƒ…ç»ªè½¬æ¢ç±»å‹\"\"\"\n        current_valence = current.get(\"valence\", 0)\n        current_arousal = current.get(\"arousal\", 0.5)\n        \n        if current_arousal > 0.7 and current_valence < 0:\n            return \"anxiety_to_calm\"  # é«˜å”¤é†’è´Ÿæƒ…ç»ª\n        elif current_valence < -0.3:\n            return \"sadness_to_comfort\"  # ä½æ•ˆä»·\n        elif current_arousal > 0.8:\n            return \"anger_to_peace\"  # é«˜å”¤é†’\n        else:\n            return \"anxiety_to_calm\"  # é»˜è®¤\n    \n    def validate_prescription(self, prescription: TherapyPrescription) -> Dict:\n        \"\"\"éªŒè¯å¤„æ–¹æœ‰æ•ˆæ€§\"\"\"\n        return {\n            \"is_valid\": True,\n            \"confidence\": prescription.confidence_score,\n            \"warnings\": [],\n            \"suggestions\": [\"å»ºè®®åœ¨å®‰é™ç¯å¢ƒä¸­è¿›è¡Œ\", \"ä¿æŒèˆ’é€‚çš„åå§¿æˆ–èººå§¿\"]\n        }\n'''\n        }\n        \n        # å†™å…¥Mockæ–‡ä»¶\n        for filename, content in mock_therapy_files.items():\n            file_path = therapy_dir / filename\n            with open(file_path, 'w', encoding='utf-8') as f:\n                f.write(content)\n            print(f\"ğŸ”§ åˆ›å»º {filename}\")\n        \n        # åˆ›å»º__init__.py\n        init_file = therapy_dir / '__init__.py'\n        with open(init_file, 'w', encoding='utf-8') as f:\n            f.write('\"\"\"ç–—æ„ˆç³»ç»Ÿæ¨¡å—\"\"\"\\n')\n        \n        # é‡æ–°å¯¼å…¥\n        try:\n            import importlib\n            import sys\n            \n            # æ¸…ç†ç¼“å­˜\n            modules_to_clear = [name for name in sys.modules.keys() if name.startswith('src.therapy')]\n            for module_name in modules_to_clear:\n                del sys.modules[module_name]\n            \n            from src.therapy.core import TherapyOrchestrator, TherapySession, EmotionTrajectoryPlanner\n            from src.therapy.stages import ISOStageManager\n            from src.therapy.prescriptions import PrescriptionEngine\n            \n            print(\"âœ… Mockç–—æ„ˆç³»ç»Ÿåˆ›å»ºå¹¶å¯¼å…¥æˆåŠŸ\")\n            therapy_system_ok = True\n            \n        except Exception as e2:\n            print(f\"âŒ Mockç–—æ„ˆç³»ç»Ÿåˆ›å»ºå¤±è´¥: {e2}\")\n    \n    # æµ‹è¯•ç–—æ„ˆç³»ç»ŸåŠŸèƒ½\n    if therapy_system_ok:\n        try:\n            # åˆ›å»ºå¤„æ–¹å¼•æ“æµ‹è¯•\n            prescription_engine = PrescriptionEngine()\n            print(\"âœ… å¤„æ–¹å¼•æ“åˆå§‹åŒ–æˆåŠŸ\")\n            \n            # åˆ›å»ºç–—æ„ˆç¼–æ’å™¨æµ‹è¯•\n            orchestrator = TherapyOrchestrator()\n            \n            # æµ‹è¯•åˆ›å»ºä¼šè¯\n            test_current = {\"valence\": -0.4, \"arousal\": 0.8}  # ç„¦è™‘çŠ¶æ€\n            test_target = {\"valence\": 0.6, \"arousal\": 0.3}    # å¹³é™çŠ¶æ€\n            \n            session = orchestrator.create_session(\"test_user\", test_current, test_target)\n            recommendations = orchestrator.get_session_recommendations(session.session_id)\n            \n            print(f\"âœ… ç–—æ„ˆä¼šè¯åˆ›å»ºæˆåŠŸ: {session.session_id}\")\n            print(f\"âœ… è½¨è¿¹è§„åˆ’: {len(recommendations['trajectory'])}ä¸ªæ­¥éª¤\")\n            \n            # æµ‹è¯•å¤„æ–¹ç”Ÿæˆ\n            prescription = prescription_engine.generate_prescription(\n                {\"user_type\": \"student\"}, test_current, test_target\n            )\n            print(f\"âœ… å¤„æ–¹ç”ŸæˆæˆåŠŸ: {prescription.prescription_id}\")\n            \n        except Exception as e:\n            print(f\"âš ï¸  ç–—æ„ˆç³»ç»Ÿæµ‹è¯•è­¦å‘Š: {e}\")\n            \nexcept Exception as e:\n    print(f\"âŒ ç–—æ„ˆç³»ç»Ÿä¸¥é‡é”™è¯¯: {e}\")\n    therapy_system_ok = False\n\nif not therapy_system_ok:\n    print(\"ğŸ’¡ æç¤º: ç–—æ„ˆç³»ç»Ÿæµ‹è¯•å¤±è´¥ï¼Œä½†ç³»ç»Ÿä»å¯ç»§ç»­è¿è¡Œ\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ”¥ æ¨¡å‹ç¼“å­˜é¢„çƒ­:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"âš ï¸ æ­¤æ­¥éª¤å¯é€‰ï¼Œé¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ä¸‹è½½æ¨¡å‹\")\n",
    "print(\"å»ºè®®åœ¨ç½‘ç»œæ¡ä»¶è‰¯å¥½æ—¶è¿è¡Œ\")\n",
    "\n",
    "# é¢„çƒ­å°æ¨¡å‹ (è¾ƒå¿«)\n",
    "PRELOAD_MODELS = input(\"\\næ˜¯å¦é¢„åŠ è½½å°æ¨¡å‹è¿›è¡Œæµ‹è¯•? (y/n): \").lower().strip() == 'y'\n",
    "\n",
    "if PRELOAD_MODELS:\n",
    "    try:\n",
    "        print(\"\\nå¼€å§‹é¢„åŠ è½½æ¨¡å‹...\")\n",
    "        \n",
    "        # é¢„åŠ è½½æ–‡æœ¬æƒ…ç»ªè¯†åˆ«æ¨¡å‹ (è¾ƒå°)\n",
    "        print(\"ğŸ“¥ é¢„åŠ è½½æ–‡æœ¬æƒ…ç»ªè¯†åˆ«æ¨¡å‹...\")\n",
    "        from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "        \n",
    "        model_id = \"cardiffnlp/twitter-roberta-base-emotion\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_id, \n",
    "            cache_dir=cache_dir / 'transformers'\n",
    "        )\n",
    "        print(f\"âœ… TokenizeråŠ è½½æˆåŠŸ: {model_id}\")\n",
    "        \n",
    "        # å¦‚æœGPUå¯ç”¨ä¸”å†…å­˜å……è¶³ï¼Œé¢„åŠ è½½æ¨¡å‹\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "            if gpu_memory >= 8:  # è‡³å°‘8GBæ‰åŠ è½½æ¨¡å‹\n",
    "                model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                    model_id,\n",
    "                    cache_dir=cache_dir / 'transformers'\n",
    "                )\n",
    "                print(f\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_id}\")\n",
    "                \n",
    "                # é‡Šæ”¾å†…å­˜\n",
    "                del model\n",
    "                torch.cuda.empty_cache()\n",
    "                print(\"âœ… GPUå†…å­˜å·²æ¸…ç†\")\n",
    "            else:\n",
    "                print(\"âš ï¸ GPUå†…å­˜ä¸è¶³ï¼Œè·³è¿‡æ¨¡å‹é¢„åŠ è½½\")\n",
    "        \n",
    "        del tokenizer\n",
    "        gc.collect()\n",
    "        print(\"âœ… æ¨¡å‹ç¼“å­˜é¢„çƒ­å®Œæˆ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¨¡å‹é¢„çƒ­å¤±è´¥: {e}\")\n",
    "        print(\"è¿™å¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜ï¼Œå¯ä»¥ç¨åå†è¯•\")\nelse:\n",
    "    print(\"â­ï¸ è·³è¿‡æ¨¡å‹é¢„çƒ­\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ç³»ç»ŸçŠ¶æ€æ€»ç»“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ã€Šå¿ƒå¢ƒæµè½¬ã€‹ç³»ç»Ÿåˆå§‹åŒ–æ€»ç»“\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# æ”¶é›†çŠ¶æ€ä¿¡æ¯\n",
    "status_report = {\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'environment': {\n",
    "        'python_version': sys.version.split()[0],\n",
    "        'platform': platform.system(),\n",
    "        'working_directory': str(PROJECT_ROOT)\n",
    "    },\n",
    "    'resources': {\n",
    "        'cpu_cores': cpu_count,\n",
    "        'total_memory_gb': round(memory_gb, 1),\n",
    "        'available_memory_gb': round(memory_available_gb, 1),\n",
    "        'disk_free_gb': round(disk_free_gb, 1)\n",
    "    },\n",
    "    'gpu': {\n",
    "        'cuda_available': torch.cuda.is_available() if 'torch' in locals() else False,\n",
    "        'gpu_count': torch.cuda.device_count() if 'torch' in locals() and torch.cuda.is_available() else 0\n",
    "    },\n",
    "    'dependencies': {\n",
    "        'core_installed': len(missing_packages) == 0,\n",
    "        'missing_packages': missing_packages\n",
    "    },\n",
    "    'modules': {\n",
    "        'theory_modules': 'iso_planner' in locals(),\n",
    "        'model_adapters': 'ModelFactory' in locals(),\n",
    "        'therapy_system': 'TherapyOrchestrator' in locals()\n",
    "    }\n",
    "}\n",
    "\n",
    "# æ·»åŠ GPUè¯¦ç»†ä¿¡æ¯\n",
    "if status_report['gpu']['cuda_available']:\n",
    "    gpu_info = []\n",
    "    for i in range(status_report['gpu']['gpu_count']):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        gpu_info.append({\n",
    "            'name': props.name,\n",
    "            'memory_gb': round(props.total_memory / (1024**3), 1),\n",
    "            'compute_capability': f\"{props.major}.{props.minor}\"\n",
    "        })\n",
    "    status_report['gpu']['devices'] = gpu_info\n",
    "\n",
    "# æ˜¾ç¤ºçŠ¶æ€æŠ¥å‘Š\n",
    "print(f\"åˆå§‹åŒ–æ—¶é—´: {status_report['timestamp']}\")\n",
    "print(f\"Pythonç‰ˆæœ¬: {status_report['environment']['python_version']}\")\n",
    "print(f\"ç³»ç»Ÿå¹³å°: {status_report['environment']['platform']}\")\n",
    "print(f\"\\nğŸ’¾ ç³»ç»Ÿèµ„æº:\")\n",
    "print(f\"  CPUæ ¸å¿ƒ: {status_report['resources']['cpu_cores']}\")\n",
    "print(f\"  æ€»å†…å­˜: {status_report['resources']['total_memory_gb']} GB\")\n",
    "print(f\"  å¯ç”¨å†…å­˜: {status_report['resources']['available_memory_gb']} GB\")\n",
    "print(f\"  ç£ç›˜å¯ç”¨: {status_report['resources']['disk_free_gb']} GB\")\n",
    "\n",
    "print(f\"\\nğŸ® GPUçŠ¶æ€:\")\n",
    "if status_report['gpu']['cuda_available']:\n",
    "    print(f\"  CUDAå¯ç”¨: âœ…\")\n",
    "    print(f\"  GPUæ•°é‡: {status_report['gpu']['gpu_count']}\")\n",
    "    for i, gpu in enumerate(status_report['gpu']['devices']):\n",
    "        print(f\"  GPU {i}: {gpu['name']} ({gpu['memory_gb']} GB)\")\nelse:\n",
    "    print(f\"  CUDAå¯ç”¨: âŒ\")\n",
    "\n",
    "print(f\"\\nğŸ“¦ ä¾èµ–çŠ¶æ€:\")\n",
    "if status_report['dependencies']['core_installed']:\n",
    "    print(\"  æ ¸å¿ƒä¾èµ–: âœ… å…¨éƒ¨å·²å®‰è£…\")\nelse:\n",
    "    print(f\"  æ ¸å¿ƒä¾èµ–: âŒ ç¼ºå¤± {len(status_report['dependencies']['missing_packages'])} ä¸ª\")\n",
    "\n",
    "print(f\"\\nğŸ§© æ¨¡å—çŠ¶æ€:\")\n",
    "modules = status_report['modules']\n",
    "print(f\"  ç†è®ºæ¨¡å—: {'âœ…' if modules['theory_modules'] else 'âŒ'}\")\n",
    "print(f\"  æ¨¡å‹é€‚é…å™¨: {'âœ…' if modules['model_adapters'] else 'âŒ'}\")\n",
    "print(f\"  ç–—æ„ˆç³»ç»Ÿ: {'âœ…' if modules['therapy_system'] else 'âŒ'}\")\n",
    "\n",
    "# ç»™å‡ºæ€»ä½“è¯„ä¼°\n",
    "all_good = (\n",
    "    status_report['dependencies']['core_installed'] and\n",
    "    modules['theory_modules'] and\n",
    "    modules['model_adapters'] and\n",
    "    modules['therapy_system']\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ¯ ç³»ç»ŸçŠ¶æ€:\")\n",
    "if all_good:\n",
    "    print(\"âœ… ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼å¯ä»¥ç»§ç»­è¿è¡Œå…¶ä»–notebook\")\n",
    "    recommendation = \"å»ºè®®æŒ‰é¡ºåºè¿è¡Œï¼š02_theory_models_demo â†’ 03_model_adapters_test â†’ 04_therapy_session_demo\"\nelse:\n",
    "    print(\"âš ï¸ ç³»ç»Ÿåˆå§‹åŒ–å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯\")\n",
    "    recommendation = \"è¯·å…ˆè§£å†³ä¾èµ–åº“å’Œæ¨¡å—å¯¼å…¥é—®é¢˜ï¼Œç„¶åé‡æ–°è¿è¡Œæ­¤notebook\"\n",
    "\n",
    "print(f\"\\nğŸ’¡ å»ºè®®: {recommendation}\")\n",
    "\n",
    "# ä¿å­˜çŠ¶æ€æŠ¥å‘Š\n",
    "status_file = PROJECT_ROOT / 'outputs' / 'initialization_status.json'\n",
    "with open(status_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(status_report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nğŸ“„ çŠ¶æ€æŠ¥å‘Šå·²ä¿å­˜: {status_file}\")\nprint(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ åˆå§‹åŒ–å®Œæˆï¼\n",
    "\n",
    "å¦‚æœä¸Šè¿°æ‰€æœ‰æ£€æŸ¥éƒ½é€šè¿‡ï¼Œæ‚¨ç°åœ¨å¯ä»¥ï¼š\n",
    "\n",
    "1. **ç»§ç»­è¿è¡Œç†è®ºæ¼”ç¤º**: `02_theory_models_demo.ipynb`\n",
    "2. **æµ‹è¯•æ¨¡å‹é€‚é…å™¨**: `03_model_adapters_test.ipynb`  \n",
    "3. **ä½“éªŒå®Œæ•´ç–—æ„ˆæµç¨‹**: `04_therapy_session_demo.ipynb`\n",
    "\n",
    "### ğŸ“ é‡è¦æç¤º\n",
    "- æ‰€æœ‰è¾“å‡ºæ–‡ä»¶å°†ä¿å­˜åœ¨ `outputs/` ç›®å½•\n",
    "- æ¨¡å‹ç¼“å­˜ä½äº `outputs/cache/` ç›®å½•\n",
    "- å¦‚é‡é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ `10_troubleshooting_guide.ipynb`\n",
    "\n",
    "### ğŸ†˜ è·å–å¸®åŠ©\n",
    "å¦‚æœåˆå§‹åŒ–è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·ï¼š\n",
    "1. æ£€æŸ¥JupyterHubç¯å¢ƒçš„GPUå’Œå†…å­˜èµ„æº\n",
    "2. ç¡®è®¤ç½‘ç»œè¿æ¥æ­£å¸¸ (æ¨¡å‹ä¸‹è½½éœ€è¦)\n",
    "3. æŸ¥çœ‹é”™è¯¯æ—¥å¿—å®šä½é—®é¢˜\n",
    "4. å‚è€ƒæ•…éšœæ’é™¤æŒ‡å—"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}