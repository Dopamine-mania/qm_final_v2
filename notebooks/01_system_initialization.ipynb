{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《心境流转》系统初始化\n",
    "\n",
    "本notebook用于初始化和验证\"心境流转\"睡前音画疗愈系统的运行环境。\n",
    "\n",
    "## 🎯 目标\n",
    "- 检测JupyterHub环境和硬件资源\n",
    "- 验证所需依赖库的安装\n",
    "- 配置系统参数和路径\n",
    "- 进行基础功能测试\n",
    "\n",
    "## ⚠️ 重要提示\n",
    "- 首次运行此系统时必须执行此notebook\n",
    "- 确保具有足够的GPU内存 (40GB+)\n",
    "- 某些模型可能需要较长时间下载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"《心境流转》系统环境检测\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 基础环境信息\n",
    "print(f\"Python 版本: {sys.version}\")\n",
    "print(f\"操作系统: {platform.system()} {platform.release()}\")\n",
    "print(f\"CPU架构: {platform.machine()}\")\n",
    "print(f\"当前工作目录: {os.getcwd()}\")\n",
    "print(f\"Python路径: {sys.executable}\")\n",
    "print(f\"检测时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置项目根目录\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "print(f\"项目根目录: {PROJECT_ROOT}\")\n",
    "\n",
    "# 添加项目路径到sys.path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "    print(\"✅ 项目路径已添加到sys.path\")\n",
    "\n",
    "# 检查关键目录\n",
    "key_directories = [\n",
    "    'src',\n",
    "    'configs', \n",
    "    'research/theory',\n",
    "    'notebooks',\n",
    "    'outputs'\n",
    "]\n",
    "\n",
    "print(\"\\n📁 关键目录检查:\")\n",
    "for directory in key_directories:\n",
    "    dir_path = PROJECT_ROOT / directory\n",
    "    if dir_path.exists():\n",
    "        print(f\"✅ {directory}\")\n",
    "    else:\n",
    "        print(f\"❌ {directory} (缺失)\")\n",
    "        # 创建缺失的输出目录\n",
    "        if directory == 'outputs':\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"  📁 已创建输出目录: {dir_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 硬件资源检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import gc\n",
    "\n",
    "print(\"\\n🖥️ 系统资源检测:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# CPU信息\n",
    "cpu_count = psutil.cpu_count()\n",
    "cpu_freq = psutil.cpu_freq()\n",
    "print(f\"CPU核心数: {cpu_count}\")\n",
    "if cpu_freq:\n",
    "    print(f\"CPU频率: {cpu_freq.current:.2f} MHz\")\n",
    "\n",
    "# 内存信息\n",
    "memory = psutil.virtual_memory()\n",
    "memory_gb = memory.total / (1024**3)\n",
    "memory_available_gb = memory.available / (1024**3)\n",
    "print(f\"总内存: {memory_gb:.1f} GB\")\n",
    "print(f\"可用内存: {memory_available_gb:.1f} GB\")\n",
    "print(f\"内存使用率: {memory.percent}%\")\n",
    "\n",
    "# 磁盘信息\n",
    "disk = psutil.disk_usage('/')\n",
    "disk_total_gb = disk.total / (1024**3)\n",
    "disk_free_gb = disk.free / (1024**3)\n",
    "print(f\"磁盘总容量: {disk_total_gb:.1f} GB\")\n",
    "print(f\"磁盘可用: {disk_free_gb:.1f} GB\")\n",
    "\n",
    "# 资源充足性评估\n",
    "print(\"\\n📊 资源评估:\")\n",
    "if memory_gb >= 32:\n",
    "    print(\"✅ 内存充足 (>=32GB)\")\n",
    "elif memory_gb >= 16:\n",
    "    print(\"⚠️ 内存可用但可能不足 (16-32GB)\")\n",
    "else:\n",
    "    print(\"❌ 内存不足 (<16GB)\")\n",
    "\n",
    "if disk_free_gb >= 50:\n",
    "    print(\"✅ 磁盘空间充足 (>=50GB)\")\n",
    "elif disk_free_gb >= 20:\n",
    "    print(\"⚠️ 磁盘空间紧张 (20-50GB)\")\n",
    "else:\n",
    "    print(\"❌ 磁盘空间不足 (<20GB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU检测\n",
    "print(\"\\n🎮 GPU检测:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    \n",
    "    print(f\"PyTorch版本: {torch.__version__}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"✅ CUDA可用，发现 {gpu_count} 个GPU\")\n",
    "        print(f\"CUDA版本: {torch.version.cuda}\")\n",
    "        \n",
    "        for i in range(gpu_count):\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            total_memory = props.total_memory / (1024**3)\n",
    "            allocated_memory = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "            cached_memory = torch.cuda.memory_reserved(i) / (1024**3)\n",
    "            free_memory = total_memory - allocated_memory\n",
    "            \n",
    "            print(f\"\\nGPU {i}: {props.name}\")\n",
    "            print(f\"  计算能力: {props.major}.{props.minor}\")\n",
    "            print(f\"  总显存: {total_memory:.1f} GB\")\n",
    "            print(f\"  已分配: {allocated_memory:.2f} GB\")\n",
    "            print(f\"  已缓存: {cached_memory:.2f} GB\")\n",
    "            print(f\"  可用显存: {free_memory:.1f} GB\")\n",
    "            \n",
    "            # GPU评估\n",
    "            if total_memory >= 80:\n",
    "                gpu_status = \"✅ 80GB GPU - 优秀配置\"\n",
    "            elif total_memory >= 40:\n",
    "                gpu_status = \"✅ 40GB GPU - 推荐配置\"\n",
    "            elif total_memory >= 24:\n",
    "                gpu_status = \"⚠️ 24GB GPU - 可用但受限\"\n",
    "            elif total_memory >= 12:\n",
    "                gpu_status = \"⚠️ 12GB GPU - 仅支持小模型\"\n",
    "            else:\n",
    "                gpu_status = \"❌ <12GB GPU - 不建议使用\"\n",
    "            \n",
    "            print(f\"  状态: {gpu_status}\")\n",
    "    else:\n",
    "        print(\"❌ CUDA不可用，将使用CPU模式\")\n",
    "        print(\"⚠️ 警告: CPU模式下性能将显著降低\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"❌ PyTorch未安装，无法检测GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 依赖库检测和安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📦 关键依赖库检测:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 核心依赖库列表\n",
    "core_dependencies = {\n",
    "    'torch': '>=1.12.0',\n",
    "    'transformers': '>=4.20.0',\n",
    "    'diffusers': '>=0.20.0',\n",
    "    'numpy': '>=1.21.0',\n",
    "    'scipy': '>=1.8.0',\n",
    "    'librosa': '>=0.9.0',\n",
    "    'opencv-python': '>=4.5.0',\n",
    "    'pillow': '>=8.0.0',\n",
    "    'matplotlib': '>=3.5.0',\n",
    "    'seaborn': '>=0.11.0',\n",
    "    'plotly': '>=5.0.0',\n",
    "    'ipywidgets': '>=7.6.0',\n",
    "    'tqdm': '>=4.60.0',\n",
    "    'pyyaml': '>=5.4.0',\n",
    "    'psutil': '>=5.8.0'\n",
    "}\n",
    "\n",
    "missing_packages = []\n",
    "installed_packages = {}\n",
    "\n",
    "for package, min_version in core_dependencies.items():\n",
    "    try:\n",
    "        if package == 'opencv-python':\n",
    "            import cv2\n",
    "            version = cv2.__version__\n",
    "            package_name = 'cv2'\n",
    "        else:\n",
    "            module = __import__(package)\n",
    "            version = getattr(module, '__version__', 'unknown')\n",
    "            package_name = package\n",
    "        \n",
    "        installed_packages[package] = version\n",
    "        print(f\"✅ {package_name}: {version}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        missing_packages.append(package)\n",
    "        print(f\"❌ {package}: 未安装\")\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\n⚠️ 缺失依赖库: {', '.join(missing_packages)}\")\n",
    "    print(\"请运行以下命令安装:\")\n",
    "    print(f\"pip install {' '.join(missing_packages)}\")\n",
    "else:\n",
    "    print(\"\\n✅ 所有核心依赖库已安装\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可选依赖库检测 (用于特定功能)\n",
    "print(\"\\n📦 可选依赖库检测:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "optional_dependencies = {\n",
    "    'audiocraft': 'MusicGen音乐生成',\n",
    "    'accelerate': 'Hugging Face模型加速',\n",
    "    'xformers': '内存优化 (推荐)',\n",
    "    'pynvml': 'GPU监控',\n",
    "    'wandb': '实验跟踪 (可选)',\n",
    "    'tensorboard': 'TensorBoard日志',\n",
    "    'jupyter_contrib_nbextensions': 'Jupyter扩展',\n",
    "    'ipywidgets': 'Jupyter交互组件'\n",
    "}\n",
    "\n",
    "for package, description in optional_dependencies.items():\n",
    "    try:\n",
    "        module = __import__(package)\n",
    "        version = getattr(module, '__version__', 'unknown')\n",
    "        print(f\"✅ {package}: {version} - {description}\")\n",
    "    except ImportError:\n",
    "        print(f\"⚪ {package}: 未安装 - {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 系统配置初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n⚙️ 系统配置初始化:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 创建输出目录结构\n",
    "output_dirs = [\n",
    "    'outputs',\n",
    "    'outputs/audio',\n",
    "    'outputs/video', \n",
    "    'outputs/logs',\n",
    "    'outputs/models',\n",
    "    'outputs/reports',\n",
    "    'outputs/cache'\n",
    "]\n",
    "\n",
    "for dir_name in output_dirs:\n",
    "    dir_path = PROJECT_ROOT / dir_name\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"✅ 创建目录: {dir_path}\")\n",
    "\n",
    "# 设置缓存目录\n",
    "cache_dir = PROJECT_ROOT / 'outputs' / 'cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(cache_dir / 'transformers')\n",
    "os.environ['HF_HOME'] = str(cache_dir / 'huggingface')\n",
    "print(f\"✅ 设置缓存目录: {cache_dir}\")\n",
    "\n",
    "# 创建系统配置\n",
    "system_config = {\n",
    "    'project_root': str(PROJECT_ROOT),\n",
    "    'python_version': sys.version,\n",
    "    'platform': platform.platform(),\n",
    "    'initialization_time': datetime.now().isoformat(),\n",
    "    'gpu_available': torch.cuda.is_available() if 'torch' in locals() else False,\n",
    "    'gpu_count': torch.cuda.device_count() if 'torch' in locals() and torch.cuda.is_available() else 0,\n",
    "    'total_memory_gb': memory_gb,\n",
    "    'available_memory_gb': memory_available_gb,\n",
    "    'installed_packages': installed_packages,\n",
    "    'output_directories': [str(PROJECT_ROOT / d) for d in output_dirs]\n",
    "}\n",
    "\n",
    "# 保存配置文件\n",
    "config_file = PROJECT_ROOT / 'outputs' / 'system_config.json'\n",
    "with open(config_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(system_config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ 系统配置已保存: {config_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 基础功能测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🧪 基础功能测试:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 测试理论模块导入\n",
    "try:\n",
    "    from research.theory.iso_principle import ISOPrinciple, EmotionState\n",
    "    from research.theory.valence_arousal import ValenceArousalModel\n",
    "    from research.theory.sleep_physiology import SleepPhysiologyModel\n",
    "    from research.theory.music_psychology import MusicPsychologyModel\n",
    "    print(\"✅ 理论模块导入成功\")\n",
    "    \n",
    "    # 快速功能测试\n",
    "    emotion_state = EmotionState(valence=-0.2, arousal=0.6, confidence=0.8)\n",
    "    iso_planner = ISOPrinciple()\n",
    "    va_model = ValenceArousalModel()\n",
    "    \n",
    "    print(f\"✅ 情绪状态创建: V={emotion_state.valence}, A={emotion_state.arousal}\")\n",
    "    print(\"✅ 理论模型初始化成功\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 理论模块测试失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试模型适配器导入\n",
    "try:\n",
    "    from src.models.base import BaseModelAdapter, ModelConfig, HardwareConfig\n",
    "    from src.models.registry import ModelRegistry\n",
    "    from src.models.factory import ModelFactory\n",
    "    print(\"✅ 模型适配器导入成功\")\n",
    "    \n",
    "    # 创建硬件配置\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        if gpu_memory >= 75:\n",
    "            hardware_profile = \"gpu_80gb\"\n",
    "        elif gpu_memory >= 35:\n",
    "            hardware_profile = \"gpu_40gb\"\n",
    "        else:\n",
    "            hardware_profile = \"gpu_low\"\n",
    "    else:\n",
    "        hardware_profile = \"cpu_only\"\n",
    "    \n",
    "    print(f\"✅ 硬件配置检测: {hardware_profile}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 模型适配器测试失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试疗愈系统导入\n",
    "try:\n",
    "    from src.therapy.core import TherapyOrchestrator, TherapySession, EmotionTrajectoryPlanner\n",
    "    from src.therapy.stages import ISOStageManager\n",
    "    from src.therapy.prescriptions import PrescriptionEngine\n",
    "    print(\"✅ 疗愈系统导入成功\")\n",
    "    \n",
    "    # 创建处方引擎测试\n",
    "    prescription_engine = PrescriptionEngine()\n",
    "    print(\"✅ 处方引擎初始化成功\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 疗愈系统测试失败: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 模型缓存预热 (可选)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🔥 模型缓存预热:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"⚠️ 此步骤可选，首次运行可能需要较长时间下载模型\")\n",
    "print(\"建议在网络条件良好时运行\")\n",
    "\n",
    "# 预热小模型 (较快)\n",
    "PRELOAD_MODELS = input(\"\\n是否预加载小模型进行测试? (y/n): \").lower().strip() == 'y'\n",
    "\n",
    "if PRELOAD_MODELS:\n",
    "    try:\n",
    "        print(\"\\n开始预加载模型...\")\n",
    "        \n",
    "        # 预加载文本情绪识别模型 (较小)\n",
    "        print(\"📥 预加载文本情绪识别模型...\")\n",
    "        from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "        \n",
    "        model_id = \"cardiffnlp/twitter-roberta-base-emotion\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_id, \n",
    "            cache_dir=cache_dir / 'transformers'\n",
    "        )\n",
    "        print(f\"✅ Tokenizer加载成功: {model_id}\")\n",
    "        \n",
    "        # 如果GPU可用且内存充足，预加载模型\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "            if gpu_memory >= 8:  # 至少8GB才加载模型\n",
    "                model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                    model_id,\n",
    "                    cache_dir=cache_dir / 'transformers'\n",
    "                )\n",
    "                print(f\"✅ 模型加载成功: {model_id}\")\n",
    "                \n",
    "                # 释放内存\n",
    "                del model\n",
    "                torch.cuda.empty_cache()\n",
    "                print(\"✅ GPU内存已清理\")\n",
    "            else:\n",
    "                print(\"⚠️ GPU内存不足，跳过模型预加载\")\n",
    "        \n",
    "        del tokenizer\n",
    "        gc.collect()\n",
    "        print(\"✅ 模型缓存预热完成\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 模型预热失败: {e}\")\n",
    "        print(\"这可能是网络问题，可以稍后再试\")\nelse:\n",
    "    print(\"⏭️ 跳过模型预热\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 系统状态总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"《心境流转》系统初始化总结\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 收集状态信息\n",
    "status_report = {\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'environment': {\n",
    "        'python_version': sys.version.split()[0],\n",
    "        'platform': platform.system(),\n",
    "        'working_directory': str(PROJECT_ROOT)\n",
    "    },\n",
    "    'resources': {\n",
    "        'cpu_cores': cpu_count,\n",
    "        'total_memory_gb': round(memory_gb, 1),\n",
    "        'available_memory_gb': round(memory_available_gb, 1),\n",
    "        'disk_free_gb': round(disk_free_gb, 1)\n",
    "    },\n",
    "    'gpu': {\n",
    "        'cuda_available': torch.cuda.is_available() if 'torch' in locals() else False,\n",
    "        'gpu_count': torch.cuda.device_count() if 'torch' in locals() and torch.cuda.is_available() else 0\n",
    "    },\n",
    "    'dependencies': {\n",
    "        'core_installed': len(missing_packages) == 0,\n",
    "        'missing_packages': missing_packages\n",
    "    },\n",
    "    'modules': {\n",
    "        'theory_modules': 'iso_planner' in locals(),\n",
    "        'model_adapters': 'ModelFactory' in locals(),\n",
    "        'therapy_system': 'TherapyOrchestrator' in locals()\n",
    "    }\n",
    "}\n",
    "\n",
    "# 添加GPU详细信息\n",
    "if status_report['gpu']['cuda_available']:\n",
    "    gpu_info = []\n",
    "    for i in range(status_report['gpu']['gpu_count']):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        gpu_info.append({\n",
    "            'name': props.name,\n",
    "            'memory_gb': round(props.total_memory / (1024**3), 1),\n",
    "            'compute_capability': f\"{props.major}.{props.minor}\"\n",
    "        })\n",
    "    status_report['gpu']['devices'] = gpu_info\n",
    "\n",
    "# 显示状态报告\n",
    "print(f\"初始化时间: {status_report['timestamp']}\")\n",
    "print(f\"Python版本: {status_report['environment']['python_version']}\")\n",
    "print(f\"系统平台: {status_report['environment']['platform']}\")\n",
    "print(f\"\\n💾 系统资源:\")\n",
    "print(f\"  CPU核心: {status_report['resources']['cpu_cores']}\")\n",
    "print(f\"  总内存: {status_report['resources']['total_memory_gb']} GB\")\n",
    "print(f\"  可用内存: {status_report['resources']['available_memory_gb']} GB\")\n",
    "print(f\"  磁盘可用: {status_report['resources']['disk_free_gb']} GB\")\n",
    "\n",
    "print(f\"\\n🎮 GPU状态:\")\n",
    "if status_report['gpu']['cuda_available']:\n",
    "    print(f\"  CUDA可用: ✅\")\n",
    "    print(f\"  GPU数量: {status_report['gpu']['gpu_count']}\")\n",
    "    for i, gpu in enumerate(status_report['gpu']['devices']):\n",
    "        print(f\"  GPU {i}: {gpu['name']} ({gpu['memory_gb']} GB)\")\nelse:\n",
    "    print(f\"  CUDA可用: ❌\")\n",
    "\n",
    "print(f\"\\n📦 依赖状态:\")\n",
    "if status_report['dependencies']['core_installed']:\n",
    "    print(\"  核心依赖: ✅ 全部已安装\")\nelse:\n",
    "    print(f\"  核心依赖: ❌ 缺失 {len(status_report['dependencies']['missing_packages'])} 个\")\n",
    "\n",
    "print(f\"\\n🧩 模块状态:\")\n",
    "modules = status_report['modules']\n",
    "print(f\"  理论模块: {'✅' if modules['theory_modules'] else '❌'}\")\n",
    "print(f\"  模型适配器: {'✅' if modules['model_adapters'] else '❌'}\")\n",
    "print(f\"  疗愈系统: {'✅' if modules['therapy_system'] else '❌'}\")\n",
    "\n",
    "# 给出总体评估\n",
    "all_good = (\n",
    "    status_report['dependencies']['core_installed'] and\n",
    "    modules['theory_modules'] and\n",
    "    modules['model_adapters'] and\n",
    "    modules['therapy_system']\n",
    ")\n",
    "\n",
    "print(f\"\\n🎯 系统状态:\")\n",
    "if all_good:\n",
    "    print(\"✅ 系统初始化成功！可以继续运行其他notebook\")\n",
    "    recommendation = \"建议按顺序运行：02_theory_models_demo → 03_model_adapters_test → 04_therapy_session_demo\"\nelse:\n",
    "    print(\"⚠️ 系统初始化存在问题，请检查上述错误信息\")\n",
    "    recommendation = \"请先解决依赖库和模块导入问题，然后重新运行此notebook\"\n",
    "\n",
    "print(f\"\\n💡 建议: {recommendation}\")\n",
    "\n",
    "# 保存状态报告\n",
    "status_file = PROJECT_ROOT / 'outputs' / 'initialization_status.json'\n",
    "with open(status_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(status_report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n📄 状态报告已保存: {status_file}\")\nprint(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 初始化完成！\n",
    "\n",
    "如果上述所有检查都通过，您现在可以：\n",
    "\n",
    "1. **继续运行理论演示**: `02_theory_models_demo.ipynb`\n",
    "2. **测试模型适配器**: `03_model_adapters_test.ipynb`  \n",
    "3. **体验完整疗愈流程**: `04_therapy_session_demo.ipynb`\n",
    "\n",
    "### 📝 重要提示\n",
    "- 所有输出文件将保存在 `outputs/` 目录\n",
    "- 模型缓存位于 `outputs/cache/` 目录\n",
    "- 如遇问题，请查看 `10_troubleshooting_guide.ipynb`\n",
    "\n",
    "### 🆘 获取帮助\n",
    "如果初始化过程中遇到问题，请：\n",
    "1. 检查JupyterHub环境的GPU和内存资源\n",
    "2. 确认网络连接正常 (模型下载需要)\n",
    "3. 查看错误日志定位问题\n",
    "4. 参考故障排除指南"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}