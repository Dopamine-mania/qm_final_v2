{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ã€Šå¿ƒå¢ƒæµè½¬ã€‹ç³»ç»Ÿåˆå§‹åŒ–\n",
    "\n",
    "æœ¬notebookç”¨äºåˆå§‹åŒ–å’ŒéªŒè¯\"å¿ƒå¢ƒæµè½¬\"ç¡å‰éŸ³ç”»ç–—æ„ˆç³»ç»Ÿçš„è¿è¡Œç¯å¢ƒã€‚\n",
    "\n",
    "## ğŸ¯ ç›®æ ‡\n",
    "- æ£€æµ‹JupyterHubç¯å¢ƒå’Œç¡¬ä»¶èµ„æº\n",
    "- éªŒè¯æ‰€éœ€ä¾èµ–åº“çš„å®‰è£…\n",
    "- é…ç½®ç³»ç»Ÿå‚æ•°å’Œè·¯å¾„\n",
    "- è¿›è¡ŒåŸºç¡€åŠŸèƒ½æµ‹è¯•\n",
    "\n",
    "## âš ï¸ é‡è¦æç¤º\n",
    "- é¦–æ¬¡è¿è¡Œæ­¤ç³»ç»Ÿæ—¶å¿…é¡»æ‰§è¡Œæ­¤notebook\n",
    "- ç¡®ä¿å…·æœ‰è¶³å¤Ÿçš„GPUå†…å­˜ (40GB+)\n",
    "- æŸäº›æ¨¡å‹å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ä¸‹è½½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒæ£€æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ã€Šå¿ƒå¢ƒæµè½¬ã€‹ç³»ç»Ÿç¯å¢ƒæ£€æµ‹\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# åŸºç¡€ç¯å¢ƒä¿¡æ¯\n",
    "print(f\"Python ç‰ˆæœ¬: {sys.version}\")\n",
    "print(f\"æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}\")\n",
    "print(f\"CPUæ¶æ„: {platform.machine()}\")\n",
    "print(f\"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}\")\n",
    "print(f\"Pythonè·¯å¾„: {sys.executable}\")\n",
    "print(f\"æ£€æµ‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "print(f\"é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}\")\n",
    "\n",
    "# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "    print(\"âœ… é¡¹ç›®è·¯å¾„å·²æ·»åŠ åˆ°sys.path\")\n",
    "\n",
    "# æ£€æŸ¥å…³é”®ç›®å½•\n",
    "key_directories = [\n",
    "    'src',\n",
    "    'configs', \n",
    "    'research/theory',\n",
    "    'notebooks',\n",
    "    'outputs'\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ“ å…³é”®ç›®å½•æ£€æŸ¥:\")\n",
    "for directory in key_directories:\n",
    "    dir_path = PROJECT_ROOT / directory\n",
    "    if dir_path.exists():\n",
    "        print(f\"âœ… {directory}\")\n",
    "    else:\n",
    "        print(f\"âŒ {directory} (ç¼ºå¤±)\")\n",
    "        # åˆ›å»ºç¼ºå¤±çš„è¾“å‡ºç›®å½•\n",
    "        if directory == 'outputs':\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"  ğŸ“ å·²åˆ›å»ºè¾“å‡ºç›®å½•: {dir_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ç¡¬ä»¶èµ„æºæ£€æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import gc\n",
    "\n",
    "print(\"\\nğŸ–¥ï¸ ç³»ç»Ÿèµ„æºæ£€æµ‹:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# CPUä¿¡æ¯\n",
    "cpu_count = psutil.cpu_count()\n",
    "cpu_freq = psutil.cpu_freq()\n",
    "print(f\"CPUæ ¸å¿ƒæ•°: {cpu_count}\")\n",
    "if cpu_freq:\n",
    "    print(f\"CPUé¢‘ç‡: {cpu_freq.current:.2f} MHz\")\n",
    "\n",
    "# å†…å­˜ä¿¡æ¯\n",
    "memory = psutil.virtual_memory()\n",
    "memory_gb = memory.total / (1024**3)\n",
    "memory_available_gb = memory.available / (1024**3)\n",
    "print(f\"æ€»å†…å­˜: {memory_gb:.1f} GB\")\n",
    "print(f\"å¯ç”¨å†…å­˜: {memory_available_gb:.1f} GB\")\n",
    "print(f\"å†…å­˜ä½¿ç”¨ç‡: {memory.percent}%\")\n",
    "\n",
    "# ç£ç›˜ä¿¡æ¯\n",
    "disk = psutil.disk_usage('/')\n",
    "disk_total_gb = disk.total / (1024**3)\n",
    "disk_free_gb = disk.free / (1024**3)\n",
    "print(f\"ç£ç›˜æ€»å®¹é‡: {disk_total_gb:.1f} GB\")\n",
    "print(f\"ç£ç›˜å¯ç”¨: {disk_free_gb:.1f} GB\")\n",
    "\n",
    "# èµ„æºå……è¶³æ€§è¯„ä¼°\n",
    "print(\"\\nğŸ“Š èµ„æºè¯„ä¼°:\")\n",
    "if memory_gb >= 32:\n",
    "    print(\"âœ… å†…å­˜å……è¶³ (>=32GB)\")\n",
    "elif memory_gb >= 16:\n",
    "    print(\"âš ï¸ å†…å­˜å¯ç”¨ä½†å¯èƒ½ä¸è¶³ (16-32GB)\")\n",
    "else:\n",
    "    print(\"âŒ å†…å­˜ä¸è¶³ (<16GB)\")\n",
    "\n",
    "if disk_free_gb >= 50:\n",
    "    print(\"âœ… ç£ç›˜ç©ºé—´å……è¶³ (>=50GB)\")\n",
    "elif disk_free_gb >= 20:\n",
    "    print(\"âš ï¸ ç£ç›˜ç©ºé—´ç´§å¼  (20-50GB)\")\n",
    "else:\n",
    "    print(\"âŒ ç£ç›˜ç©ºé—´ä¸è¶³ (<20GB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUæ£€æµ‹\n",
    "print(\"\\nğŸ® GPUæ£€æµ‹:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    \n",
    "    print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"âœ… CUDAå¯ç”¨ï¼Œå‘ç° {gpu_count} ä¸ªGPU\")\n",
    "        print(f\"CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "        \n",
    "        for i in range(gpu_count):\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            total_memory = props.total_memory / (1024**3)\n",
    "            allocated_memory = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "            cached_memory = torch.cuda.memory_reserved(i) / (1024**3)\n",
    "            free_memory = total_memory - allocated_memory\n",
    "            \n",
    "            print(f\"\\nGPU {i}: {props.name}\")\n",
    "            print(f\"  è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}\")\n",
    "            print(f\"  æ€»æ˜¾å­˜: {total_memory:.1f} GB\")\n",
    "            print(f\"  å·²åˆ†é…: {allocated_memory:.2f} GB\")\n",
    "            print(f\"  å·²ç¼“å­˜: {cached_memory:.2f} GB\")\n",
    "            print(f\"  å¯ç”¨æ˜¾å­˜: {free_memory:.1f} GB\")\n",
    "            \n",
    "            # GPUè¯„ä¼°\n",
    "            if total_memory >= 80:\n",
    "                gpu_status = \"âœ… 80GB GPU - ä¼˜ç§€é…ç½®\"\n",
    "            elif total_memory >= 40:\n",
    "                gpu_status = \"âœ… 40GB GPU - æ¨èé…ç½®\"\n",
    "            elif total_memory >= 24:\n",
    "                gpu_status = \"âš ï¸ 24GB GPU - å¯ç”¨ä½†å—é™\"\n",
    "            elif total_memory >= 12:\n",
    "                gpu_status = \"âš ï¸ 12GB GPU - ä»…æ”¯æŒå°æ¨¡å‹\"\n",
    "            else:\n",
    "                gpu_status = \"âŒ <12GB GPU - ä¸å»ºè®®ä½¿ç”¨\"\n",
    "            \n",
    "            print(f\"  çŠ¶æ€: {gpu_status}\")\n",
    "    else:\n",
    "        print(\"âŒ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼\")\n",
    "        print(\"âš ï¸ è­¦å‘Š: CPUæ¨¡å¼ä¸‹æ€§èƒ½å°†æ˜¾è‘—é™ä½\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"âŒ PyTorchæœªå®‰è£…ï¼Œæ— æ³•æ£€æµ‹GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ä¾èµ–åº“æ£€æµ‹å’Œå®‰è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“¦ å…³é”®ä¾èµ–åº“æ£€æµ‹:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# æ ¸å¿ƒä¾èµ–åº“åˆ—è¡¨\n",
    "core_dependencies = {\n",
    "    'torch': '>=1.12.0',\n",
    "    'transformers': '>=4.20.0',\n",
    "    'diffusers': '>=0.20.0',\n",
    "    'numpy': '>=1.21.0',\n",
    "    'scipy': '>=1.8.0',\n",
    "    'librosa': '>=0.9.0',\n",
    "    'opencv-python': '>=4.5.0',\n",
    "    'pillow': '>=8.0.0',\n",
    "    'matplotlib': '>=3.5.0',\n",
    "    'seaborn': '>=0.11.0',\n",
    "    'plotly': '>=5.0.0',\n",
    "    'ipywidgets': '>=7.6.0',\n",
    "    'tqdm': '>=4.60.0',\n",
    "    'pyyaml': '>=5.4.0',\n",
    "    'psutil': '>=5.8.0'\n",
    "}\n",
    "\n",
    "missing_packages = []\n",
    "installed_packages = {}\n",
    "\n",
    "for package, min_version in core_dependencies.items():\n",
    "    try:\n",
    "        if package == 'opencv-python':\n",
    "            import cv2\n",
    "            version = cv2.__version__\n",
    "            package_name = 'cv2'\n",
    "        else:\n",
    "            module = __import__(package)\n",
    "            version = getattr(module, '__version__', 'unknown')\n",
    "            package_name = package\n",
    "        \n",
    "        installed_packages[package] = version\n",
    "        print(f\"âœ… {package_name}: {version}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        missing_packages.append(package)\n",
    "        print(f\"âŒ {package}: æœªå®‰è£…\")\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\nâš ï¸ ç¼ºå¤±ä¾èµ–åº“: {', '.join(missing_packages)}\")\n",
    "    print(\"è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:\")\n",
    "    print(f\"pip install {' '.join(missing_packages)}\")\n",
    "else:\n",
    "    print(\"\\nâœ… æ‰€æœ‰æ ¸å¿ƒä¾èµ–åº“å·²å®‰è£…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯é€‰ä¾èµ–åº“æ£€æµ‹ (ç”¨äºç‰¹å®šåŠŸèƒ½)\n",
    "print(\"\\nğŸ“¦ å¯é€‰ä¾èµ–åº“æ£€æµ‹:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "optional_dependencies = {\n",
    "    'audiocraft': 'MusicGenéŸ³ä¹ç”Ÿæˆ',\n",
    "    'accelerate': 'Hugging Faceæ¨¡å‹åŠ é€Ÿ',\n",
    "    'xformers': 'å†…å­˜ä¼˜åŒ– (æ¨è)',\n",
    "    'pynvml': 'GPUç›‘æ§',\n",
    "    'wandb': 'å®éªŒè·Ÿè¸ª (å¯é€‰)',\n",
    "    'tensorboard': 'TensorBoardæ—¥å¿—',\n",
    "    'jupyter_contrib_nbextensions': 'Jupyteræ‰©å±•',\n",
    "    'ipywidgets': 'Jupyteräº¤äº’ç»„ä»¶'\n",
    "}\n",
    "\n",
    "for package, description in optional_dependencies.items():\n",
    "    try:\n",
    "        module = __import__(package)\n",
    "        version = getattr(module, '__version__', 'unknown')\n",
    "        print(f\"âœ… {package}: {version} - {description}\")\n",
    "    except ImportError:\n",
    "        print(f\"âšª {package}: æœªå®‰è£… - {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ç³»ç»Ÿé…ç½®åˆå§‹åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nâš™ï¸ ç³»ç»Ÿé…ç½®åˆå§‹åŒ–:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„\n",
    "output_dirs = [\n",
    "    'outputs',\n",
    "    'outputs/audio',\n",
    "    'outputs/video', \n",
    "    'outputs/logs',\n",
    "    'outputs/models',\n",
    "    'outputs/reports',\n",
    "    'outputs/cache'\n",
    "]\n",
    "\n",
    "for dir_name in output_dirs:\n",
    "    dir_path = PROJECT_ROOT / dir_name\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"âœ… åˆ›å»ºç›®å½•: {dir_path}\")\n",
    "\n",
    "# è®¾ç½®ç¼“å­˜ç›®å½•\n",
    "cache_dir = PROJECT_ROOT / 'outputs' / 'cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(cache_dir / 'transformers')\n",
    "os.environ['HF_HOME'] = str(cache_dir / 'huggingface')\n",
    "print(f\"âœ… è®¾ç½®ç¼“å­˜ç›®å½•: {cache_dir}\")\n",
    "\n",
    "# åˆ›å»ºç³»ç»Ÿé…ç½®\n",
    "system_config = {\n",
    "    'project_root': str(PROJECT_ROOT),\n",
    "    'python_version': sys.version,\n",
    "    'platform': platform.platform(),\n",
    "    'initialization_time': datetime.now().isoformat(),\n",
    "    'gpu_available': torch.cuda.is_available() if 'torch' in locals() else False,\n",
    "    'gpu_count': torch.cuda.device_count() if 'torch' in locals() and torch.cuda.is_available() else 0,\n",
    "    'total_memory_gb': memory_gb,\n",
    "    'available_memory_gb': memory_available_gb,\n",
    "    'installed_packages': installed_packages,\n",
    "    'output_directories': [str(PROJECT_ROOT / d) for d in output_dirs]\n",
    "}\n",
    "\n",
    "# ä¿å­˜é…ç½®æ–‡ä»¶\n",
    "config_file = PROJECT_ROOT / 'outputs' / 'system_config.json'\n",
    "with open(config_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(system_config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… ç³»ç»Ÿé…ç½®å·²ä¿å­˜: {config_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. åŸºç¡€åŠŸèƒ½æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ§ª åŸºç¡€åŠŸèƒ½æµ‹è¯•:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# æµ‹è¯•ç†è®ºæ¨¡å—å¯¼å…¥\n",
    "try:\n",
    "    from research.theory.iso_principle import ISOPrinciple, EmotionState\n",
    "    from research.theory.valence_arousal import ValenceArousalModel\n",
    "    from research.theory.sleep_physiology import SleepPhysiologyModel\n",
    "    from research.theory.music_psychology import MusicPsychologyModel\n",
    "    print(\"âœ… ç†è®ºæ¨¡å—å¯¼å…¥æˆåŠŸ\")\n",
    "    \n",
    "    # å¿«é€ŸåŠŸèƒ½æµ‹è¯•\n",
    "    emotion_state = EmotionState(valence=-0.2, arousal=0.6, confidence=0.8)\n",
    "    iso_planner = ISOPrinciple()\n",
    "    va_model = ValenceArousalModel()\n",
    "    \n",
    "    print(f\"âœ… æƒ…ç»ªçŠ¶æ€åˆ›å»º: V={emotion_state.valence}, A={emotion_state.arousal}\")\n",
    "    print(\"âœ… ç†è®ºæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ç†è®ºæ¨¡å—æµ‹è¯•å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•æ¨¡å‹é€‚é…å™¨å¯¼å…¥\n",
    "try:\n",
    "    from src.models.base import BaseModelAdapter, ModelConfig, HardwareConfig\n",
    "    from src.models.registry import ModelRegistry\n",
    "    from src.models.factory import ModelFactory\n",
    "    print(\"âœ… æ¨¡å‹é€‚é…å™¨å¯¼å…¥æˆåŠŸ\")\n",
    "    \n",
    "    # åˆ›å»ºç¡¬ä»¶é…ç½®\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        if gpu_memory >= 75:\n",
    "            hardware_profile = \"gpu_80gb\"\n",
    "        elif gpu_memory >= 35:\n",
    "            hardware_profile = \"gpu_40gb\"\n",
    "        else:\n",
    "            hardware_profile = \"gpu_low\"\n",
    "    else:\n",
    "        hardware_profile = \"cpu_only\"\n",
    "    \n",
    "    print(f\"âœ… ç¡¬ä»¶é…ç½®æ£€æµ‹: {hardware_profile}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ¨¡å‹é€‚é…å™¨æµ‹è¯•å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ç–—æ„ˆç³»ç»Ÿå¯¼å…¥\n",
    "try:\n",
    "    from src.therapy.core import TherapyOrchestrator, TherapySession, EmotionTrajectoryPlanner\n",
    "    from src.therapy.stages import ISOStageManager\n",
    "    from src.therapy.prescriptions import PrescriptionEngine\n",
    "    print(\"âœ… ç–—æ„ˆç³»ç»Ÿå¯¼å…¥æˆåŠŸ\")\n",
    "    \n",
    "    # åˆ›å»ºå¤„æ–¹å¼•æ“æµ‹è¯•\n",
    "    prescription_engine = PrescriptionEngine()\n",
    "    print(\"âœ… å¤„æ–¹å¼•æ“åˆå§‹åŒ–æˆåŠŸ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ç–—æ„ˆç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ¨¡å‹ç¼“å­˜é¢„çƒ­ (å¯é€‰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ”¥ æ¨¡å‹ç¼“å­˜é¢„çƒ­:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"âš ï¸ æ­¤æ­¥éª¤å¯é€‰ï¼Œé¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ä¸‹è½½æ¨¡å‹\")\n",
    "print(\"å»ºè®®åœ¨ç½‘ç»œæ¡ä»¶è‰¯å¥½æ—¶è¿è¡Œ\")\n",
    "\n",
    "# é¢„çƒ­å°æ¨¡å‹ (è¾ƒå¿«)\n",
    "PRELOAD_MODELS = input(\"\\næ˜¯å¦é¢„åŠ è½½å°æ¨¡å‹è¿›è¡Œæµ‹è¯•? (y/n): \").lower().strip() == 'y'\n",
    "\n",
    "if PRELOAD_MODELS:\n",
    "    try:\n",
    "        print(\"\\nå¼€å§‹é¢„åŠ è½½æ¨¡å‹...\")\n",
    "        \n",
    "        # é¢„åŠ è½½æ–‡æœ¬æƒ…ç»ªè¯†åˆ«æ¨¡å‹ (è¾ƒå°)\n",
    "        print(\"ğŸ“¥ é¢„åŠ è½½æ–‡æœ¬æƒ…ç»ªè¯†åˆ«æ¨¡å‹...\")\n",
    "        from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "        \n",
    "        model_id = \"cardiffnlp/twitter-roberta-base-emotion\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_id, \n",
    "            cache_dir=cache_dir / 'transformers'\n",
    "        )\n",
    "        print(f\"âœ… TokenizeråŠ è½½æˆåŠŸ: {model_id}\")\n",
    "        \n",
    "        # å¦‚æœGPUå¯ç”¨ä¸”å†…å­˜å……è¶³ï¼Œé¢„åŠ è½½æ¨¡å‹\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "            if gpu_memory >= 8:  # è‡³å°‘8GBæ‰åŠ è½½æ¨¡å‹\n",
    "                model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                    model_id,\n",
    "                    cache_dir=cache_dir / 'transformers'\n",
    "                )\n",
    "                print(f\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_id}\")\n",
    "                \n",
    "                # é‡Šæ”¾å†…å­˜\n",
    "                del model\n",
    "                torch.cuda.empty_cache()\n",
    "                print(\"âœ… GPUå†…å­˜å·²æ¸…ç†\")\n",
    "            else:\n",
    "                print(\"âš ï¸ GPUå†…å­˜ä¸è¶³ï¼Œè·³è¿‡æ¨¡å‹é¢„åŠ è½½\")\n",
    "        \n",
    "        del tokenizer\n",
    "        gc.collect()\n",
    "        print(\"âœ… æ¨¡å‹ç¼“å­˜é¢„çƒ­å®Œæˆ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¨¡å‹é¢„çƒ­å¤±è´¥: {e}\")\n",
    "        print(\"è¿™å¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜ï¼Œå¯ä»¥ç¨åå†è¯•\")\nelse:\n",
    "    print(\"â­ï¸ è·³è¿‡æ¨¡å‹é¢„çƒ­\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ç³»ç»ŸçŠ¶æ€æ€»ç»“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ã€Šå¿ƒå¢ƒæµè½¬ã€‹ç³»ç»Ÿåˆå§‹åŒ–æ€»ç»“\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# æ”¶é›†çŠ¶æ€ä¿¡æ¯\n",
    "status_report = {\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'environment': {\n",
    "        'python_version': sys.version.split()[0],\n",
    "        'platform': platform.system(),\n",
    "        'working_directory': str(PROJECT_ROOT)\n",
    "    },\n",
    "    'resources': {\n",
    "        'cpu_cores': cpu_count,\n",
    "        'total_memory_gb': round(memory_gb, 1),\n",
    "        'available_memory_gb': round(memory_available_gb, 1),\n",
    "        'disk_free_gb': round(disk_free_gb, 1)\n",
    "    },\n",
    "    'gpu': {\n",
    "        'cuda_available': torch.cuda.is_available() if 'torch' in locals() else False,\n",
    "        'gpu_count': torch.cuda.device_count() if 'torch' in locals() and torch.cuda.is_available() else 0\n",
    "    },\n",
    "    'dependencies': {\n",
    "        'core_installed': len(missing_packages) == 0,\n",
    "        'missing_packages': missing_packages\n",
    "    },\n",
    "    'modules': {\n",
    "        'theory_modules': 'iso_planner' in locals(),\n",
    "        'model_adapters': 'ModelFactory' in locals(),\n",
    "        'therapy_system': 'TherapyOrchestrator' in locals()\n",
    "    }\n",
    "}\n",
    "\n",
    "# æ·»åŠ GPUè¯¦ç»†ä¿¡æ¯\n",
    "if status_report['gpu']['cuda_available']:\n",
    "    gpu_info = []\n",
    "    for i in range(status_report['gpu']['gpu_count']):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        gpu_info.append({\n",
    "            'name': props.name,\n",
    "            'memory_gb': round(props.total_memory / (1024**3), 1),\n",
    "            'compute_capability': f\"{props.major}.{props.minor}\"\n",
    "        })\n",
    "    status_report['gpu']['devices'] = gpu_info\n",
    "\n",
    "# æ˜¾ç¤ºçŠ¶æ€æŠ¥å‘Š\n",
    "print(f\"åˆå§‹åŒ–æ—¶é—´: {status_report['timestamp']}\")\n",
    "print(f\"Pythonç‰ˆæœ¬: {status_report['environment']['python_version']}\")\n",
    "print(f\"ç³»ç»Ÿå¹³å°: {status_report['environment']['platform']}\")\n",
    "print(f\"\\nğŸ’¾ ç³»ç»Ÿèµ„æº:\")\n",
    "print(f\"  CPUæ ¸å¿ƒ: {status_report['resources']['cpu_cores']}\")\n",
    "print(f\"  æ€»å†…å­˜: {status_report['resources']['total_memory_gb']} GB\")\n",
    "print(f\"  å¯ç”¨å†…å­˜: {status_report['resources']['available_memory_gb']} GB\")\n",
    "print(f\"  ç£ç›˜å¯ç”¨: {status_report['resources']['disk_free_gb']} GB\")\n",
    "\n",
    "print(f\"\\nğŸ® GPUçŠ¶æ€:\")\n",
    "if status_report['gpu']['cuda_available']:\n",
    "    print(f\"  CUDAå¯ç”¨: âœ…\")\n",
    "    print(f\"  GPUæ•°é‡: {status_report['gpu']['gpu_count']}\")\n",
    "    for i, gpu in enumerate(status_report['gpu']['devices']):\n",
    "        print(f\"  GPU {i}: {gpu['name']} ({gpu['memory_gb']} GB)\")\nelse:\n",
    "    print(f\"  CUDAå¯ç”¨: âŒ\")\n",
    "\n",
    "print(f\"\\nğŸ“¦ ä¾èµ–çŠ¶æ€:\")\n",
    "if status_report['dependencies']['core_installed']:\n",
    "    print(\"  æ ¸å¿ƒä¾èµ–: âœ… å…¨éƒ¨å·²å®‰è£…\")\nelse:\n",
    "    print(f\"  æ ¸å¿ƒä¾èµ–: âŒ ç¼ºå¤± {len(status_report['dependencies']['missing_packages'])} ä¸ª\")\n",
    "\n",
    "print(f\"\\nğŸ§© æ¨¡å—çŠ¶æ€:\")\n",
    "modules = status_report['modules']\n",
    "print(f\"  ç†è®ºæ¨¡å—: {'âœ…' if modules['theory_modules'] else 'âŒ'}\")\n",
    "print(f\"  æ¨¡å‹é€‚é…å™¨: {'âœ…' if modules['model_adapters'] else 'âŒ'}\")\n",
    "print(f\"  ç–—æ„ˆç³»ç»Ÿ: {'âœ…' if modules['therapy_system'] else 'âŒ'}\")\n",
    "\n",
    "# ç»™å‡ºæ€»ä½“è¯„ä¼°\n",
    "all_good = (\n",
    "    status_report['dependencies']['core_installed'] and\n",
    "    modules['theory_modules'] and\n",
    "    modules['model_adapters'] and\n",
    "    modules['therapy_system']\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ¯ ç³»ç»ŸçŠ¶æ€:\")\n",
    "if all_good:\n",
    "    print(\"âœ… ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼å¯ä»¥ç»§ç»­è¿è¡Œå…¶ä»–notebook\")\n",
    "    recommendation = \"å»ºè®®æŒ‰é¡ºåºè¿è¡Œï¼š02_theory_models_demo â†’ 03_model_adapters_test â†’ 04_therapy_session_demo\"\nelse:\n",
    "    print(\"âš ï¸ ç³»ç»Ÿåˆå§‹åŒ–å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯\")\n",
    "    recommendation = \"è¯·å…ˆè§£å†³ä¾èµ–åº“å’Œæ¨¡å—å¯¼å…¥é—®é¢˜ï¼Œç„¶åé‡æ–°è¿è¡Œæ­¤notebook\"\n",
    "\n",
    "print(f\"\\nğŸ’¡ å»ºè®®: {recommendation}\")\n",
    "\n",
    "# ä¿å­˜çŠ¶æ€æŠ¥å‘Š\n",
    "status_file = PROJECT_ROOT / 'outputs' / 'initialization_status.json'\n",
    "with open(status_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(status_report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nğŸ“„ çŠ¶æ€æŠ¥å‘Šå·²ä¿å­˜: {status_file}\")\nprint(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ åˆå§‹åŒ–å®Œæˆï¼\n",
    "\n",
    "å¦‚æœä¸Šè¿°æ‰€æœ‰æ£€æŸ¥éƒ½é€šè¿‡ï¼Œæ‚¨ç°åœ¨å¯ä»¥ï¼š\n",
    "\n",
    "1. **ç»§ç»­è¿è¡Œç†è®ºæ¼”ç¤º**: `02_theory_models_demo.ipynb`\n",
    "2. **æµ‹è¯•æ¨¡å‹é€‚é…å™¨**: `03_model_adapters_test.ipynb`  \n",
    "3. **ä½“éªŒå®Œæ•´ç–—æ„ˆæµç¨‹**: `04_therapy_session_demo.ipynb`\n",
    "\n",
    "### ğŸ“ é‡è¦æç¤º\n",
    "- æ‰€æœ‰è¾“å‡ºæ–‡ä»¶å°†ä¿å­˜åœ¨ `outputs/` ç›®å½•\n",
    "- æ¨¡å‹ç¼“å­˜ä½äº `outputs/cache/` ç›®å½•\n",
    "- å¦‚é‡é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ `10_troubleshooting_guide.ipynb`\n",
    "\n",
    "### ğŸ†˜ è·å–å¸®åŠ©\n",
    "å¦‚æœåˆå§‹åŒ–è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·ï¼š\n",
    "1. æ£€æŸ¥JupyterHubç¯å¢ƒçš„GPUå’Œå†…å­˜èµ„æº\n",
    "2. ç¡®è®¤ç½‘ç»œè¿æ¥æ­£å¸¸ (æ¨¡å‹ä¸‹è½½éœ€è¦)\n",
    "3. æŸ¥çœ‹é”™è¯¯æ—¥å¿—å®šä½é—®é¢˜\n",
    "4. å‚è€ƒæ•…éšœæ’é™¤æŒ‡å—"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}