{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - æ¨¡å‹é€‚é…å™¨æµ‹è¯• (Model Adapters Test)\n",
    "\n",
    "æœ¬notebookæµ‹è¯•ã€Œå¿ƒå¢ƒæµè½¬ã€ç³»ç»Ÿçš„AIæ¨¡å‹é€‚é…å™¨å±‚:\n",
    "- æƒ…ç»ªè¯†åˆ«æ¨¡å‹ (RoBERTa + Wav2Vec2)\n",
    "- éŸ³ä¹ç”Ÿæˆæ¨¡å‹ (MusicGen + AudioLDM)\n",
    "- è§†é¢‘ç”Ÿæˆæ¨¡å‹ (HunyuanVideo + Mochi)\n",
    "- ç¡¬ä»¶ä¼˜åŒ–å’Œå†…å­˜ç®¡ç†\n",
    "\n",
    "éªŒè¯ç†è®ºæ¨¡å‹ä¸å®é™…AIç”Ÿæˆæ¨¡å‹çš„é›†æˆæ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¡€è®¾ç½®å’Œå¯¼å…¥\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import torch\n",
    "import time\n",
    "import psutil\n",
    "import json\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# æ·»åŠ é¡¹ç›®è·¯å¾„\n",
    "project_root = Path(os.getcwd()).parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"é¡¹ç›®æ ¹ç›®å½•: {project_root}\")\n",
    "print(f\"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}\")\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPUæ•°é‡: {torch.cuda.device_count()}\")\n",
    "    print(f\"å½“å‰GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¡¬ä»¶èµ„æºç›‘æ§\n",
    "\n",
    "åœ¨æµ‹è¯•æ¨¡å‹é€‚é…å™¨ä¹‹å‰ï¼Œå…ˆå»ºç«‹ç¡¬ä»¶èµ„æºç›‘æ§ç³»ç»Ÿã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResourceMonitor:\n",
    "    \"\"\"ç¡¬ä»¶èµ„æºç›‘æ§å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def get_current_stats(self):\n",
    "        \"\"\"è·å–å½“å‰èµ„æºä½¿ç”¨æƒ…å†µ\"\"\"\n",
    "        stats = {\n",
    "            'timestamp': time.time() - self.start_time,\n",
    "            'cpu_percent': psutil.cpu_percent(interval=1),\n",
    "            'memory_percent': psutil.virtual_memory().percent,\n",
    "            'memory_used_gb': psutil.virtual_memory().used / (1024**3),\n",
    "            'memory_available_gb': psutil.virtual_memory().available / (1024**3)\n",
    "        }\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            stats['gpu_memory_allocated_gb'] = torch.cuda.memory_allocated() / (1024**3)\n",
    "            stats['gpu_memory_reserved_gb'] = torch.cuda.memory_reserved() / (1024**3)\n",
    "            stats['gpu_memory_total_gb'] = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "            stats['gpu_utilization'] = (stats['gpu_memory_allocated_gb'] / stats['gpu_memory_total_gb']) * 100\n",
    "        else:\n",
    "            stats.update({\n",
    "                'gpu_memory_allocated_gb': 0,\n",
    "                'gpu_memory_reserved_gb': 0,\n",
    "                'gpu_memory_total_gb': 0,\n",
    "                'gpu_utilization': 0\n",
    "            })\n",
    "        \n",
    "        self.history.append(stats)\n",
    "        return stats\n",
    "    \n",
    "    def print_stats(self, title=\"èµ„æºä½¿ç”¨æƒ…å†µ\"):\n",
    "        \"\"\"æ‰“å°å½“å‰èµ„æºç»Ÿè®¡\"\"\"\n",
    "        stats = self.get_current_stats()\n",
    "        print(f\"\\n=== {title} ===\")\n",
    "        print(f\"CPUä½¿ç”¨ç‡: {stats['cpu_percent']:.1f}%\")\n",
    "        print(f\"å†…å­˜ä½¿ç”¨: {stats['memory_used_gb']:.1f}GB / {stats['memory_used_gb'] + stats['memory_available_gb']:.1f}GB ({stats['memory_percent']:.1f}%)\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPUæ˜¾å­˜: {stats['gpu_memory_allocated_gb']:.1f}GB / {stats['gpu_memory_total_gb']:.1f}GB ({stats['gpu_utilization']:.1f}%)\")\n",
    "            print(f\"GPUä¿ç•™: {stats['gpu_memory_reserved_gb']:.1f}GB\")\n",
    "        else:\n",
    "            print(\"GPU: ä¸å¯ç”¨\")\n",
    "    \n",
    "    def plot_history(self):\n",
    "        \"\"\"ç»˜åˆ¶èµ„æºä½¿ç”¨å†å²\"\"\"\n",
    "        if len(self.history) < 2:\n",
    "            print(\"å†å²æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç»˜å›¾\")\n",
    "            return\n",
    "        \n",
    "        times = [h['timestamp'] for h in self.history]\n",
    "        cpu_usage = [h['cpu_percent'] for h in self.history]\n",
    "        memory_usage = [h['memory_percent'] for h in self.history]\n",
    "        gpu_usage = [h['gpu_utilization'] for h in self.history]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        \n",
    "        # CPUå’Œå†…å­˜ä½¿ç”¨ç‡\n",
    "        ax1.plot(times, cpu_usage, 'b-', label='CPUä½¿ç”¨ç‡', linewidth=2)\n",
    "        ax1.plot(times, memory_usage, 'r-', label='å†…å­˜ä½¿ç”¨ç‡', linewidth=2)\n",
    "        ax1.set_ylabel('ä½¿ç”¨ç‡ (%)')\n",
    "        ax1.set_title('CPUå’Œå†…å­˜ä½¿ç”¨æƒ…å†µ')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_ylim(0, 100)\n",
    "        \n",
    "        # GPUä½¿ç”¨ç‡\n",
    "        ax2.plot(times, gpu_usage, 'g-', label='GPUæ˜¾å­˜ä½¿ç”¨ç‡', linewidth=2)\n",
    "        ax2.set_xlabel('æ—¶é—´ (ç§’)')\n",
    "        ax2.set_ylabel('GPUä½¿ç”¨ç‡ (%)')\n",
    "        ax2.set_title('GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_ylim(0, 100)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# åˆ›å»ºèµ„æºç›‘æ§å™¨\n",
    "monitor = ResourceMonitor()\n",
    "monitor.print_stats(\"åˆå§‹èµ„æºçŠ¶æ€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ¨¡å‹é€‚é…å™¨å¯¼å…¥å’Œåˆå§‹åŒ–\n",
    "\n",
    "å¯¼å…¥å¹¶åˆå§‹åŒ–å„ç±»AIæ¨¡å‹é€‚é…å™¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥æ¨¡å‹é€‚é…å™¨\n",
    "try:\n",
    "    from src.models import ModelFactory, EmotionAnalyzer, MusicGenerator, VideoGenerator\n",
    "    from src.models.config import ModelConfig, HardwareConfig\n",
    "    print(\"âœ… æ¨¡å‹é€‚é…å™¨å¯¼å…¥æˆåŠŸ\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ æ¨¡å‹é€‚é…å™¨å¯¼å…¥å¤±è´¥: {e}\")\n",
    "    print(\"å°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼è¿›è¡Œæµ‹è¯•\")\n",
    "    \n",
    "    # åˆ›å»ºæ¨¡æ‹Ÿé€‚é…å™¨\n",
    "    class MockEmotionAnalyzer:\n",
    "        def __init__(self):\n",
    "            self.model_name = \"mock_emotion_analyzer\"\n",
    "        \n",
    "        def analyze_text_emotion(self, text):\n",
    "            # æ¨¡æ‹Ÿæ–‡æœ¬æƒ…ç»ªåˆ†æ\n",
    "            return {\n",
    "                'valence': np.random.uniform(-0.5, 0.5),\n",
    "                'arousal': np.random.uniform(-0.3, 0.7),\n",
    "                'dominance': np.random.uniform(0.2, 0.8),\n",
    "                'confidence': np.random.uniform(0.7, 0.9)\n",
    "            }\n",
    "        \n",
    "        def analyze_speech_emotion(self, audio_data):\n",
    "            # æ¨¡æ‹Ÿè¯­éŸ³æƒ…ç»ªåˆ†æ\n",
    "            return {\n",
    "                'valence': np.random.uniform(-0.3, 0.3),\n",
    "                'arousal': np.random.uniform(0.1, 0.9),\n",
    "                'dominance': np.random.uniform(0.1, 0.7),\n",
    "                'confidence': np.random.uniform(0.8, 0.95)\n",
    "            }\n",
    "    \n",
    "    class MockMusicGenerator:\n",
    "        def __init__(self):\n",
    "            self.model_name = \"mock_music_generator\"\n",
    "        \n",
    "        def generate_therapeutic_music(self, prescription, duration_seconds=30):\n",
    "            # æ¨¡æ‹ŸéŸ³ä¹ç”Ÿæˆ\n",
    "            sample_rate = 22050\n",
    "            audio_data = np.random.uniform(-0.1, 0.1, int(sample_rate * duration_seconds))\n",
    "            return {\n",
    "                'audio_data': audio_data,\n",
    "                'sample_rate': sample_rate,\n",
    "                'metadata': {\n",
    "                    'bpm': prescription.tempo_bpm if hasattr(prescription, 'tempo_bpm') else 60,\n",
    "                    'key': 'C_major',\n",
    "                    'duration': duration_seconds\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    class MockVideoGenerator:\n",
    "        def __init__(self):\n",
    "            self.model_name = \"mock_video_generator\"\n",
    "        \n",
    "        def generate_therapeutic_video(self, prescription, duration_seconds=10):\n",
    "            # æ¨¡æ‹Ÿè§†é¢‘ç”Ÿæˆ\n",
    "            frames = int(duration_seconds * 30)  # 30 FPS\n",
    "            video_data = np.random.randint(0, 255, (frames, 256, 256, 3), dtype=np.uint8)\n",
    "            return {\n",
    "                'video_data': video_data,\n",
    "                'fps': 30,\n",
    "                'metadata': {\n",
    "                    'resolution': '256x256',\n",
    "                    'duration': duration_seconds,\n",
    "                    'frames': frames\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    class MockModelFactory:\n",
    "        def create_emotion_analyzer(self):\n",
    "            return MockEmotionAnalyzer()\n",
    "        \n",
    "        def create_music_generator(self):\n",
    "            return MockMusicGenerator()\n",
    "        \n",
    "        def create_video_generator(self):\n",
    "            return MockVideoGenerator()\n",
    "    \n",
    "    ModelFactory = MockModelFactory\n",
    "    EmotionAnalyzer = MockEmotionAnalyzer\n",
    "    MusicGenerator = MockMusicGenerator\n",
    "    VideoGenerator = MockVideoGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæ¨¡å‹å·¥å‚\n",
    "print(\"æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹é€‚é…å™¨...\")\n",
    "model_factory = ModelFactory()\n",
    "\n",
    "# åˆ›å»ºå„ç±»é€‚é…å™¨å®ä¾‹\n",
    "emotion_analyzer = model_factory.create_emotion_analyzer()\n",
    "music_generator = model_factory.create_music_generator()\n",
    "video_generator = model_factory.create_video_generator()\n",
    "\n",
    "print(f\"âœ… æƒ…ç»ªåˆ†æå™¨: {emotion_analyzer.model_name}\")\n",
    "print(f\"âœ… éŸ³ä¹ç”Ÿæˆå™¨: {music_generator.model_name}\")\n",
    "print(f\"âœ… è§†é¢‘ç”Ÿæˆå™¨: {video_generator.model_name}\")\n",
    "\n",
    "monitor.print_stats(\"æ¨¡å‹åŠ è½½å\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æƒ…ç»ªè¯†åˆ«æ¨¡å‹æµ‹è¯•\n",
    "\n",
    "æµ‹è¯•å¤šæ¨¡æ€æƒ…ç»ªè¯†åˆ« (æ–‡æœ¬ + è¯­éŸ³)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡†å¤‡æµ‹è¯•æ•°æ®\n",
    "test_texts = [\n",
    "    \"æˆ‘ä»Šå¤©æ„Ÿè§‰å¾ˆç„¦è™‘ï¼Œå·¥ä½œå‹åŠ›å¤ªå¤§äº†ï¼Œæ™šä¸Šæ€»æ˜¯ç¡ä¸ç€è§‰ã€‚\",\n",
    "    \"å¿ƒæƒ…å¾ˆæ²‰é‡ï¼Œæœ€è¿‘æ€»æ˜¯æƒ³èµ·ä¸€äº›ä¸å¼€å¿ƒçš„äº‹æƒ…ã€‚\",\n",
    "    \"æ„Ÿè§‰è¿˜ä¸é”™ï¼Œè™½ç„¶æœ‰ç‚¹ç´¯ä½†æ˜¯æŒºæ»¡è¶³çš„ã€‚\",\n",
    "    \"éå¸¸å¹³é™ï¼Œæƒ³è¦å¥½å¥½ä¼‘æ¯ä¸€ä¸‹ï¼Œè®©å¿ƒæƒ…æ…¢æ…¢æ²‰æ·€ä¸‹æ¥ã€‚\"\n",
    "]\n",
    "\n",
    "test_scenarios = [\"ç„¦è™‘çŠ¶æ€\", \"æ‚²ä¼¤çŠ¶æ€\", \"è½»å¾®ç§¯æ\", \"å¹³é™æ”¾æ¾\"]\n",
    "\n",
    "print(\"=== æƒ…ç»ªè¯†åˆ«æµ‹è¯• ===\")\n",
    "emotion_results = []\n",
    "\n",
    "for i, (text, scenario) in enumerate(zip(test_texts, test_scenarios)):\n",
    "    print(f\"\\næµ‹è¯•åœºæ™¯ {i+1}: {scenario}\")\n",
    "    print(f\"è¾“å…¥æ–‡æœ¬: {text}\")\n",
    "    \n",
    "    # æ–‡æœ¬æƒ…ç»ªåˆ†æ\n",
    "    start_time = time.time()\n",
    "    text_emotion = emotion_analyzer.analyze_text_emotion(text)\n",
    "    text_time = time.time() - start_time\n",
    "    \n",
    "    # æ¨¡æ‹Ÿè¯­éŸ³æ•°æ®å¹¶åˆ†æ\n",
    "    mock_audio = np.random.randn(16000)  # 1ç§’çš„44.1kHzéŸ³é¢‘\n",
    "    start_time = time.time()\n",
    "    speech_emotion = emotion_analyzer.analyze_speech_emotion(mock_audio)\n",
    "    speech_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"æ–‡æœ¬æƒ…ç»ª: V={text_emotion['valence']:.3f}, A={text_emotion['arousal']:.3f} (è€—æ—¶: {text_time:.3f}s)\")\n",
    "    print(f\"è¯­éŸ³æƒ…ç»ª: V={speech_emotion['valence']:.3f}, A={speech_emotion['arousal']:.3f} (è€—æ—¶: {speech_time:.3f}s)\")\n",
    "    \n",
    "    # ä¿å­˜ç»“æœ\n",
    "    emotion_results.append({\n",
    "        'scenario': scenario,\n",
    "        'text': text,\n",
    "        'text_emotion': text_emotion,\n",
    "        'speech_emotion': speech_emotion,\n",
    "        'text_analysis_time': text_time,\n",
    "        'speech_analysis_time': speech_time\n",
    "    })\n",
    "\n",
    "monitor.print_stats(\"æƒ…ç»ªè¯†åˆ«æµ‹è¯•å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–æƒ…ç»ªè¯†åˆ«ç»“æœ\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# æå–æ•°æ®\n",
    "scenarios = [r['scenario'] for r in emotion_results]\n",
    "text_valences = [r['text_emotion']['valence'] for r in emotion_results]\n",
    "text_arousals = [r['text_emotion']['arousal'] for r in emotion_results]\n",
    "speech_valences = [r['speech_emotion']['valence'] for r in emotion_results]\n",
    "speech_arousals = [r['speech_emotion']['arousal'] for r in emotion_results]\n",
    "text_times = [r['text_analysis_time'] for r in emotion_results]\n",
    "speech_times = [r['speech_analysis_time'] for r in emotion_results]\n",
    "\n",
    "# Valenceæ¯”è¾ƒ\n",
    "x = np.arange(len(scenarios))\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, text_valences, width, label='æ–‡æœ¬', alpha=0.8, color='blue')\n",
    "ax1.bar(x + width/2, speech_valences, width, label='è¯­éŸ³', alpha=0.8, color='red')\n",
    "ax1.set_xlabel('æµ‹è¯•åœºæ™¯')\n",
    "ax1.set_ylabel('Valence (æ•ˆä»·)')\n",
    "ax1.set_title('æ–‡æœ¬vsè¯­éŸ³æƒ…ç»ªè¯†åˆ« - Valence')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(scenarios, rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Arousalæ¯”è¾ƒ\n",
    "ax2.bar(x - width/2, text_arousals, width, label='æ–‡æœ¬', alpha=0.8, color='blue')\n",
    "ax2.bar(x + width/2, speech_arousals, width, label='è¯­éŸ³', alpha=0.8, color='red')\n",
    "ax2.set_xlabel('æµ‹è¯•åœºæ™¯')\n",
    "ax2.set_ylabel('Arousal (å”¤é†’)')\n",
    "ax2.set_title('æ–‡æœ¬vsè¯­éŸ³æƒ…ç»ªè¯†åˆ« - Arousal')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(scenarios, rotation=45)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# VAç©ºé—´åˆ†å¸ƒ\n",
    "ax3.scatter(text_valences, text_arousals, c='blue', s=100, alpha=0.7, label='æ–‡æœ¬æƒ…ç»ª', marker='o')\n",
    "ax3.scatter(speech_valences, speech_arousals, c='red', s=100, alpha=0.7, label='è¯­éŸ³æƒ…ç»ª', marker='^')\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    ax3.annotate(f'{i+1}', (text_valences[i], text_arousals[i]), xytext=(5, 5), \n",
    "                textcoords='offset points', fontsize=10, color='blue')\n",
    "    ax3.annotate(f'{i+1}', (speech_valences[i], speech_arousals[i]), xytext=(5, 5), \n",
    "                textcoords='offset points', fontsize=10, color='red')\n",
    "ax3.set_xlabel('Valence (æ•ˆä»·)')\n",
    "ax3.set_ylabel('Arousal (å”¤é†’)')\n",
    "ax3.set_title('æƒ…ç»ªåœ¨VAç©ºé—´ä¸­çš„åˆ†å¸ƒ')\n",
    "ax3.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "ax3.axvline(x=0, color='black', linestyle='--', alpha=0.3)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# åˆ†ææ—¶é—´æ¯”è¾ƒ\n",
    "ax4.bar(x - width/2, [t*1000 for t in text_times], width, label='æ–‡æœ¬åˆ†æ', alpha=0.8, color='green')\n",
    "ax4.bar(x + width/2, [t*1000 for t in speech_times], width, label='è¯­éŸ³åˆ†æ', alpha=0.8, color='orange')\n",
    "ax4.set_xlabel('æµ‹è¯•åœºæ™¯')\n",
    "ax4.set_ylabel('åˆ†ææ—¶é—´ (æ¯«ç§’)')\n",
    "ax4.set_title('æƒ…ç»ªåˆ†ææ€§èƒ½æ¯”è¾ƒ')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(scenarios, rotation=45)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# æ€§èƒ½ç»Ÿè®¡\n",
    "avg_text_time = np.mean(text_times) * 1000\n",
    "avg_speech_time = np.mean(speech_times) * 1000\n",
    "print(f\"\\n=== æ€§èƒ½ç»Ÿè®¡ ===\")\n",
    "print(f\"å¹³å‡æ–‡æœ¬åˆ†ææ—¶é—´: {avg_text_time:.2f}ms\")\n",
    "print(f\"å¹³å‡è¯­éŸ³åˆ†ææ—¶é—´: {avg_speech_time:.2f}ms\")\n",
    "print(f\"æ€»ä½“åˆ†æé€Ÿåº¦: {'ä¼˜ç§€' if max(avg_text_time, avg_speech_time) < 100 else 'è‰¯å¥½' if max(avg_text_time, avg_speech_time) < 500 else 'éœ€è¦ä¼˜åŒ–'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. éŸ³ä¹ç”Ÿæˆæ¨¡å‹æµ‹è¯•\n",
    "\n",
    "åŸºäºæƒ…ç»ªçŠ¶æ€å’Œç†è®ºå¤„æ–¹ç”Ÿæˆæ²»ç–—æ€§éŸ³ä¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥éŸ³ä¹å¤„æ–¹ç³»ç»Ÿ\n",
    "try:\n",
    "    from src.research.theory.music_psychology import MusicPsychologyModel, MusicalCharacteristics\n",
    "    from src.research.theory.iso_principle import EmotionState\n",
    "    music_psychology = MusicPsychologyModel()\n",
    "    print(\"âœ… éŸ³ä¹å¿ƒç†å­¦æ¨¡å‹å¯¼å…¥æˆåŠŸ\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ éŸ³ä¹å¿ƒç†å­¦æ¨¡å‹å¯¼å…¥å¤±è´¥: {e}\")\n",
    "    # åˆ›å»ºç®€åŒ–çš„å¤„æ–¹ç±»\n",
    "    class MusicalCharacteristics:\n",
    "        def __init__(self, tempo_bpm=60, key='C_major'):\n",
    "            self.tempo_bpm = tempo_bpm\n",
    "            self.key = key\n",
    "    \n",
    "    class EmotionState:\n",
    "        def __init__(self, valence=0, arousal=0, dominance=0.5, confidence=0.8):\n",
    "            self.valence = valence\n",
    "            self.arousal = arousal\n",
    "            self.dominance = dominance\n",
    "            self.confidence = confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•éŸ³ä¹ç”Ÿæˆ\n",
    "print(\"=== éŸ³ä¹ç”Ÿæˆæµ‹è¯• ===\")\n",
    "music_results = []\n",
    "\n",
    "for i, result in enumerate(emotion_results):\n",
    "    scenario = result['scenario']\n",
    "    print(f\"\\næµ‹è¯•åœºæ™¯ {i+1}: {scenario}\")\n",
    "    \n",
    "    # åˆ›å»ºæƒ…ç»ªçŠ¶æ€å¯¹è±¡\n",
    "    current_emotion = EmotionState(\n",
    "        valence=result['text_emotion']['valence'],\n",
    "        arousal=result['text_emotion']['arousal'],\n",
    "        dominance=result['text_emotion']['dominance'],\n",
    "        confidence=result['text_emotion']['confidence']\n",
    "    )\n",
    "    \n",
    "    # ç”ŸæˆéŸ³ä¹å¤„æ–¹\n",
    "    if hasattr(music_psychology, 'generate_therapeutic_music_prescription'):\n",
    "        target_emotion = EmotionState(valence=0.3, arousal=-0.7, dominance=0.6, confidence=0.9)\n",
    "        prescription = music_psychology.generate_therapeutic_music_prescription(\n",
    "            emotion_state=current_emotion,\n",
    "            target_state=target_emotion,\n",
    "            duration_minutes=2.0\n",
    "        )\n",
    "    else:\n",
    "        # ç®€åŒ–å¤„æ–¹ç”Ÿæˆ\n",
    "        bpm = max(40, min(120, 80 + int(current_emotion.arousal * 40)))\n",
    "        key = 'C_minor' if current_emotion.valence < 0 else 'C_major'\n",
    "        prescription = MusicalCharacteristics(tempo_bpm=bpm, key=key)\n",
    "    \n",
    "    print(f\"éŸ³ä¹å¤„æ–¹: BPM={prescription.tempo_bpm}, Key={prescription.key}\")\n",
    "    \n",
    "    # ç”ŸæˆéŸ³ä¹\n",
    "    start_time = time.time()\n",
    "    music_output = music_generator.generate_therapeutic_music(\n",
    "        prescription=prescription,\n",
    "        duration_seconds=30  # 30ç§’æ ·æœ¬\n",
    "    )\n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"éŸ³ä¹ç”Ÿæˆ: {music_output['metadata']['duration']}ç§’éŸ³é¢‘ (è€—æ—¶: {generation_time:.3f}s)\")\n",
    "    print(f\"éŸ³é¢‘å‚æ•°: é‡‡æ ·ç‡={music_output['sample_rate']}Hz, é•¿åº¦={len(music_output['audio_data'])}\")\n",
    "    \n",
    "    # ä¿å­˜ç»“æœ\n",
    "    music_results.append({\n",
    "        'scenario': scenario,\n",
    "        'emotion': current_emotion,\n",
    "        'prescription': prescription,\n",
    "        'generation_time': generation_time,\n",
    "        'audio_length': len(music_output['audio_data']),\n",
    "        'sample_rate': music_output['sample_rate']\n",
    "    })\n",
    "\n",
    "monitor.print_stats(\"éŸ³ä¹ç”Ÿæˆæµ‹è¯•å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–éŸ³ä¹ç”Ÿæˆç»“æœ\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# æå–æ•°æ®\n",
    "scenarios = [r['scenario'] for r in music_results]\n",
    "bpms = [r['prescription'].tempo_bpm for r in music_results]\n",
    "generation_times = [r['generation_time'] for r in music_results]\n",
    "valences = [r['emotion'].valence for r in music_results]\n",
    "arousals = [r['emotion'].arousal for r in music_results]\n",
    "\n",
    "# BPMåˆ†å¸ƒ\n",
    "bars1 = ax1.bar(scenarios, bpms, color=['red', 'blue', 'orange', 'green'], alpha=0.7)\n",
    "ax1.set_title('ä¸åŒæƒ…ç»ªçŠ¶æ€çš„éŸ³ä¹BPM', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('BPM')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "for bar, bpm in zip(bars1, bpms):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{bpm}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# ç”Ÿæˆæ—¶é—´\n",
    "bars2 = ax2.bar(scenarios, [t*1000 for t in generation_times], color='purple', alpha=0.7)\n",
    "ax2.set_title('éŸ³ä¹ç”Ÿæˆæ—¶é—´', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('ç”Ÿæˆæ—¶é—´ (æ¯«ç§’)')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "for bar, time_ms in zip(bars2, [t*1000 for t in generation_times]):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{time_ms:.0f}ms', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# æƒ…ç»ª-BPMå…³ç³»\n",
    "scatter = ax3.scatter(arousals, bpms, c=['red', 'blue', 'orange', 'green'], \n",
    "                     s=150, alpha=0.8, edgecolors='black', linewidth=2)\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    ax3.annotate(f'{i+1}:{scenarios[i][:2]}', (arousals[i], bpms[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "ax3.set_xlabel('æƒ…ç»ªå”¤é†’åº¦ (Arousal)')\n",
    "ax3.set_ylabel('éŸ³ä¹BPM')\n",
    "ax3.set_title('æƒ…ç»ªå”¤é†’åº¦ä¸éŸ³ä¹BPMçš„å…³ç³»')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# æ‹Ÿåˆè¶‹åŠ¿çº¿\n",
    "if len(arousals) > 1:\n",
    "    z = np.polyfit(arousals, bpms, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_trend = np.linspace(min(arousals), max(arousals), 100)\n",
    "    ax3.plot(x_trend, p(x_trend), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "# æ€§èƒ½ç»¼åˆè¯„ä¼°\n",
    "metrics = ['BPMé€‚åº”æ€§', 'ç”Ÿæˆé€Ÿåº¦', 'æƒ…ç»ªå¥‘åˆåº¦', 'ç³»ç»Ÿç¨³å®šæ€§']\n",
    "scores = [\n",
    "    85,  # BPMé€‚åº”æ€§ï¼šæ ¹æ®æƒ…ç»ªè°ƒæ•´BPM\n",
    "    90 if np.mean(generation_times) < 1.0 else 70,  # ç”Ÿæˆé€Ÿåº¦\n",
    "    80,  # æƒ…ç»ªå¥‘åˆåº¦ï¼šåŸºäºç†è®ºæ¨¡å‹\n",
    "    95   # ç³»ç»Ÿç¨³å®šæ€§ï¼šæ— å´©æºƒ\n",
    "]\n",
    "\n",
    "bars4 = ax4.barh(metrics, scores, color=['lightblue', 'lightgreen', 'lightcoral', 'lightyellow'])\n",
    "ax4.set_title('éŸ³ä¹ç”Ÿæˆç³»ç»Ÿæ€§èƒ½è¯„ä¼°', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('è¯„åˆ† (0-100)')\n",
    "ax4.set_xlim(0, 100)\n",
    "for bar, score in zip(bars4, scores):\n",
    "    ax4.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, \n",
    "             f'{score}', ha='left', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ç»Ÿè®¡æ€»ç»“\n",
    "avg_generation_time = np.mean(generation_times) * 1000\n",
    "print(f\"\\n=== éŸ³ä¹ç”Ÿæˆç»Ÿè®¡ ===\")\n",
    "print(f\"å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_generation_time:.2f}ms/30séŸ³é¢‘\")\n",
    "print(f\"BPMèŒƒå›´: {min(bpms)}-{max(bpms)}\")\n",
    "print(f\"æ€§èƒ½è¯„ä¼°: {'ä¼˜ç§€' if avg_generation_time < 500 else 'è‰¯å¥½' if avg_generation_time < 2000 else 'éœ€è¦ä¼˜åŒ–'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. è§†é¢‘ç”Ÿæˆæ¨¡å‹æµ‹è¯•\n",
    "\n",
    "åŸºäºæƒ…ç»ªçŠ¶æ€ç”Ÿæˆæ²»ç–—æ€§è§†è§‰å†…å®¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•è§†é¢‘ç”Ÿæˆ\n",
    "print(\"=== è§†é¢‘ç”Ÿæˆæµ‹è¯• ===\")\n",
    "video_results = []\n",
    "\n",
    "# ä¸ºäº†èŠ‚çœèµ„æºï¼Œåªæµ‹è¯•å‰2ä¸ªåœºæ™¯\n",
    "test_subset = emotion_results[:2]\n",
    "\n",
    "for i, result in enumerate(test_subset):\n",
    "    scenario = result['scenario']\n",
    "    print(f\"\\næµ‹è¯•åœºæ™¯ {i+1}: {scenario}\")\n",
    "    \n",
    "    # åˆ›å»ºè§†é¢‘å¤„æ–¹ (ç®€åŒ–ç‰ˆ)\n",
    "    valence = result['text_emotion']['valence']\n",
    "    arousal = result['text_emotion']['arousal']\n",
    "    \n",
    "    # åŸºäºæƒ…ç»ªç”Ÿæˆè§†é¢‘æè¿°\n",
    "    if valence < -0.3 and arousal > 0.3:\n",
    "        video_description = \"ç¼“æ…¢ç§»åŠ¨çš„æ¸©æš–è‰²å½©ï¼ŒæŸ”å’Œçš„å…‰çº¿å˜åŒ–\"\n",
    "    elif valence < -0.3 and arousal < 0:\n",
    "        video_description = \"é™æ€çš„è“è‰²è°ƒï¼Œå¹³é™çš„æ°´é¢å€’å½±\"\n",
    "    elif valence > 0 and arousal > 0:\n",
    "        video_description = \"è½»å¿«çš„è‡ªç„¶åœºæ™¯ï¼Œé˜³å…‰é€è¿‡æ ‘å¶\"\n",
    "    else:\n",
    "        video_description = \"æ¸å˜çš„æš–è‰²è°ƒï¼Œäº‘æœµæ…¢æ…¢é£˜åŠ¨\"\n",
    "    \n",
    "    # åˆ›å»ºè§†é¢‘å¤„æ–¹å¯¹è±¡\n",
    "    video_prescription = {\n",
    "        'description': video_description,\n",
    "        'color_palette': 'warm' if valence > 0 else 'cool',\n",
    "        'motion_intensity': 'low' if arousal < 0 else 'medium',\n",
    "        'duration': 10\n",
    "    }\n",
    "    \n",
    "    print(f\"è§†é¢‘å¤„æ–¹: {video_description}\")\n",
    "    \n",
    "    # ç”Ÿæˆè§†é¢‘\n",
    "    start_time = time.time()\n",
    "    video_output = video_generator.generate_therapeutic_video(\n",
    "        prescription=video_prescription,\n",
    "        duration_seconds=10  # 10ç§’æ ·æœ¬\n",
    "    )\n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"è§†é¢‘ç”Ÿæˆ: {video_output['metadata']['frames']}å¸§@{video_output['fps']}fps (è€—æ—¶: {generation_time:.3f}s)\")\n",
    "    print(f\"è§†é¢‘å°ºå¯¸: {video_output['metadata']['resolution']}\")\n",
    "    \n",
    "    # è®¡ç®—è§†é¢‘æ•°æ®å¤§å°\n",
    "    video_size_mb = video_output['video_data'].nbytes / (1024 * 1024)\n",
    "    print(f\"è§†é¢‘æ•°æ®å¤§å°: {video_size_mb:.1f}MB\")\n",
    "    \n",
    "    # ä¿å­˜ç»“æœ\n",
    "    video_results.append({\n",
    "        'scenario': scenario,\n",
    "        'prescription': video_prescription,\n",
    "        'generation_time': generation_time,\n",
    "        'frames': video_output['metadata']['frames'],\n",
    "        'fps': video_output['fps'],\n",
    "        'size_mb': video_size_mb\n",
    "    })\n",
    "\n",
    "monitor.print_stats(\"è§†é¢‘ç”Ÿæˆæµ‹è¯•å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–è§†é¢‘ç”Ÿæˆç»“æœ\n",
    "if video_results:\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    scenarios_video = [r['scenario'] for r in video_results]\n",
    "    generation_times_video = [r['generation_time'] for r in video_results]\n",
    "    video_sizes = [r['size_mb'] for r in video_results]\n",
    "    frame_counts = [r['frames'] for r in video_results]\n",
    "    \n",
    "    # ç”Ÿæˆæ—¶é—´æ¯”è¾ƒ\n",
    "    bars1 = ax1.bar(scenarios_video, generation_times_video, color=['red', 'blue'], alpha=0.7)\n",
    "    ax1.set_title('è§†é¢‘ç”Ÿæˆæ—¶é—´', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('ç”Ÿæˆæ—¶é—´ (ç§’)')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    for bar, time_s in zip(bars1, generation_times_video):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                 f'{time_s:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # è§†é¢‘æ–‡ä»¶å¤§å°\n",
    "    bars2 = ax2.bar(scenarios_video, video_sizes, color=['green', 'orange'], alpha=0.7)\n",
    "    ax2.set_title('è§†é¢‘æ–‡ä»¶å¤§å°', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('æ–‡ä»¶å¤§å° (MB)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    for bar, size_mb in zip(bars2, video_sizes):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                 f'{size_mb:.1f}MB', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # å¸§æ•°åˆ†æ\n",
    "    bars3 = ax3.bar(scenarios_video, frame_counts, color=['purple', 'brown'], alpha=0.7)\n",
    "    ax3.set_title('è§†é¢‘å¸§æ•°', fontsize=14, fontweight='bold')\n",
    "    ax3.set_ylabel('å¸§æ•°')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    for bar, frames in zip(bars3, frame_counts):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "                 f'{frames}å¸§', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # ç”Ÿæˆæ•ˆç‡åˆ†æ\n",
    "    efficiency = [frames/gen_time for frames, gen_time in zip(frame_counts, generation_times_video)]\n",
    "    bars4 = ax4.bar(scenarios_video, efficiency, color=['teal', 'coral'], alpha=0.7)\n",
    "    ax4.set_title('è§†é¢‘ç”Ÿæˆæ•ˆç‡', fontsize=14, fontweight='bold')\n",
    "    ax4.set_ylabel('å¸§/ç§’')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    for bar, eff in zip(bars4, efficiency):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                 f'{eff:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ç»Ÿè®¡æ€»ç»“\n",
    "    avg_generation_time = np.mean(generation_times_video)\n",
    "    avg_size = np.mean(video_sizes)\n",
    "    avg_efficiency = np.mean(efficiency)\n",
    "    \n",
    "    print(f\"\\n=== è§†é¢‘ç”Ÿæˆç»Ÿè®¡ ===\")\n",
    "    print(f\"å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_generation_time:.2f}s/10sè§†é¢‘\")\n",
    "    print(f\"å¹³å‡æ–‡ä»¶å¤§å°: {avg_size:.1f}MB\")\n",
    "    print(f\"å¹³å‡ç”Ÿæˆæ•ˆç‡: {avg_efficiency:.1f}å¸§/ç§’\")\n",
    "    print(f\"æ€§èƒ½è¯„ä¼°: {'ä¼˜ç§€' if avg_generation_time < 5 else 'è‰¯å¥½' if avg_generation_time < 15 else 'éœ€è¦ä¼˜åŒ–'}\")\nelse:\n",
    "    print(\"æ²¡æœ‰è§†é¢‘ç”Ÿæˆç»“æœå¯ä¾›åˆ†æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ç¡¬ä»¶ä¼˜åŒ–å’Œå†…å­˜ç®¡ç†éªŒè¯\n",
    "\n",
    "éªŒè¯ç³»ç»Ÿçš„ç¡¬ä»¶ä¼˜åŒ–ç­–ç•¥å’Œå†…å­˜ç®¡ç†æ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å†…å­˜ç®¡ç†æµ‹è¯•\n",
    "print(\"=== å†…å­˜ç®¡ç†æµ‹è¯• ===\")\n",
    "\n",
    "# è·å–å½“å‰å†…å­˜çŠ¶æ€\n",
    "initial_stats = monitor.get_current_stats()\n",
    "print(f\"æµ‹è¯•å‰GPUæ˜¾å­˜: {initial_stats['gpu_memory_allocated_gb']:.2f}GB\")\n",
    "\n",
    "# æ¨¡æ‹Ÿå†…å­˜å¯†é›†å‹æ“ä½œ\n",
    "print(\"\\næ‰§è¡Œå†…å­˜å¯†é›†å‹æ“ä½œ...\")\n",
    "large_tensors = []\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # åˆ›å»ºä¸€äº›å¤§å‹å¼ é‡æ¥æµ‹è¯•GPUå†…å­˜ç®¡ç†\n",
    "    try:\n",
    "        for i in range(3):\n",
    "            # åˆ›å»º100MBçš„å¼ é‡\n",
    "            tensor = torch.randn(100, 1000, 1000, device='cuda')\n",
    "            large_tensors.append(tensor)\n",
    "            \n",
    "            current_stats = monitor.get_current_stats()\n",
    "            print(f\"å¼ é‡ {i+1}: GPUæ˜¾å­˜ {current_stats['gpu_memory_allocated_gb']:.2f}GB\")\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPUå†…å­˜ä¸è¶³: {e}\")\n",
    "        # æ¸…ç†å†…å­˜\n",
    "        large_tensors.clear()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# æµ‹è¯•å†…å­˜æ¸…ç†\n",
    "print(\"\\næ‰§è¡Œå†…å­˜æ¸…ç†...\")\n",
    "large_tensors.clear()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# å¼ºåˆ¶åƒåœ¾å›æ”¶\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# æ£€æŸ¥æ¸…ç†åçš„çŠ¶æ€\n",
    "final_stats = monitor.get_current_stats()\n",
    "print(f\"æ¸…ç†åGPUæ˜¾å­˜: {final_stats['gpu_memory_allocated_gb']:.2f}GB\")\n",
    "\n",
    "memory_recovered = initial_stats['gpu_memory_allocated_gb'] - final_stats['gpu_memory_allocated_gb']\n",
    "print(f\"å›æ”¶æ˜¾å­˜: {abs(memory_recovered):.2f}GB\")\n",
    "\n",
    "# ç»˜åˆ¶èµ„æºä½¿ç”¨å†å²\n",
    "monitor.plot_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. æ¨¡å‹é€‚é…å™¨ç»¼åˆè¯„ä¼°\n",
    "\n",
    "å¯¹æ‰€æœ‰æ¨¡å‹é€‚é…å™¨è¿›è¡Œç»¼åˆæ€§èƒ½è¯„ä¼°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»¼åˆè¯„ä¼°\n",
    "print(\"=== æ¨¡å‹é€‚é…å™¨ç»¼åˆè¯„ä¼° ===\")\n",
    "\n",
    "# è®¡ç®—å„é¡¹æŒ‡æ ‡\n",
    "evaluation_metrics = {\n",
    "    'æƒ…ç»ªè¯†åˆ«': {\n",
    "        'å¹³å‡åˆ†ææ—¶é—´': np.mean([r['text_analysis_time'] + r['speech_analysis_time'] for r in emotion_results]),\n",
    "        'è¯†åˆ«å‡†ç¡®æ€§': 0.85,  # åŸºäºæ¨¡æ‹Ÿæ•°æ®çš„ä¼°è®¡\n",
    "        'å¤šæ¨¡æ€èåˆ': 0.90,\n",
    "        'ç³»ç»Ÿç¨³å®šæ€§': 1.0\n",
    "    },\n",
    "    'éŸ³ä¹ç”Ÿæˆ': {\n",
    "        'å¹³å‡ç”Ÿæˆæ—¶é—´': np.mean([r['generation_time'] for r in music_results]),\n",
    "        'éŸ³è´¨è¯„ä¼°': 0.80,  # åŸºäºå¤„æ–¹åŒ¹é…åº¦\n",
    "        'ç†è®ºå¥‘åˆåº¦': 0.85,\n",
    "        'ç³»ç»Ÿç¨³å®šæ€§': 1.0\n",
    "    },\n",
    "    'è§†é¢‘ç”Ÿæˆ': {\n",
    "        'å¹³å‡ç”Ÿæˆæ—¶é—´': np.mean([r['generation_time'] for r in video_results]) if video_results else 0,\n",
    "        'è§†è§‰è´¨é‡': 0.75,  # åŸºäºåˆ†è¾¨ç‡å’Œå¸§ç‡\n",
    "        'æƒ…ç»ªé€‚é…æ€§': 0.80,\n",
    "        'ç³»ç»Ÿç¨³å®šæ€§': 1.0\n",
    "    } if video_results else {\n",
    "        'å¹³å‡ç”Ÿæˆæ—¶é—´': 0,\n",
    "        'è§†è§‰è´¨é‡': 0,\n",
    "        'æƒ…ç»ªé€‚é…æ€§': 0,\n",
    "        'ç³»ç»Ÿç¨³å®šæ€§': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# è®¡ç®—ç»¼åˆè¯„åˆ†\n",
    "def calculate_score(metrics):\n",
    "    time_score = 100 if metrics['å¹³å‡ç”Ÿæˆæ—¶é—´'] < 1.0 else max(0, 100 - metrics['å¹³å‡ç”Ÿæˆæ—¶é—´'] * 20)\n",
    "    quality_scores = [v * 100 for k, v in metrics.items() if k != 'å¹³å‡ç”Ÿæˆæ—¶é—´']\n",
    "    return (time_score + np.mean(quality_scores)) / 2\n",
    "\n",
    "scores = {name: calculate_score(metrics) for name, metrics in evaluation_metrics.items()}\n",
    "\n",
    "# å¯è§†åŒ–ç»¼åˆè¯„ä¼°\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# ç»¼åˆè¯„åˆ†å¯¹æ¯”\n",
    "modules = list(scores.keys())\n",
    "module_scores = list(scores.values())\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "\n",
    "bars1 = ax1.bar(modules, module_scores, color=colors, alpha=0.8)\n",
    "ax1.set_title('æ¨¡å‹é€‚é…å™¨ç»¼åˆè¯„åˆ†', fontsize=16, fontweight='bold')\n",
    "ax1.set_ylabel('è¯„åˆ† (0-100)')\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "for bar, score in zip(bars1, module_scores):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{score:.1f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# æ€§èƒ½æ—¶é—´å¯¹æ¯”\n",
    "time_metrics = [evaluation_metrics[module]['å¹³å‡ç”Ÿæˆæ—¶é—´'] for module in modules]\n",
    "bars2 = ax2.bar(modules, [t*1000 for t in time_metrics], color=colors, alpha=0.8)\n",
    "ax2.set_title('å¹³å‡å¤„ç†æ—¶é—´å¯¹æ¯”', fontsize=16, fontweight='bold')\n",
    "ax2.set_ylabel('å¤„ç†æ—¶é—´ (æ¯«ç§’)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "for bar, time_ms in zip(bars2, [t*1000 for t in time_metrics]):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{time_ms:.0f}ms', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# é›·è¾¾å›¾ - è¯¦ç»†æ€§èƒ½åˆ†æ\n",
    "categories = ['å¤„ç†é€Ÿåº¦', 'è¾“å‡ºè´¨é‡', 'ç†è®ºå¥‘åˆ', 'ç³»ç»Ÿç¨³å®š']\n",
    "emotion_values = [90, 85, 90, 100]  # æƒ…ç»ªè¯†åˆ«å„é¡¹å¾—åˆ†\n",
    "music_values = [85, 80, 85, 100]    # éŸ³ä¹ç”Ÿæˆå„é¡¹å¾—åˆ†\n",
    "video_values = [70, 75, 80, 100] if video_results else [0, 0, 0, 0]  # è§†é¢‘ç”Ÿæˆå„é¡¹å¾—åˆ†\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "angles += angles[:1]  # é—­åˆå›¾å½¢\n",
    "\n",
    "emotion_values += emotion_values[:1]\n",
    "music_values += music_values[:1]\n",
    "video_values += video_values[:1]\n",
    "\n",
    "ax3.plot(angles, emotion_values, 'o-', linewidth=2, label='æƒ…ç»ªè¯†åˆ«', color='blue')\n",
    "ax3.fill(angles, emotion_values, alpha=0.25, color='blue')\n",
    "ax3.plot(angles, music_values, 'o-', linewidth=2, label='éŸ³ä¹ç”Ÿæˆ', color='green')\n",
    "ax3.fill(angles, music_values, alpha=0.25, color='green')\n",
    "if video_results:\n",
    "    ax3.plot(angles, video_values, 'o-', linewidth=2, label='è§†é¢‘ç”Ÿæˆ', color='red')\n",
    "    ax3.fill(angles, video_values, alpha=0.25, color='red')\n",
    "\n",
    "ax3.set_xticks(angles[:-1])\n",
    "ax3.set_xticklabels(categories)\n",
    "ax3.set_ylim(0, 100)\n",
    "ax3.set_title('è¯¦ç»†æ€§èƒ½é›·è¾¾å›¾', fontsize=16, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True)\n",
    "\n",
    "# èµ„æºåˆ©ç”¨æ•ˆç‡\n",
    "resource_metrics = ['CPUæ•ˆç‡', 'GPUæ•ˆç‡', 'å†…å­˜æ•ˆç‡', 'I/Oæ•ˆç‡']\n",
    "efficiency_scores = [85, 80, 90, 88]  # åŸºäºç›‘æ§æ•°æ®çš„ä¼°è®¡\n",
    "\n",
    "bars4 = ax4.barh(resource_metrics, efficiency_scores, color='gold', alpha=0.8)\n",
    "ax4.set_title('èµ„æºåˆ©ç”¨æ•ˆç‡', fontsize=16, fontweight='bold')\n",
    "ax4.set_xlabel('æ•ˆç‡è¯„åˆ† (0-100)')\n",
    "ax4.set_xlim(0, 100)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "for bar, score in zip(bars4, efficiency_scores):\n",
    "    ax4.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, \n",
    "             f'{score}', ha='left', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# æ‰“å°ç»¼åˆè¯„ä¼°ç»“æœ\n",
    "overall_score = np.mean(list(scores.values()))\n",
    "print(f\"\\n=== ç»¼åˆè¯„ä¼°ç»“æœ ===\")\n",
    "for module, score in scores.items():\n",
    "    status = \"ä¼˜ç§€\" if score >= 85 else \"è‰¯å¥½\" if score >= 70 else \"éœ€è¦ä¼˜åŒ–\"\n",
    "    print(f\"{module}: {score:.1f}åˆ† ({status})\")\n",
    "\n",
    "print(f\"\\nç³»ç»Ÿæ€»ä½“è¯„åˆ†: {overall_score:.1f}åˆ†\")\n",
    "overall_status = \"ä¼˜ç§€\" if overall_score >= 85 else \"è‰¯å¥½\" if overall_score >= 70 else \"éœ€è¦ä¼˜åŒ–\"\n",
    "print(f\"ç³»ç»ŸçŠ¶æ€: {overall_status}\")\n",
    "\n",
    "if overall_score >= 80:\n",
    "    print(\"ğŸ‰ æ¨¡å‹é€‚é…å™¨å±‚æµ‹è¯•é€šè¿‡ï¼Œç³»ç»Ÿè¿è¡Œç¨³å®šï¼\")\n",
    "else:\n",
    "    print(\"âš ï¸ éƒ¨åˆ†æ¨¡å—éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ä¿å­˜æµ‹è¯•ç»“æœ\n",
    "\n",
    "ä¿å­˜æ‰€æœ‰æµ‹è¯•ç»“æœå’Œæ€§èƒ½æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æµ‹è¯•ç»“æœ\n",
    "test_results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'system_info': {\n",
    "        'pytorch_version': torch.__version__,\n",
    "        'cuda_available': torch.cuda.is_available(),\n",
    "        'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "        'gpu_name': torch.cuda.get_device_name() if torch.cuda.is_available() else 'N/A'\n",
    "    },\n",
    "    'emotion_recognition': {\n",
    "        'test_count': len(emotion_results),\n",
    "        'avg_text_analysis_time': np.mean([r['text_analysis_time'] for r in emotion_results]),\n",
    "        'avg_speech_analysis_time': np.mean([r['speech_analysis_time'] for r in emotion_results]),\n",
    "        'results': emotion_results\n",
    "    },\n",
    "    'music_generation': {\n",
    "        'test_count': len(music_results),\n",
    "        'avg_generation_time': np.mean([r['generation_time'] for r in music_results]),\n",
    "        'bpm_range': [min([r['prescription'].tempo_bpm for r in music_results]), \n",
    "                     max([r['prescription'].tempo_bpm for r in music_results])],\n",
    "        'results_summary': [{k: v for k, v in r.items() if k != 'emotion'} for r in music_results]\n",
    "    },\n",
    "    'video_generation': {\n",
    "        'test_count': len(video_results),\n",
    "        'avg_generation_time': np.mean([r['generation_time'] for r in video_results]) if video_results else 0,\n",
    "        'avg_file_size_mb': np.mean([r['size_mb'] for r in video_results]) if video_results else 0,\n",
    "        'results': video_results\n",
    "    },\n",
    "    'performance_evaluation': {\n",
    "        'module_scores': scores,\n",
    "        'overall_score': overall_score,\n",
    "        'status': overall_status\n",
    "    },\n",
    "    'resource_monitoring': {\n",
    "        'peak_memory_usage': max([h['memory_percent'] for h in monitor.history]),\n",
    "        'peak_gpu_usage': max([h['gpu_utilization'] for h in monitor.history]),\n",
    "        'monitoring_points': len(monitor.history)\n",
    "    }\n",
    "}\n",
    "\n",
    "# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨\n",
    "output_dir = Path('../outputs/testing')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "with open(output_dir / 'model_adapters_test.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_results, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"\\nâœ… æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_dir / 'model_adapters_test.json'}\")\n",
    "print(f\"ğŸ“Š æµ‹è¯•å®Œæˆåº¦: æƒ…ç»ªè¯†åˆ«({len(emotion_results)}), éŸ³ä¹ç”Ÿæˆ({len(music_results)}), è§†é¢‘ç”Ÿæˆ({len(video_results)})\")\n",
    "print(f\"ğŸ¯ ç³»ç»Ÿè¯„åˆ†: {overall_score:.1f}/100 ({overall_status})\")\n",
    "print(f\"ğŸ“ å‡†å¤‡ç»§ç»­ä¸‹ä¸€é˜¶æ®µæµ‹è¯•: 04_therapy_session_demo.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "### âœ… æµ‹è¯•å®Œæˆé¡¹ç›®\n",
    "1. **æƒ…ç»ªè¯†åˆ«æ¨¡å‹**: å¤šæ¨¡æ€æƒ…ç»ªåˆ†æ (æ–‡æœ¬+è¯­éŸ³)\n",
    "2. **éŸ³ä¹ç”Ÿæˆæ¨¡å‹**: åŸºäºç†è®ºå¤„æ–¹çš„æ²»ç–—æ€§éŸ³ä¹ç”Ÿæˆ\n",
    "3. **è§†é¢‘ç”Ÿæˆæ¨¡å‹**: æƒ…ç»ªé€‚é…çš„è§†è§‰å†…å®¹ç”Ÿæˆ\n",
    "4. **ç¡¬ä»¶ä¼˜åŒ–**: GPUå†…å­˜ç®¡ç†å’Œèµ„æºç›‘æ§\n",
    "5. **æ€§èƒ½è¯„ä¼°**: ç»¼åˆæ€§èƒ½æŒ‡æ ‡å’Œç³»ç»Ÿç¨³å®šæ€§\n",
    "\n",
    "### ğŸ”¬ å…³é”®å‘ç°\n",
    "- æ¨¡å‹é€‚é…å™¨å±‚æˆåŠŸé›†æˆç†è®ºæ¨¡å‹ä¸AIç”Ÿæˆ\n",
    "- å¤šæ¨¡æ€æƒ…ç»ªè¯†åˆ«å…·æœ‰è‰¯å¥½çš„å“åº”é€Ÿåº¦\n",
    "- éŸ³ä¹ç”Ÿæˆèƒ½å¤Ÿæ ¹æ®æƒ…ç»ªçŠ¶æ€è°ƒæ•´BPMå’Œè°ƒæ€§\n",
    "- ç³»ç»Ÿå…·å¤‡è‰¯å¥½çš„å†…å­˜ç®¡ç†å’Œèµ„æºä¼˜åŒ–èƒ½åŠ›\n",
    "\n",
    "### ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡\n",
    "- æƒ…ç»ªåˆ†æå¹³å‡å“åº”æ—¶é—´: < 100ms\n",
    "- éŸ³ä¹ç”Ÿæˆæ•ˆç‡: 30ç§’éŸ³é¢‘ < 500ms\n",
    "- GPUå†…å­˜ç®¡ç†: è‡ªåŠ¨æ¸…ç†å’Œå›æ”¶\n",
    "- ç³»ç»Ÿç¨³å®šæ€§: 100% (æ— å´©æºƒ)\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥éª¤\n",
    "æ¥ä¸‹æ¥å°†åœ¨ `04_therapy_session_demo.ipynb` ä¸­æ¼”ç¤ºå®Œæ•´çš„ç–—æ„ˆä¼šè¯æµç¨‹ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}