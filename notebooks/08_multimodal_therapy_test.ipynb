{
 "cells": [
  {
   "cell_type": "code",
   "source": "# === 3. 多模态治疗场景测试 ===\n\n# 定义测试场景\nmultimodal_scenarios = [\n    {\n        'name': '深度睡眠诱导',\n        'description': '重度失眠患者的深度睡眠诱导',\n        'request': MultimodalTherapyRequest(\n            user_id='user_insomnia_001',\n            current_emotion={'valence': -0.4, 'arousal': 0.6},\n            target_emotion={'valence': 0.2, 'arousal': -0.7},\n            therapy_focus='sleep',\n            session_duration=900,\n            sensitivity_profile={'visual_sensitivity': 0.3, 'audio_sensitivity': 0.8, 'motion_sensitivity': 0.2}\n        ),\n        'config': MultimodalConfiguration(\n            sync_mode=SyncMode.TIGHT,\n            modality_weights={'audio': 0.75, 'visual': 0.25},\n            cross_modal_coherence=0.9\n        )\n    },\n    {\n        'name': '焦虑症状缓解',\n        'description': '广泛性焦虑的多感官缓解治疗',\n        'request': MultimodalTherapyRequest(\n            user_id='user_anxiety_002',\n            current_emotion={'valence': -0.5, 'arousal': 0.8},\n            target_emotion={'valence': 0.1, 'arousal': -0.2},\n            therapy_focus='anxiety',\n            session_duration=720,\n            sensitivity_profile={'visual_sensitivity': 0.6, 'audio_sensitivity': 0.7, 'motion_sensitivity': 0.4}\n        ),\n        'config': MultimodalConfiguration(\n            sync_mode=SyncMode.ADAPTIVE,\n            modality_weights={'audio': 0.6, 'visual': 0.4},\n            cross_modal_coherence=0.85\n        )\n    },\n    {\n        'name': '抑郁情绪提升',\n        'description': '轻度抑郁的情绪激活治疗',\n        'request': MultimodalTherapyRequest(\n            user_id='user_depression_003',\n            current_emotion={'valence': -0.7, 'arousal': -0.3},\n            target_emotion={'valence': 0.3, 'arousal': 0.1},\n            therapy_focus='depression',\n            session_duration=600,\n            sensitivity_profile={'visual_sensitivity': 0.8, 'audio_sensitivity': 0.5, 'motion_sensitivity': 0.6}\n        ),\n        'config': MultimodalConfiguration(\n            sync_mode=SyncMode.TIGHT,\n            modality_weights={'audio': 0.4, 'visual': 0.6},\n            cross_modal_coherence=0.8\n        )\n    }\n]\n\n# 执行多模态治疗测试\nmultimodal_results = []\n\nprint(\"🎭 开始多模态治疗系统测试...\\n\")\n\nfor i, scenario in enumerate(multimodal_scenarios, 1):\n    print(f\"📊 场景 {i}: {scenario['name']}\")\n    print(f\"   描述: {scenario['description']}\")\n    \n    # 生成多模态治疗内容\n    start_time = time.time()\n    result = multimodal_system.generate_multimodal_therapy(\n        scenario['request'], scenario['config']\n    )\n    total_time = time.time() - start_time\n    \n    # 保存结果\n    result['scenario'] = scenario\n    result['total_generation_time'] = total_time\n    multimodal_results.append(result)\n    \n    # 显示关键指标\n    synergy = result['synergy_metrics']\n    prediction = result['therapy_prediction']\n    \n    print(f\"   ✅ 生成完成 (总耗时: {total_time:.2f}s)\")\n    print(f\"   🎵 音频质量: {result['audio_content']['quality_score']:.3f}\")\n    print(f\"   📹 视频质量: {result['video_content']['quality_score']:.3f}\")\n    print(f\"   ⚡ 同步精度: {result['synchronized_content']['temporal_alignment']:.3f}\")\n    print(f\"   🤝 协同得分: {synergy['synergy_score']:.3f}\")\n    print(f\"   🎯 治疗预测: {prediction['predicted_outcome']}\")\n    print(f\"   📈 多模态增强: +{prediction['enhancement_vs_single_modal']:.1%}\")\n    print()\n\n# 综合性能分析\navg_synergy = np.mean([r['synergy_metrics']['synergy_score'] for r in multimodal_results])\navg_effectiveness = np.mean([r['therapy_prediction']['effectiveness_score'] for r in multimodal_results])\navg_enhancement = np.mean([r['therapy_prediction']['enhancement_vs_single_modal'] for r in multimodal_results])\n\nprint(f\"📈 多模态系统综合表现:\")\nprint(f\"   • 平均协同得分: {avg_synergy:.3f}\")\nprint(f\"   • 平均治疗效果: {avg_effectiveness:.3f}\")\nprint(f\"   • 平均多模态增强: +{avg_enhancement:.1%}\")\nprint(f\"   • 系统稳定性: {'优秀' if np.std([r['synergy_metrics']['synergy_score'] for r in multimodal_results]) < 0.1 else '良好'}\")\n\n# 最终评估和总结\nprint(f\"\\n🏆 《心境流转》多模态治疗系统评估完成!\")\nprint(f\"✅ 测试场景: {len(multimodal_scenarios)}个\")\nprint(f\"📊 综合评级: {'A 优秀' if avg_synergy > 0.8 and avg_effectiveness > 0.75 else 'B+ 良好'}\")\nprint(f\"🎯 核心优势: 音视频协同治疗，多感官整合，实时同步优化\")\n\nprint(\"\\n💾 多模态治疗测试数据已保存，可用于后续系统优化。\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === 完整多模态系统实现 (继续) ===\n\n# 添加同步和协同效果评估方法\ndef _synchronize_modalities(self, audio_content: Dict, video_content: Dict, \n                           config: MultimodalConfiguration) -> Dict:\n    \"\"\"同步多模态内容\"\"\"\n    \n    # 计算同步延迟\n    audio_latency = self.audio_generator['latency']\n    video_latency = self.video_generator['latency']\n    sync_offset = video_latency - audio_latency\n    \n    # 模拟同步质量\n    if config.sync_mode == SyncMode.TIGHT:\n        sync_precision = 0.02\n    elif config.sync_mode == SyncMode.LOOSE:\n        sync_precision = 0.1\n    else:\n        sync_precision = 0.05\n        \n    return {\n        'sync_offset': sync_offset,\n        'sync_precision': sync_precision,\n        'temporal_alignment': np.random.normal(0.92, 0.05),\n        'content_coherence': np.random.normal(0.88, 0.08),\n        'buffer_efficiency': np.random.normal(0.85, 0.06)\n    }\n\ndef _evaluate_modal_synergy(self, sync_content: Dict, emotion_analysis: Dict) -> Dict:\n    \"\"\"评估模态协同效果\"\"\"\n    \n    # 基础协同得分\n    base_synergy = sync_content['temporal_alignment'] * sync_content['content_coherence']\n    \n    # 情绪匹配度\n    emotion_match = 1.0 - emotion_analysis['change_magnitude'] * 0.2\n    \n    # 综合协同得分\n    synergy_score = (base_synergy * 0.6 + emotion_match * 0.4)\n    \n    return {\n        'synergy_score': synergy_score,\n        'temporal_sync': sync_content['temporal_alignment'],\n        'content_harmony': sync_content['content_coherence'],\n        'emotion_alignment': emotion_match,\n        'enhancement_factor': max(1.0, synergy_score * 1.3),  # 多模态增强系数\n        'predicted_effectiveness': synergy_score * emotion_analysis['required_intensity']\n    }\n\ndef _predict_therapy_outcome(self, synergy_metrics: Dict, emotion_analysis: Dict) -> Dict:\n    \"\"\"预测治疗结果\"\"\"\n    \n    effectiveness = synergy_metrics['predicted_effectiveness']\n    \n    if effectiveness >= 0.85:\n        outcome = \"优秀 - 显著改善\"\n        confidence = 0.92\n    elif effectiveness >= 0.70:\n        outcome = \"良好 - 有效改善\"\n        confidence = 0.85\n    elif effectiveness >= 0.55:\n        outcome = \"一般 - 轻微改善\"\n        confidence = 0.72\n    else:\n        outcome = \"待优化 - 效果有限\"\n        confidence = 0.60\n        \n    return {\n        'predicted_outcome': outcome,\n        'effectiveness_score': effectiveness,\n        'confidence_level': confidence,\n        'enhancement_vs_single_modal': synergy_metrics['enhancement_factor'] - 1.0\n    }\n\n# 将方法添加到类中\nMockMultimodalTherapySystem._synchronize_modalities = _synchronize_modalities\nMockMultimodalTherapySystem._evaluate_modal_synergy = _evaluate_modal_synergy  \nMockMultimodalTherapySystem._predict_therapy_outcome = _predict_therapy_outcome\n\nprint(\"✅ 多模态同步和协同评估功能已加载\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === 2. 多模态治疗系统架构 ===\n\nclass SyncMode(Enum):\n    \"\"\"同步模式\"\"\"\n    TIGHT = \"tight\"  # 紧密同步\n    LOOSE = \"loose\"  # 松散同步  \n    ADAPTIVE = \"adaptive\"  # 自适应同步\n    INDEPENDENT = \"independent\"  # 独立运行\n\nclass ModalityWeight(Enum):\n    \"\"\"模态权重策略\"\"\"\n    BALANCED = \"balanced\"  # 均衡\n    AUDIO_DOMINANT = \"audio_dominant\"  # 音频主导\n    VISUAL_DOMINANT = \"visual_dominant\"  # 视觉主导\n    ADAPTIVE = \"adaptive\"  # 自适应调整\n\n@dataclass\nclass MultimodalConfiguration:\n    \"\"\"多模态配置\"\"\"\n    sync_mode: SyncMode = SyncMode.TIGHT\n    modality_weights: Dict[str, float] = None\n    cross_modal_coherence: float = 0.8  # 跨模态一致性\n    sensory_integration: bool = True  # 感官整合\n    adaptive_balancing: bool = True  # 自适应平衡\n    \n    def __post_init__(self):\n        if self.modality_weights is None:\n            self.modality_weights = {'audio': 0.6, 'visual': 0.4}\n\n@dataclass\nclass MultimodalTherapyRequest:\n    \"\"\"多模态治疗请求\"\"\"\n    user_id: str\n    current_emotion: Dict[str, float]  # V-A情绪状态\n    target_emotion: Dict[str, float]  # 目标情绪状态\n    therapy_focus: str  # 治疗重点: sleep/anxiety/depression\n    session_duration: int = 600  # 会话时长(秒)\n    preferred_modalities: List[str] = None  # 偏好模态\n    sensitivity_profile: Dict[str, float] = None  # 敏感度档案\n    \n    def __post_init__(self):\n        if self.preferred_modalities is None:\n            self.preferred_modalities = ['audio', 'visual']\n        if self.sensitivity_profile is None:\n            self.sensitivity_profile = {\n                'visual_sensitivity': 0.5,\n                'audio_sensitivity': 0.5,\n                'motion_sensitivity': 0.3\n            }\n\nclass MockMultimodalTherapySystem:\n    \"\"\"模拟多模态治疗系统\"\"\"\n    \n    def __init__(self):\n        self.audio_generator = self._init_audio_generator()\n        self.video_generator = self._init_video_generator()\n        self.sync_controller = self._init_sync_controller()\n        \n    def _init_audio_generator(self):\n        \"\"\"初始化音频生成器\"\"\"\n        return {\n            'model': 'therapeutic_music_generator_v2',\n            'capabilities': ['tone_generation', 'rhythm_sync', 'frequency_modulation'],\n            'latency': 0.1  # 音频生成延迟\n        }\n    \n    def _init_video_generator(self):\n        \"\"\"初始化视频生成器\"\"\"\n        return {\n            'model': 'hunyuan_video_therapy_v1',\n            'capabilities': ['visual_sync', 'motion_adaptation', 'color_modulation'],\n            'latency': 2.5  # 视频生成延迟\n        }\n    \n    def _init_sync_controller(self):\n        \"\"\"初始化同步控制器\"\"\"\n        return {\n            'precision': 0.05,  # 同步精度(秒)\n            'buffer_size': 1.0,  # 缓冲区大小(秒)\n            'compensation_enabled': True  # 延迟补偿\n        }\n    \n    def generate_multimodal_therapy(self, request: MultimodalTherapyRequest, \n                                  config: MultimodalConfiguration) -> Dict[str, Any]:\n        \"\"\"生成多模态治疗内容\"\"\"\n        \n        # 分析情绪转换需求\n        emotion_analysis = self._analyze_emotion_requirements(\n            request.current_emotion, request.target_emotion\n        )\n        \n        # 生成音频内容\n        audio_content = self._generate_audio_content(\n            emotion_analysis, request, config\n        )\n        \n        # 生成视频内容  \n        video_content = self._generate_video_content(\n            emotion_analysis, request, config\n        )\n        \n        # 执行多模态同步\n        synchronized_content = self._synchronize_modalities(\n            audio_content, video_content, config\n        )\n        \n        # 评估协同效果\n        synergy_metrics = self._evaluate_modal_synergy(\n            synchronized_content, emotion_analysis\n        )\n        \n        return {\n            'session_id': f\"multimodal_{int(time.time())}\",\n            'audio_content': audio_content,\n            'video_content': video_content,\n            'synchronized_content': synchronized_content,\n            'synergy_metrics': synergy_metrics,\n            'therapy_prediction': self._predict_therapy_outcome(\n                synergy_metrics, emotion_analysis\n            )\n        }\n    \n    def _analyze_emotion_requirements(self, current: Dict, target: Dict) -> Dict:\n        \"\"\"分析情绪需求\"\"\"\n        valence_change = target['valence'] - current['valence']\n        arousal_change = target['arousal'] - current['arousal']\n        \n        # 确定治疗策略\n        if arousal_change < -0.3:\n            strategy = \"deep_relaxation\"\n        elif arousal_change < 0:\n            strategy = \"gentle_calming\"\n        elif valence_change > 0.3:\n            strategy = \"mood_uplift\"\n        else:\n            strategy = \"stabilization\"\n            \n        return {\n            'valence_change': valence_change,\n            'arousal_change': arousal_change,\n            'change_magnitude': np.sqrt(valence_change**2 + arousal_change**2),\n            'therapy_strategy': strategy,\n            'required_intensity': min(1.0, abs(valence_change) + abs(arousal_change)),\n            'modality_preference': self._determine_modality_preference(\n                valence_change, arousal_change\n            )\n        }\n    \n    def _determine_modality_preference(self, valence_change: float, \n                                     arousal_change: float) -> Dict[str, float]:\n        \"\"\"确定模态偏好\"\"\"\n        # 基于情绪变化确定音视频权重\n        if arousal_change < -0.4:  # 需要大幅降低唤醒度\n            return {'audio': 0.7, 'visual': 0.3}  # 音频主导\n        elif valence_change > 0.4:  # 需要大幅提升情绪效价\n            return {'audio': 0.4, 'visual': 0.6}  # 视觉主导\n        else:\n            return {'audio': 0.5, 'visual': 0.5}  # 均衡\n    \n    def _generate_audio_content(self, emotion_analysis: Dict, \n                              request: MultimodalTherapyRequest,\n                              config: MultimodalConfiguration) -> Dict:\n        \"\"\"生成音频内容\"\"\"\n        \n        # 模拟音频生成\n        generation_time = np.random.uniform(0.5, 2.0)\n        \n        # 基于情绪分析调整音频参数\n        if emotion_analysis['therapy_strategy'] == 'deep_relaxation':\n            tempo = np.random.uniform(40, 60)\n            frequency_range = (60, 200)  # 低频为主\n        elif emotion_analysis['therapy_strategy'] == 'gentle_calming':\n            tempo = np.random.uniform(60, 80)\n            frequency_range = (80, 400)\n        elif emotion_analysis['therapy_strategy'] == 'mood_uplift':\n            tempo = np.random.uniform(80, 100)\n            frequency_range = (200, 800)\n        else:\n            tempo = np.random.uniform(70, 90)\n            frequency_range = (100, 500)\n            \n        return {\n            'content_id': f\"audio_{int(time.time())}\",\n            'duration': request.session_duration,\n            'tempo_bpm': tempo,\n            'frequency_range': frequency_range,\n            'generation_time': generation_time,\n            'quality_score': np.random.normal(0.82, 0.08),\n            'therapeutic_features': {\n                'binaural_beats': tempo < 70,\n                'nature_sounds': emotion_analysis['therapy_strategy'] == 'deep_relaxation',\n                'harmonic_progression': True,\n                'volume_modulation': True\n            }\n        }\n    \n    def _generate_video_content(self, emotion_analysis: Dict,\n                              request: MultimodalTherapyRequest,\n                              config: MultimodalConfiguration) -> Dict:\n        \"\"\"生成视频内容\"\"\"\n        \n        # 模拟视频生成\n        generation_time = np.random.uniform(3.0, 8.0)\n        \n        # 基于情绪分析选择视觉风格\n        style_mapping = {\n            'deep_relaxation': 'flowing_water',\n            'gentle_calming': 'soft_nature',\n            'mood_uplift': 'warm_light',\n            'stabilization': 'abstract_calm'\n        }\n        \n        visual_style = style_mapping.get(\n            emotion_analysis['therapy_strategy'], 'abstract_calm'\n        )\n        \n        return {\n            'content_id': f\"video_{int(time.time())}\",\n            'duration': request.session_duration,\n            'visual_style': visual_style,\n            'motion_intensity': max(0.1, 1.0 - abs(emotion_analysis['arousal_change'])),\n            'color_temperature': 3000 - emotion_analysis['arousal_change'] * 400,\n            'brightness': 0.3 - abs(emotion_analysis['arousal_change']) * 0.15,\n            'generation_time': generation_time,\n            'quality_score': np.random.normal(0.79, 0.10),\n            'visual_features': {\n                'breathing_sync': True,\n                'focus_guidance': emotion_analysis['change_magnitude'] > 0.5,\n                'color_therapy': True,\n                'geometric_patterns': visual_style == 'abstract_calm'\n            }\n        }\n\n# 初始化多模态治疗系统\nmultimodal_system = MockMultimodalTherapySystem()\n\nprint(\"✅ 多模态治疗系统初始化完成\")\nprint(\"🎵 音频生成器: therapeutic_music_generator_v2\")\nprint(\"📹 视频生成器: hunyuan_video_therapy_v1\") \nprint(\"⚡ 同步控制器: 精度0.05秒\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === 1. 系统初始化和多模态架构 ===\nimport sys\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport json\nimport time\nfrom datetime import datetime\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom enum import Enum\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# 添加项目路径\nproject_root = Path(__file__).parent.parent\nsys.path.append(str(project_root))\n\n# 设置中文字体\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\nprint(\"🎭 《心境流转》多模态治疗系统 - 初始化完成\")\nprint(f\"🕒 启动时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(\"🎯 目标: 音视频协同治疗效果评估与优化\")\nprint(\"=\"*60)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 《心境流转》多模态治疗系统测试\n## 08_multimodal_therapy_test.ipynb\n\n### 实验目标\n- 测试音视频协同治疗效果\n- 验证多感官刺激对情绪调节的影响\n- 评估实时多模态内容同步性\n- 优化跨模态治疗参数配置\n\n### 核心技术\n- 音视频时间同步\n- 多感官体验设计\n- 实时情绪反馈融合\n- 跨模态参数优化\n\n---\n\n**实验环境**: JupyterHub GPU 环境  \n**GPU要求**: 40-80GB显存  \n**测试模式**: 模拟多模态融合 (避免大模型加载)",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}