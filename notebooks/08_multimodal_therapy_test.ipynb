{
 "cells": [
  {
   "cell_type": "code",
   "source": "# === 3. å¤šæ¨¡æ€æ²»ç–—åœºæ™¯æµ‹è¯• ===\n\n# å®šä¹‰æµ‹è¯•åœºæ™¯\nmultimodal_scenarios = [\n    {\n        'name': 'æ·±åº¦ç¡çœ è¯±å¯¼',\n        'description': 'é‡åº¦å¤±çœ æ‚£è€…çš„æ·±åº¦ç¡çœ è¯±å¯¼',\n        'request': MultimodalTherapyRequest(\n            user_id='user_insomnia_001',\n            current_emotion={'valence': -0.4, 'arousal': 0.6},\n            target_emotion={'valence': 0.2, 'arousal': -0.7},\n            therapy_focus='sleep',\n            session_duration=900,\n            sensitivity_profile={'visual_sensitivity': 0.3, 'audio_sensitivity': 0.8, 'motion_sensitivity': 0.2}\n        ),\n        'config': MultimodalConfiguration(\n            sync_mode=SyncMode.TIGHT,\n            modality_weights={'audio': 0.75, 'visual': 0.25},\n            cross_modal_coherence=0.9\n        )\n    },\n    {\n        'name': 'ç„¦è™‘ç—‡çŠ¶ç¼“è§£',\n        'description': 'å¹¿æ³›æ€§ç„¦è™‘çš„å¤šæ„Ÿå®˜ç¼“è§£æ²»ç–—',\n        'request': MultimodalTherapyRequest(\n            user_id='user_anxiety_002',\n            current_emotion={'valence': -0.5, 'arousal': 0.8},\n            target_emotion={'valence': 0.1, 'arousal': -0.2},\n            therapy_focus='anxiety',\n            session_duration=720,\n            sensitivity_profile={'visual_sensitivity': 0.6, 'audio_sensitivity': 0.7, 'motion_sensitivity': 0.4}\n        ),\n        'config': MultimodalConfiguration(\n            sync_mode=SyncMode.ADAPTIVE,\n            modality_weights={'audio': 0.6, 'visual': 0.4},\n            cross_modal_coherence=0.85\n        )\n    },\n    {\n        'name': 'æŠ‘éƒæƒ…ç»ªæå‡',\n        'description': 'è½»åº¦æŠ‘éƒçš„æƒ…ç»ªæ¿€æ´»æ²»ç–—',\n        'request': MultimodalTherapyRequest(\n            user_id='user_depression_003',\n            current_emotion={'valence': -0.7, 'arousal': -0.3},\n            target_emotion={'valence': 0.3, 'arousal': 0.1},\n            therapy_focus='depression',\n            session_duration=600,\n            sensitivity_profile={'visual_sensitivity': 0.8, 'audio_sensitivity': 0.5, 'motion_sensitivity': 0.6}\n        ),\n        'config': MultimodalConfiguration(\n            sync_mode=SyncMode.TIGHT,\n            modality_weights={'audio': 0.4, 'visual': 0.6},\n            cross_modal_coherence=0.8\n        )\n    }\n]\n\n# æ‰§è¡Œå¤šæ¨¡æ€æ²»ç–—æµ‹è¯•\nmultimodal_results = []\n\nprint(\"ğŸ­ å¼€å§‹å¤šæ¨¡æ€æ²»ç–—ç³»ç»Ÿæµ‹è¯•...\\n\")\n\nfor i, scenario in enumerate(multimodal_scenarios, 1):\n    print(f\"ğŸ“Š åœºæ™¯ {i}: {scenario['name']}\")\n    print(f\"   æè¿°: {scenario['description']}\")\n    \n    # ç”Ÿæˆå¤šæ¨¡æ€æ²»ç–—å†…å®¹\n    start_time = time.time()\n    result = multimodal_system.generate_multimodal_therapy(\n        scenario['request'], scenario['config']\n    )\n    total_time = time.time() - start_time\n    \n    # ä¿å­˜ç»“æœ\n    result['scenario'] = scenario\n    result['total_generation_time'] = total_time\n    multimodal_results.append(result)\n    \n    # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡\n    synergy = result['synergy_metrics']\n    prediction = result['therapy_prediction']\n    \n    print(f\"   âœ… ç”Ÿæˆå®Œæˆ (æ€»è€—æ—¶: {total_time:.2f}s)\")\n    print(f\"   ğŸµ éŸ³é¢‘è´¨é‡: {result['audio_content']['quality_score']:.3f}\")\n    print(f\"   ğŸ“¹ è§†é¢‘è´¨é‡: {result['video_content']['quality_score']:.3f}\")\n    print(f\"   âš¡ åŒæ­¥ç²¾åº¦: {result['synchronized_content']['temporal_alignment']:.3f}\")\n    print(f\"   ğŸ¤ ååŒå¾—åˆ†: {synergy['synergy_score']:.3f}\")\n    print(f\"   ğŸ¯ æ²»ç–—é¢„æµ‹: {prediction['predicted_outcome']}\")\n    print(f\"   ğŸ“ˆ å¤šæ¨¡æ€å¢å¼º: +{prediction['enhancement_vs_single_modal']:.1%}\")\n    print()\n\n# ç»¼åˆæ€§èƒ½åˆ†æ\navg_synergy = np.mean([r['synergy_metrics']['synergy_score'] for r in multimodal_results])\navg_effectiveness = np.mean([r['therapy_prediction']['effectiveness_score'] for r in multimodal_results])\navg_enhancement = np.mean([r['therapy_prediction']['enhancement_vs_single_modal'] for r in multimodal_results])\n\nprint(f\"ğŸ“ˆ å¤šæ¨¡æ€ç³»ç»Ÿç»¼åˆè¡¨ç°:\")\nprint(f\"   â€¢ å¹³å‡ååŒå¾—åˆ†: {avg_synergy:.3f}\")\nprint(f\"   â€¢ å¹³å‡æ²»ç–—æ•ˆæœ: {avg_effectiveness:.3f}\")\nprint(f\"   â€¢ å¹³å‡å¤šæ¨¡æ€å¢å¼º: +{avg_enhancement:.1%}\")\nprint(f\"   â€¢ ç³»ç»Ÿç¨³å®šæ€§: {'ä¼˜ç§€' if np.std([r['synergy_metrics']['synergy_score'] for r in multimodal_results]) < 0.1 else 'è‰¯å¥½'}\")\n\n# æœ€ç»ˆè¯„ä¼°å’Œæ€»ç»“\nprint(f\"\\nğŸ† ã€Šå¿ƒå¢ƒæµè½¬ã€‹å¤šæ¨¡æ€æ²»ç–—ç³»ç»Ÿè¯„ä¼°å®Œæˆ!\")\nprint(f\"âœ… æµ‹è¯•åœºæ™¯: {len(multimodal_scenarios)}ä¸ª\")\nprint(f\"ğŸ“Š ç»¼åˆè¯„çº§: {'A ä¼˜ç§€' if avg_synergy > 0.8 and avg_effectiveness > 0.75 else 'B+ è‰¯å¥½'}\")\nprint(f\"ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿: éŸ³è§†é¢‘ååŒæ²»ç–—ï¼Œå¤šæ„Ÿå®˜æ•´åˆï¼Œå®æ—¶åŒæ­¥ä¼˜åŒ–\")\n\nprint(\"\\nğŸ’¾ å¤šæ¨¡æ€æ²»ç–—æµ‹è¯•æ•°æ®å·²ä¿å­˜ï¼Œå¯ç”¨äºåç»­ç³»ç»Ÿä¼˜åŒ–ã€‚\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === å®Œæ•´å¤šæ¨¡æ€ç³»ç»Ÿå®ç° (ç»§ç»­) ===\n\n# æ·»åŠ åŒæ­¥å’ŒååŒæ•ˆæœè¯„ä¼°æ–¹æ³•\ndef _synchronize_modalities(self, audio_content: Dict, video_content: Dict, \n                           config: MultimodalConfiguration) -> Dict:\n    \"\"\"åŒæ­¥å¤šæ¨¡æ€å†…å®¹\"\"\"\n    \n    # è®¡ç®—åŒæ­¥å»¶è¿Ÿ\n    audio_latency = self.audio_generator['latency']\n    video_latency = self.video_generator['latency']\n    sync_offset = video_latency - audio_latency\n    \n    # æ¨¡æ‹ŸåŒæ­¥è´¨é‡\n    if config.sync_mode == SyncMode.TIGHT:\n        sync_precision = 0.02\n    elif config.sync_mode == SyncMode.LOOSE:\n        sync_precision = 0.1\n    else:\n        sync_precision = 0.05\n        \n    return {\n        'sync_offset': sync_offset,\n        'sync_precision': sync_precision,\n        'temporal_alignment': np.random.normal(0.92, 0.05),\n        'content_coherence': np.random.normal(0.88, 0.08),\n        'buffer_efficiency': np.random.normal(0.85, 0.06)\n    }\n\ndef _evaluate_modal_synergy(self, sync_content: Dict, emotion_analysis: Dict) -> Dict:\n    \"\"\"è¯„ä¼°æ¨¡æ€ååŒæ•ˆæœ\"\"\"\n    \n    # åŸºç¡€ååŒå¾—åˆ†\n    base_synergy = sync_content['temporal_alignment'] * sync_content['content_coherence']\n    \n    # æƒ…ç»ªåŒ¹é…åº¦\n    emotion_match = 1.0 - emotion_analysis['change_magnitude'] * 0.2\n    \n    # ç»¼åˆååŒå¾—åˆ†\n    synergy_score = (base_synergy * 0.6 + emotion_match * 0.4)\n    \n    return {\n        'synergy_score': synergy_score,\n        'temporal_sync': sync_content['temporal_alignment'],\n        'content_harmony': sync_content['content_coherence'],\n        'emotion_alignment': emotion_match,\n        'enhancement_factor': max(1.0, synergy_score * 1.3),  # å¤šæ¨¡æ€å¢å¼ºç³»æ•°\n        'predicted_effectiveness': synergy_score * emotion_analysis['required_intensity']\n    }\n\ndef _predict_therapy_outcome(self, synergy_metrics: Dict, emotion_analysis: Dict) -> Dict:\n    \"\"\"é¢„æµ‹æ²»ç–—ç»“æœ\"\"\"\n    \n    effectiveness = synergy_metrics['predicted_effectiveness']\n    \n    if effectiveness >= 0.85:\n        outcome = \"ä¼˜ç§€ - æ˜¾è‘—æ”¹å–„\"\n        confidence = 0.92\n    elif effectiveness >= 0.70:\n        outcome = \"è‰¯å¥½ - æœ‰æ•ˆæ”¹å–„\"\n        confidence = 0.85\n    elif effectiveness >= 0.55:\n        outcome = \"ä¸€èˆ¬ - è½»å¾®æ”¹å–„\"\n        confidence = 0.72\n    else:\n        outcome = \"å¾…ä¼˜åŒ– - æ•ˆæœæœ‰é™\"\n        confidence = 0.60\n        \n    return {\n        'predicted_outcome': outcome,\n        'effectiveness_score': effectiveness,\n        'confidence_level': confidence,\n        'enhancement_vs_single_modal': synergy_metrics['enhancement_factor'] - 1.0\n    }\n\n# å°†æ–¹æ³•æ·»åŠ åˆ°ç±»ä¸­\nMockMultimodalTherapySystem._synchronize_modalities = _synchronize_modalities\nMockMultimodalTherapySystem._evaluate_modal_synergy = _evaluate_modal_synergy  \nMockMultimodalTherapySystem._predict_therapy_outcome = _predict_therapy_outcome\n\nprint(\"âœ… å¤šæ¨¡æ€åŒæ­¥å’ŒååŒè¯„ä¼°åŠŸèƒ½å·²åŠ è½½\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === 2. å¤šæ¨¡æ€æ²»ç–—ç³»ç»Ÿæ¶æ„ ===\n\nclass SyncMode(Enum):\n    \"\"\"åŒæ­¥æ¨¡å¼\"\"\"\n    TIGHT = \"tight\"  # ç´§å¯†åŒæ­¥\n    LOOSE = \"loose\"  # æ¾æ•£åŒæ­¥  \n    ADAPTIVE = \"adaptive\"  # è‡ªé€‚åº”åŒæ­¥\n    INDEPENDENT = \"independent\"  # ç‹¬ç«‹è¿è¡Œ\n\nclass ModalityWeight(Enum):\n    \"\"\"æ¨¡æ€æƒé‡ç­–ç•¥\"\"\"\n    BALANCED = \"balanced\"  # å‡è¡¡\n    AUDIO_DOMINANT = \"audio_dominant\"  # éŸ³é¢‘ä¸»å¯¼\n    VISUAL_DOMINANT = \"visual_dominant\"  # è§†è§‰ä¸»å¯¼\n    ADAPTIVE = \"adaptive\"  # è‡ªé€‚åº”è°ƒæ•´\n\n@dataclass\nclass MultimodalConfiguration:\n    \"\"\"å¤šæ¨¡æ€é…ç½®\"\"\"\n    sync_mode: SyncMode = SyncMode.TIGHT\n    modality_weights: Dict[str, float] = None\n    cross_modal_coherence: float = 0.8  # è·¨æ¨¡æ€ä¸€è‡´æ€§\n    sensory_integration: bool = True  # æ„Ÿå®˜æ•´åˆ\n    adaptive_balancing: bool = True  # è‡ªé€‚åº”å¹³è¡¡\n    \n    def __post_init__(self):\n        if self.modality_weights is None:\n            self.modality_weights = {'audio': 0.6, 'visual': 0.4}\n\n@dataclass\nclass MultimodalTherapyRequest:\n    \"\"\"å¤šæ¨¡æ€æ²»ç–—è¯·æ±‚\"\"\"\n    user_id: str\n    current_emotion: Dict[str, float]  # V-Aæƒ…ç»ªçŠ¶æ€\n    target_emotion: Dict[str, float]  # ç›®æ ‡æƒ…ç»ªçŠ¶æ€\n    therapy_focus: str  # æ²»ç–—é‡ç‚¹: sleep/anxiety/depression\n    session_duration: int = 600  # ä¼šè¯æ—¶é•¿(ç§’)\n    preferred_modalities: List[str] = None  # åå¥½æ¨¡æ€\n    sensitivity_profile: Dict[str, float] = None  # æ•æ„Ÿåº¦æ¡£æ¡ˆ\n    \n    def __post_init__(self):\n        if self.preferred_modalities is None:\n            self.preferred_modalities = ['audio', 'visual']\n        if self.sensitivity_profile is None:\n            self.sensitivity_profile = {\n                'visual_sensitivity': 0.5,\n                'audio_sensitivity': 0.5,\n                'motion_sensitivity': 0.3\n            }\n\nclass MockMultimodalTherapySystem:\n    \"\"\"æ¨¡æ‹Ÿå¤šæ¨¡æ€æ²»ç–—ç³»ç»Ÿ\"\"\"\n    \n    def __init__(self):\n        self.audio_generator = self._init_audio_generator()\n        self.video_generator = self._init_video_generator()\n        self.sync_controller = self._init_sync_controller()\n        \n    def _init_audio_generator(self):\n        \"\"\"åˆå§‹åŒ–éŸ³é¢‘ç”Ÿæˆå™¨\"\"\"\n        return {\n            'model': 'therapeutic_music_generator_v2',\n            'capabilities': ['tone_generation', 'rhythm_sync', 'frequency_modulation'],\n            'latency': 0.1  # éŸ³é¢‘ç”Ÿæˆå»¶è¿Ÿ\n        }\n    \n    def _init_video_generator(self):\n        \"\"\"åˆå§‹åŒ–è§†é¢‘ç”Ÿæˆå™¨\"\"\"\n        return {\n            'model': 'hunyuan_video_therapy_v1',\n            'capabilities': ['visual_sync', 'motion_adaptation', 'color_modulation'],\n            'latency': 2.5  # è§†é¢‘ç”Ÿæˆå»¶è¿Ÿ\n        }\n    \n    def _init_sync_controller(self):\n        \"\"\"åˆå§‹åŒ–åŒæ­¥æ§åˆ¶å™¨\"\"\"\n        return {\n            'precision': 0.05,  # åŒæ­¥ç²¾åº¦(ç§’)\n            'buffer_size': 1.0,  # ç¼“å†²åŒºå¤§å°(ç§’)\n            'compensation_enabled': True  # å»¶è¿Ÿè¡¥å¿\n        }\n    \n    def generate_multimodal_therapy(self, request: MultimodalTherapyRequest, \n                                  config: MultimodalConfiguration) -> Dict[str, Any]:\n        \"\"\"ç”Ÿæˆå¤šæ¨¡æ€æ²»ç–—å†…å®¹\"\"\"\n        \n        # åˆ†ææƒ…ç»ªè½¬æ¢éœ€æ±‚\n        emotion_analysis = self._analyze_emotion_requirements(\n            request.current_emotion, request.target_emotion\n        )\n        \n        # ç”ŸæˆéŸ³é¢‘å†…å®¹\n        audio_content = self._generate_audio_content(\n            emotion_analysis, request, config\n        )\n        \n        # ç”Ÿæˆè§†é¢‘å†…å®¹  \n        video_content = self._generate_video_content(\n            emotion_analysis, request, config\n        )\n        \n        # æ‰§è¡Œå¤šæ¨¡æ€åŒæ­¥\n        synchronized_content = self._synchronize_modalities(\n            audio_content, video_content, config\n        )\n        \n        # è¯„ä¼°ååŒæ•ˆæœ\n        synergy_metrics = self._evaluate_modal_synergy(\n            synchronized_content, emotion_analysis\n        )\n        \n        return {\n            'session_id': f\"multimodal_{int(time.time())}\",\n            'audio_content': audio_content,\n            'video_content': video_content,\n            'synchronized_content': synchronized_content,\n            'synergy_metrics': synergy_metrics,\n            'therapy_prediction': self._predict_therapy_outcome(\n                synergy_metrics, emotion_analysis\n            )\n        }\n    \n    def _analyze_emotion_requirements(self, current: Dict, target: Dict) -> Dict:\n        \"\"\"åˆ†ææƒ…ç»ªéœ€æ±‚\"\"\"\n        valence_change = target['valence'] - current['valence']\n        arousal_change = target['arousal'] - current['arousal']\n        \n        # ç¡®å®šæ²»ç–—ç­–ç•¥\n        if arousal_change < -0.3:\n            strategy = \"deep_relaxation\"\n        elif arousal_change < 0:\n            strategy = \"gentle_calming\"\n        elif valence_change > 0.3:\n            strategy = \"mood_uplift\"\n        else:\n            strategy = \"stabilization\"\n            \n        return {\n            'valence_change': valence_change,\n            'arousal_change': arousal_change,\n            'change_magnitude': np.sqrt(valence_change**2 + arousal_change**2),\n            'therapy_strategy': strategy,\n            'required_intensity': min(1.0, abs(valence_change) + abs(arousal_change)),\n            'modality_preference': self._determine_modality_preference(\n                valence_change, arousal_change\n            )\n        }\n    \n    def _determine_modality_preference(self, valence_change: float, \n                                     arousal_change: float) -> Dict[str, float]:\n        \"\"\"ç¡®å®šæ¨¡æ€åå¥½\"\"\"\n        # åŸºäºæƒ…ç»ªå˜åŒ–ç¡®å®šéŸ³è§†é¢‘æƒé‡\n        if arousal_change < -0.4:  # éœ€è¦å¤§å¹…é™ä½å”¤é†’åº¦\n            return {'audio': 0.7, 'visual': 0.3}  # éŸ³é¢‘ä¸»å¯¼\n        elif valence_change > 0.4:  # éœ€è¦å¤§å¹…æå‡æƒ…ç»ªæ•ˆä»·\n            return {'audio': 0.4, 'visual': 0.6}  # è§†è§‰ä¸»å¯¼\n        else:\n            return {'audio': 0.5, 'visual': 0.5}  # å‡è¡¡\n    \n    def _generate_audio_content(self, emotion_analysis: Dict, \n                              request: MultimodalTherapyRequest,\n                              config: MultimodalConfiguration) -> Dict:\n        \"\"\"ç”ŸæˆéŸ³é¢‘å†…å®¹\"\"\"\n        \n        # æ¨¡æ‹ŸéŸ³é¢‘ç”Ÿæˆ\n        generation_time = np.random.uniform(0.5, 2.0)\n        \n        # åŸºäºæƒ…ç»ªåˆ†æè°ƒæ•´éŸ³é¢‘å‚æ•°\n        if emotion_analysis['therapy_strategy'] == 'deep_relaxation':\n            tempo = np.random.uniform(40, 60)\n            frequency_range = (60, 200)  # ä½é¢‘ä¸ºä¸»\n        elif emotion_analysis['therapy_strategy'] == 'gentle_calming':\n            tempo = np.random.uniform(60, 80)\n            frequency_range = (80, 400)\n        elif emotion_analysis['therapy_strategy'] == 'mood_uplift':\n            tempo = np.random.uniform(80, 100)\n            frequency_range = (200, 800)\n        else:\n            tempo = np.random.uniform(70, 90)\n            frequency_range = (100, 500)\n            \n        return {\n            'content_id': f\"audio_{int(time.time())}\",\n            'duration': request.session_duration,\n            'tempo_bpm': tempo,\n            'frequency_range': frequency_range,\n            'generation_time': generation_time,\n            'quality_score': np.random.normal(0.82, 0.08),\n            'therapeutic_features': {\n                'binaural_beats': tempo < 70,\n                'nature_sounds': emotion_analysis['therapy_strategy'] == 'deep_relaxation',\n                'harmonic_progression': True,\n                'volume_modulation': True\n            }\n        }\n    \n    def _generate_video_content(self, emotion_analysis: Dict,\n                              request: MultimodalTherapyRequest,\n                              config: MultimodalConfiguration) -> Dict:\n        \"\"\"ç”Ÿæˆè§†é¢‘å†…å®¹\"\"\"\n        \n        # æ¨¡æ‹Ÿè§†é¢‘ç”Ÿæˆ\n        generation_time = np.random.uniform(3.0, 8.0)\n        \n        # åŸºäºæƒ…ç»ªåˆ†æé€‰æ‹©è§†è§‰é£æ ¼\n        style_mapping = {\n            'deep_relaxation': 'flowing_water',\n            'gentle_calming': 'soft_nature',\n            'mood_uplift': 'warm_light',\n            'stabilization': 'abstract_calm'\n        }\n        \n        visual_style = style_mapping.get(\n            emotion_analysis['therapy_strategy'], 'abstract_calm'\n        )\n        \n        return {\n            'content_id': f\"video_{int(time.time())}\",\n            'duration': request.session_duration,\n            'visual_style': visual_style,\n            'motion_intensity': max(0.1, 1.0 - abs(emotion_analysis['arousal_change'])),\n            'color_temperature': 3000 - emotion_analysis['arousal_change'] * 400,\n            'brightness': 0.3 - abs(emotion_analysis['arousal_change']) * 0.15,\n            'generation_time': generation_time,\n            'quality_score': np.random.normal(0.79, 0.10),\n            'visual_features': {\n                'breathing_sync': True,\n                'focus_guidance': emotion_analysis['change_magnitude'] > 0.5,\n                'color_therapy': True,\n                'geometric_patterns': visual_style == 'abstract_calm'\n            }\n        }\n\n# åˆå§‹åŒ–å¤šæ¨¡æ€æ²»ç–—ç³»ç»Ÿ\nmultimodal_system = MockMultimodalTherapySystem()\n\nprint(\"âœ… å¤šæ¨¡æ€æ²»ç–—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\")\nprint(\"ğŸµ éŸ³é¢‘ç”Ÿæˆå™¨: therapeutic_music_generator_v2\")\nprint(\"ğŸ“¹ è§†é¢‘ç”Ÿæˆå™¨: hunyuan_video_therapy_v1\") \nprint(\"âš¡ åŒæ­¥æ§åˆ¶å™¨: ç²¾åº¦0.05ç§’\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === 1. ç³»ç»Ÿåˆå§‹åŒ–å’Œå¤šæ¨¡æ€æ¶æ„ ===\nimport sys\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport json\nimport time\nfrom datetime import datetime\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom enum import Enum\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# æ·»åŠ é¡¹ç›®è·¯å¾„\nproject_root = Path(__file__).parent.parent\nsys.path.append(str(project_root))\n\n# è®¾ç½®ä¸­æ–‡å­—ä½“\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\nprint(\"ğŸ­ ã€Šå¿ƒå¢ƒæµè½¬ã€‹å¤šæ¨¡æ€æ²»ç–—ç³»ç»Ÿ - åˆå§‹åŒ–å®Œæˆ\")\nprint(f\"ğŸ•’ å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(\"ğŸ¯ ç›®æ ‡: éŸ³è§†é¢‘ååŒæ²»ç–—æ•ˆæœè¯„ä¼°ä¸ä¼˜åŒ–\")\nprint(\"=\"*60)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# ã€Šå¿ƒå¢ƒæµè½¬ã€‹å¤šæ¨¡æ€æ²»ç–—ç³»ç»Ÿæµ‹è¯•\n## 08_multimodal_therapy_test.ipynb\n\n### å®éªŒç›®æ ‡\n- æµ‹è¯•éŸ³è§†é¢‘ååŒæ²»ç–—æ•ˆæœ\n- éªŒè¯å¤šæ„Ÿå®˜åˆºæ¿€å¯¹æƒ…ç»ªè°ƒèŠ‚çš„å½±å“\n- è¯„ä¼°å®æ—¶å¤šæ¨¡æ€å†…å®¹åŒæ­¥æ€§\n- ä¼˜åŒ–è·¨æ¨¡æ€æ²»ç–—å‚æ•°é…ç½®\n\n### æ ¸å¿ƒæŠ€æœ¯\n- éŸ³è§†é¢‘æ—¶é—´åŒæ­¥\n- å¤šæ„Ÿå®˜ä½“éªŒè®¾è®¡\n- å®æ—¶æƒ…ç»ªåé¦ˆèåˆ\n- è·¨æ¨¡æ€å‚æ•°ä¼˜åŒ–\n\n---\n\n**å®éªŒç¯å¢ƒ**: JupyterHub GPU ç¯å¢ƒ  \n**GPUè¦æ±‚**: 40-80GBæ˜¾å­˜  \n**æµ‹è¯•æ¨¡å¼**: æ¨¡æ‹Ÿå¤šæ¨¡æ€èåˆ (é¿å…å¤§æ¨¡å‹åŠ è½½)",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}