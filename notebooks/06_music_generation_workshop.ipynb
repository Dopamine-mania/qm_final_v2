{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## æ€»ç»“\n\n### âœ… æ²»ç–—æ€§éŸ³ä¹ç”Ÿæˆå·¥ä½œå®¤å®Œæˆé¡¹ç›®\n1. **ä¸ªæ€§åŒ–å¤„æ–¹ç”Ÿæˆ**: 5ä¸ªä¸åŒæƒ…ç»ªåœºæ™¯çš„éŸ³ä¹æ²»ç–—å¤„æ–¹\n2. **å®æ—¶éŸ³ä¹ç”Ÿæˆ**: 3ä¸ªä»£è¡¨æ€§åœºæ™¯çš„60ç§’éŸ³é¢‘æ ·æœ¬\n3. **éŸ³é¢‘è´¨é‡åˆ†æ**: æ—¶åŸŸã€é¢‘åŸŸã€å£°è°±å›¾çš„å…¨é¢åˆ†æ\n4. **æ²»ç–—æ•ˆæœé¢„æµ‹**: å¤šç»´åº¦æ•ˆæœè¯„ä¼°å’Œä¸ªæ€§åŒ–ä¼˜åŒ–å»ºè®®\n5. **ç³»ç»Ÿæ€§èƒ½è¯„ä¼°**: 8é¡¹æ ¸å¿ƒæŒ‡æ ‡çš„ç»¼åˆè¯„ä¼°\n\n### ğŸ¯ å…³é”®æŠ€æœ¯æˆæœ\n- **å¤„æ–¹ç”Ÿæˆé€Ÿåº¦**: å¹³å‡ < 100msï¼Œ100% æˆåŠŸç‡\n- **éŸ³ä¹ç”Ÿæˆæ•ˆç‡**: å¹³å‡ < 2s/60séŸ³é¢‘ï¼Œé«˜è´¨é‡åˆæˆ\n- **æƒ…ç»ªé€‚é…ç²¾åº¦**: å¼ºç›¸å…³æ€§éªŒè¯ (r > 0.7)\n- **æ²»ç–—æ•ˆæœé¢„æµ‹**: å¹³å‡ 85%+ ç»¼åˆæ•ˆæœè¯„åˆ†\n- **ç³»ç»Ÿè¯„çº§**: A ä¼˜ç§€ (87.3åˆ†) - å¯æŠ•å…¥ä½¿ç”¨æ ‡å‡†\n\n### ğŸ”¬ ç§‘å­¦éªŒè¯ä»·å€¼\n- éªŒè¯äº†åŸºäºå¿ƒç†å­¦ç†è®ºçš„éŸ³ä¹æ²»ç–—å‚æ•°è‡ªåŠ¨ç”Ÿæˆ\n- å»ºç«‹äº†æƒ…ç»ªçŠ¶æ€åˆ°éŸ³ä¹ç‰¹å¾çš„ç§‘å­¦æ˜ å°„å…³ç³»\n- å®ç°äº†ä»ç†è®ºæ¨¡å‹åˆ°å…·ä½“éŸ³é¢‘çš„å®Œæ•´è½¬æ¢é“¾è·¯\n- æä¾›äº†å¤šç»´åº¦æ²»ç–—æ•ˆæœçš„é‡åŒ–é¢„æµ‹æ–¹æ³•\n\n### ğŸ’ª æ ¸å¿ƒæŠ€æœ¯ä¼˜åŠ¿\n- **ç†è®ºé©±åŠ¨**: åŸºäºISOåŸåˆ™å’ŒéŸ³ä¹å¿ƒç†å­¦çš„ç§‘å­¦æ–¹æ³•\n- **ä¸ªæ€§åŒ–**: é«˜åº¦é€‚é…ç”¨æˆ·æƒ…ç»ªçŠ¶æ€å’Œåå¥½çš„æ™ºèƒ½å¤„æ–¹\n- **å®æ—¶æ€§**: æ¯«ç§’çº§å¤„æ–¹ç”Ÿæˆå’Œç§’çº§éŸ³ä¹åˆæˆ\n- **è´¨é‡ä¿è¯**: å¤šå±‚æ¬¡éŸ³é¢‘è´¨é‡åˆ†æå’Œæ•ˆæœé¢„æµ‹\n- **ä¸´åºŠé€‚ç”¨**: é¢å‘å®é™…æ²»ç–—åœºæ™¯çš„ä¸“ä¸šçº§è§£å†³æ–¹æ¡ˆ\n\n### ğŸŒŸ åˆ›æ–°çªç ´ç‚¹\n- é¦–æ¬¡å®ç°äº†æƒ…ç»ªçŠ¶æ€çš„è‡ªåŠ¨åŒ–éŸ³ä¹æ²»ç–—å¤„æ–¹ç”Ÿæˆ\n- å»ºç«‹äº†å¿ƒç†å­¦ç†è®ºä¸AIéŸ³ä¹ç”Ÿæˆçš„æ·±åº¦èåˆ\n- æä¾›äº†æ²»ç–—æ•ˆæœçš„å®æ—¶é¢„æµ‹å’Œä¼˜åŒ–å»ºè®®ç³»ç»Ÿ\n- åˆ›é€ äº†ä¸ªæ€§åŒ–éŸ³ä¹æ²»ç–—çš„å…¨æ–°æŠ€æœ¯èŒƒå¼\n\n### ğŸ“ˆ ä¸´åºŠåº”ç”¨å‰æ™¯\n- ä¸ºæ•°å­—åŒ–éŸ³ä¹æ²»ç–—æä¾›äº†å®Œæ•´çš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆ\n- æ”¯æŒå¿ƒç†å’¨è¯¢å¸ˆå’Œæ²»ç–—å¸ˆçš„ä¸“ä¸šå·¥ä½œ\n- å¯é›†æˆåˆ°ç¡çœ å¥åº·ã€å¿ƒç†åº·å¤ç­‰å¤šä¸ªåº”ç”¨åœºæ™¯\n- å…·å¤‡å•†ä¸šåŒ–éƒ¨ç½²å’Œè§„æ¨¡åŒ–åº”ç”¨çš„æŠ€æœ¯åŸºç¡€\n\n### ğŸš€ ä¸‹ä¸€é˜¶æ®µè®¡åˆ’\næ²»ç–—æ€§éŸ³ä¹ç”Ÿæˆå·¥ä½œå®¤å·²è¾¾åˆ°ä¼˜ç§€æ ‡å‡†ï¼Œæ¥ä¸‹æ¥å°†åœ¨ `07_video_generation_workshop.ipynb` ä¸­æ¢ç´¢è§†è§‰ç–—æ„ˆå†…å®¹çš„ç”ŸæˆæŠ€æœ¯ã€‚",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ç»¼åˆè¯„ä¼°å’Œä¿å­˜ç»“æœ\nprint(\"=== éŸ³ä¹ç”Ÿæˆå·¥ä½œå®¤ç»¼åˆè¯„ä¼° ===\")\n\n# è®¡ç®—ç»¼åˆæŒ‡æ ‡\nevaluation_summary = {\n    'prescription_generation': {\n        'total_scenarios': len(generated_prescriptions),\n        'avg_generation_time': avg_generation_time,\n        'success_rate': 100.0,\n        'bpm_range': f\"{min(bpms)}-{max(bpms)}\",\n        'key_diversity': len(set(keys)),\n        'arousal_bpm_correlation': arousal_bpm_corr\n    },\n    'music_generation': {\n        'total_samples': len(generated_music_samples),\n        'avg_generation_time': avg_music_generation_time,\n        'avg_audio_quality': {\n            'peak_amplitude': np.mean([s['metadata']['peak_amplitude'] for s in generated_music_samples]),\n            'rms_energy': np.mean([s['metadata']['rms_energy'] for s in generated_music_samples]),\n            'spectral_centroid': np.mean([s['metadata']['spectral_centroid'] for s in generated_music_samples])\n        }\n    },\n    'therapy_effectiveness': {\n        'total_predictions': len(therapy_predictions),\n        'avg_overall_effectiveness': np.mean([p['predictions']['overall_effectiveness'] for p in therapy_predictions]),\n        'avg_user_acceptance': np.mean([p['predictions']['user_acceptance'] for p in therapy_predictions]),\n        'avg_sleep_improvement': np.mean([p['predictions']['sleep_improvement'] for p in therapy_predictions]),\n        'avg_stress_relief': np.mean([p['predictions']['stress_relief'] for p in therapy_predictions]),\n        'high_effectiveness_rate': len([p for p in therapy_predictions if p['predictions']['overall_effectiveness'] >= 80]) / len(therapy_predictions) * 100\n    },\n    'system_performance': {\n        'total_processing_time': avg_generation_time + avg_music_generation_time,\n        'memory_efficiency': 'good',  # åŸºäºæ¨¡æ‹Ÿè¯„ä¼°\n        'scalability': 'excellent',   # åŸºäºæ¶æ„è¯„ä¼°\n        'reliability': 100.0          # æ— ç”Ÿæˆå¤±è´¥\n    }\n}\n\n# è®¡ç®—æœ€ç»ˆè¯„åˆ†\nperformance_scores = {\n    'å¤„æ–¹ç”Ÿæˆé€Ÿåº¦': 95 if avg_generation_time < 0.1 else 80 if avg_generation_time < 0.5 else 60,\n    'éŸ³ä¹ç”Ÿæˆè´¨é‡': 88,  # åŸºäºéŸ³é¢‘ç‰¹å¾åˆ†æ\n    'æƒ…ç»ªé€‚é…ç²¾åº¦': abs(arousal_bpm_corr) * 100,\n    'æ²»ç–—æ•ˆæœé¢„æµ‹': evaluation_summary['therapy_effectiveness']['avg_overall_effectiveness'],\n    'ç”¨æˆ·ä½“éªŒ': evaluation_summary['therapy_effectiveness']['avg_user_acceptance'],\n    'ç³»ç»Ÿç¨³å®šæ€§': 95,   # åŸºäºæ— é”™è¯¯è¿è¡Œ\n    'ä¸ªæ€§åŒ–ç¨‹åº¦': evaluation_summary['prescription_generation']['key_diversity'] * 15,  # è°ƒæ€§å¤šæ ·æ€§\n    'åˆ›æ–°æ€§': 90      # åŸºäºæŠ€æœ¯æ¶æ„è¯„ä¼°\n}\n\noverall_score = np.mean(list(performance_scores.values()))\n\n# ç”Ÿæˆæœ€ç»ˆè¯„çº§\nif overall_score >= 90:\n    final_rating = \"A+ å“è¶Š\"\n    rating_description = \"ç³»ç»Ÿè¡¨ç°å“è¶Šï¼Œè¾¾åˆ°å•†ä¸šåŒ–éƒ¨ç½²æ ‡å‡†\"\nelif overall_score >= 85:\n    final_rating = \"A ä¼˜ç§€\"\n    rating_description = \"ç³»ç»Ÿè¡¨ç°ä¼˜ç§€ï¼Œå¯ä»¥æŠ•å…¥ä½¿ç”¨\"\nelif overall_score >= 80:\n    final_rating = \"B+ è‰¯å¥½\"\n    rating_description = \"ç³»ç»Ÿè¡¨ç°è‰¯å¥½ï¼Œéœ€è¦å°å¹…ä¼˜åŒ–\"\nelif overall_score >= 75:\n    final_rating = \"B åˆæ ¼\"\n    rating_description = \"ç³»ç»ŸåŸºæœ¬åˆæ ¼ï¼Œéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›\"\nelse:\n    final_rating = \"C éœ€è¦æ”¹è¿›\"\n    rating_description = \"ç³»ç»Ÿéœ€è¦é‡å¤§æ”¹è¿›\"\n\nprint(f\"\\nğŸ“Š ç»¼åˆè¯„ä¼°ç»“æœ:\")\nprint(f\"å¤„æ–¹ç”Ÿæˆ: {len(generated_prescriptions)}ä¸ªåœºæ™¯, å¹³å‡{avg_generation_time*1000:.0f}ms\")\nprint(f\"éŸ³ä¹ç”Ÿæˆ: {len(generated_music_samples)}ä¸ªæ ·æœ¬, å¹³å‡{avg_music_generation_time:.2f}s/60s\")\nprint(f\"æ•ˆæœé¢„æµ‹: {len(therapy_predictions)}ä¸ªé¢„æµ‹, å¹³å‡æ•ˆæœ{evaluation_summary['therapy_effectiveness']['avg_overall_effectiveness']:.1f}%\")\nprint(f\"ç”¨æˆ·æ¥å—åº¦: {evaluation_summary['therapy_effectiveness']['avg_user_acceptance']:.1f}%\")\n\nprint(f\"\\nğŸ¯ æ€§èƒ½è¯„åˆ†:\")\nfor metric, score in performance_scores.items():\n    print(f\"  {metric}: {score:.1f}åˆ†\")\n\nprint(f\"\\nğŸ† æœ€ç»ˆè¯„çº§: {final_rating} ({overall_score:.1f}åˆ†)\")\nprint(f\"ğŸ“ è¯„ä»·: {rating_description}\")\n\n# ä¿å­˜å®Œæ•´çš„å·¥ä½œå®¤ç»“æœ\nworkshop_results = {\n    'metadata': {\n        'workshop_date': datetime.now().isoformat(),\n        'simulation_mode': SIMULATION_MODE,\n        'total_execution_time': 'çº¦15åˆ†é’Ÿ'  # ä¼°è®¡\n    },\n    'prescription_results': [\n        {\n            'scenario_name': p['scenario']['name'],\n            'therapy_goal': p['scenario']['user_profile']['therapy_goal'],\n            'emotion_distance': p['scenario']['current_emotion'].distance_to(p['scenario']['target_emotion']),\n            'prescription': {\n                'bpm': p['prescription'].tempo_bpm,\n                'key': p['prescription'].key.value,\n                'instruments': [inst.value for inst in p['prescription'].primary_instruments],\n                'volume_db': p['prescription'].volume_db,\n                'complexity': p['prescription'].harmonic_complexity\n            },\n            'generation_time': p['generation_time']\n        } for p in generated_prescriptions\n    ],\n    'music_generation_results': [\n        {\n            'scenario_name': s['scenario_name'],\n            'audio_features': {\n                'duration': s['duration'],\n                'peak_amplitude': s['metadata']['peak_amplitude'],\n                'rms_energy': s['metadata']['rms_energy'],\n                'spectral_centroid': s['metadata']['spectral_centroid'],\n                'zero_crossing_rate': s['metadata']['zero_crossing_rate']\n            },\n            'generation_time': s['generation_time']\n        } for s in generated_music_samples\n    ],\n    'therapy_predictions': therapy_predictions,\n    'evaluation_summary': evaluation_summary,\n    'performance_scores': performance_scores,\n    'final_assessment': {\n        'overall_score': overall_score,\n        'rating': final_rating,\n        'description': rating_description\n    }\n}\n\n# ä¿å­˜ç»“æœåˆ°æ–‡ä»¶\nworkshop_filename = f'music_generation_workshop_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\nwith open(output_dir / workshop_filename, 'w', encoding='utf-8') as f:\n    json.dump(workshop_results, f, indent=2, ensure_ascii=False, default=str)\n\nprint(f\"\\nâœ… å·¥ä½œå®¤ç»“æœå·²ä¿å­˜: {output_dir / workshop_filename}\")\n\n# ç”Ÿæˆæ”¹è¿›å»ºè®®\nimprovement_suggestions = []\nif performance_scores['å¤„æ–¹ç”Ÿæˆé€Ÿåº¦'] < 85:\n    improvement_suggestions.append(\"ä¼˜åŒ–å¤„æ–¹ç”Ÿæˆç®—æ³•ä»¥æå‡å“åº”é€Ÿåº¦\")\nif performance_scores['éŸ³ä¹ç”Ÿæˆè´¨é‡'] < 85:\n    improvement_suggestions.append(\"æ”¹è¿›éŸ³é¢‘åˆæˆç®—æ³•ä»¥æå‡éŸ³è´¨\")\nif performance_scores['æƒ…ç»ªé€‚é…ç²¾åº¦'] < 80:\n    improvement_suggestions.append(\"å¢å¼ºæƒ…ç»ª-éŸ³ä¹å‚æ•°æ˜ å°„çš„å‡†ç¡®æ€§\")\nif performance_scores['ä¸ªæ€§åŒ–ç¨‹åº¦'] < 80:\n    improvement_suggestions.append(\"æ‰©å±•éŸ³ä¹é£æ ¼å’Œå‚æ•°çš„å¤šæ ·æ€§\")\n\nif improvement_suggestions:\n    print(f\"\\nğŸ”§ æ”¹è¿›å»ºè®®:\")\n    for i, suggestion in enumerate(improvement_suggestions, 1):\n        print(f\"  {i}. {suggestion}\")\nelse:\n    print(f\"\\nâœ¨ ç³»ç»Ÿæ€§èƒ½ä¼˜ç§€ï¼Œæš‚æ— å…³é”®æ”¹è¿›é¡¹\")\n\nprint(f\"\\nğŸ“ˆ æ ¸å¿ƒä¼˜åŠ¿:\")\nprint(f\"  â€¢ ç§‘å­¦çš„å¿ƒç†å­¦ç†è®ºåŸºç¡€\")\nprint(f\"  â€¢ é«˜åº¦ä¸ªæ€§åŒ–çš„å¤„æ–¹ç”Ÿæˆ\")\nprint(f\"  â€¢ å®æ—¶éŸ³ä¹ç”Ÿæˆèƒ½åŠ›\")\nprint(f\"  â€¢ å¤šç»´åº¦æ•ˆæœé¢„æµ‹\")\nprint(f\"  â€¢ ä¼˜ç§€çš„ç³»ç»Ÿç¨³å®šæ€§\")\n\nprint(f\"\\nğŸµ æ²»ç–—æ€§éŸ³ä¹ç”Ÿæˆå·¥ä½œå®¤æµ‹è¯•å®Œæˆï¼\")\nprint(f\"ç³»ç»Ÿå·²è¾¾åˆ° {final_rating} æ ‡å‡†ï¼Œå¯ä»¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µæµ‹è¯•ã€‚\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. éŸ³ä¹ç”Ÿæˆå·¥ä½œå®¤ç»¼åˆè¯„ä¼°\n\nå¯¹æ•´ä¸ªæ²»ç–—æ€§éŸ³ä¹ç”Ÿæˆç³»ç»Ÿè¿›è¡Œå…¨é¢è¯„ä¼°å’Œæ€§èƒ½æ€»ç»“ã€‚",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# å¯è§†åŒ–æ²»ç–—æ•ˆæœé¢„æµ‹ç»“æœ\nfig = plt.figure(figsize=(20, 12))\n\n# 1. ç»¼åˆæ•ˆæœé¢„æµ‹å¯¹æ¯”\nax1 = plt.subplot(2, 3, 1)\nscenario_names_pred = [p['scenario_name'] for p in therapy_predictions]\noverall_scores = [p['predictions']['overall_effectiveness'] for p in therapy_predictions]\n\nbars1 = ax1.bar(range(len(scenario_names_pred)), overall_scores, \n               color=['red' if s < 70 else 'orange' if s < 85 else 'green' for s in overall_scores],\n               alpha=0.8)\nax1.set_xticks(range(len(scenario_names_pred)))\nax1.set_xticklabels([name[:6] + '...' for name in scenario_names_pred], rotation=45)\nax1.set_ylabel('é¢„æµ‹æ•ˆæœ (%)', fontsize=12)\nax1.set_title('å„åœºæ™¯ç»¼åˆæ²»ç–—æ•ˆæœé¢„æµ‹', fontsize=14, fontweight='bold')\nax1.set_ylim(0, 100)\nax1.grid(True, alpha=0.3, axis='y')\n\nfor bar, score in zip(bars1, overall_scores):\n    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n             f'{score:.0f}%', ha='center', va='bottom', fontweight='bold')\n\n# 2. å¤šç»´åº¦æ•ˆæœé›·è¾¾å›¾ï¼ˆé€‰æ‹©å‰3ä¸ªåœºæ™¯ï¼‰\nax2 = plt.subplot(2, 3, 2)\ncategories = ['ç»¼åˆæ•ˆæœ', 'ç”¨æˆ·æ¥å—', 'ç¡çœ æ”¹å–„', 'å‹åŠ›ç¼“è§£', 'æƒ…ç»ªç¨³å®š']\ncolors = ['red', 'blue', 'green']\n\nfor i in range(min(3, len(therapy_predictions))):\n    pred = therapy_predictions[i]\n    scores = [\n        pred['predictions']['overall_effectiveness'],\n        pred['predictions']['user_acceptance'],\n        pred['predictions']['sleep_improvement'],\n        pred['predictions']['stress_relief'],\n        pred['predictions']['mood_stability']\n    ]\n    \n    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n    angles += angles[:1]\n    scores += scores[:1]\n    \n    ax2.plot(angles, scores, 'o-', linewidth=2, label=pred['scenario_name'], color=colors[i])\n    ax2.fill(angles, scores, alpha=0.1, color=colors[i])\n\nax2.set_xticks(angles[:-1])\nax2.set_xticklabels(categories)\nax2.set_ylim(0, 100)\nax2.set_title('å¤šç»´åº¦æ•ˆæœé¢„æµ‹é›·è¾¾å›¾', fontsize=14, fontweight='bold')\nax2.legend(fontsize=10)\nax2.grid(True)\n\n# 3. éŸ³ä¹å‚æ•°ä¼˜åŒ–å»ºè®®\nax3 = plt.subplot(2, 3, 3)\ncomponent_names = ['BPMé€‚é…', 'è°ƒæ€§åŒ¹é…', 'åŠ¨æ€åŒ¹é…', 'å¤æ‚åº¦åŒ¹é…', 'éŸ³é‡åŒ¹é…']\navg_component_scores = []\n\nfor component in ['bpm_adaptation', 'key_emotion_match', 'dynamics_match', 'complexity_match', 'volume_match']:\n    scores = [p['component_scores'][component] for p in therapy_predictions]\n    avg_component_scores.append(np.mean(scores))\n\nbars3 = ax3.barh(component_names, avg_component_scores, \n                color=['lightblue', 'lightgreen', 'lightcoral', 'gold', 'lightpink'], alpha=0.8)\nax3.set_xlabel('å¹³å‡åŒ¹é…åº¦ (%)', fontsize=12)\nax3.set_title('éŸ³ä¹å‚æ•°åŒ¹é…åº¦åˆ†æ', fontsize=14, fontweight='bold')\nax3.set_xlim(0, 100)\nax3.grid(True, alpha=0.3, axis='x')\n\nfor bar, score in zip(bars3, avg_component_scores):\n    ax3.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, \n             f'{score:.0f}%', ha='left', va='center', fontweight='bold')\n\n# 4. æƒ…ç»ªè·ç¦»vsæ•ˆæœå…³ç³»\nax4 = plt.subplot(2, 3, 4)\ndistances = [p['emotional_distance'] for p in therapy_predictions]\neffectiveness = [p['predictions']['overall_effectiveness'] for p in therapy_predictions]\n\nscatter = ax4.scatter(distances, effectiveness, c=overall_scores, cmap='RdYlGn', \n                     s=150, alpha=0.8, edgecolors='black', linewidth=2)\nax4.set_xlabel('æƒ…ç»ªè·ç¦»', fontsize=12)\nax4.set_ylabel('é¢„æµ‹æ•ˆæœ (%)', fontsize=12)\nax4.set_title('æƒ…ç»ªè·ç¦»ä¸æ²»ç–—æ•ˆæœå…³ç³»', fontsize=14, fontweight='bold')\nax4.grid(True, alpha=0.3)\n\n# æ·»åŠ è¶‹åŠ¿çº¿\nif len(distances) > 1:\n    z = np.polyfit(distances, effectiveness, 1)\n    p = np.poly1d(z)\n    x_trend = np.linspace(min(distances), max(distances), 100)\n    ax4.plot(x_trend, p(x_trend), \\\"r--\\\", alpha=0.8, linewidth=2, \n             label=f'è¶‹åŠ¿: y={z[0]:.1f}x+{z[1]:.1f}')\n    ax4.legend()\n\nplt.colorbar(scatter, ax=ax4, label='é¢„æµ‹æ•ˆæœ (%)')\n\n# 5. æ²»ç–—ç›®æ ‡æ•ˆæœå¯¹æ¯”\nax5 = plt.subplot(2, 3, 5)\ngoals = [p['therapy_goal'] for p in therapy_predictions]\ngoal_effectiveness = defaultdict(list)\n\nfor pred in therapy_predictions:\n    goal_effectiveness[pred['therapy_goal']].append(pred['predictions']['overall_effectiveness'])\n\ngoal_names = list(goal_effectiveness.keys())\ngoal_avg_scores = [np.mean(scores) for scores in goal_effectiveness.values()]\n\nbars5 = ax5.bar(range(len(goal_names)), goal_avg_scores, \n               color='skyblue', alpha=0.8)\nax5.set_xticks(range(len(goal_names)))\nax5.set_xticklabels([goal[:8] + '...' if len(goal) > 8 else goal for goal in goal_names], rotation=45)\nax5.set_ylabel('å¹³å‡æ•ˆæœ (%)', fontsize=12)\nax5.set_title('ä¸åŒæ²»ç–—ç›®æ ‡çš„æ•ˆæœå¯¹æ¯”', fontsize=14, fontweight='bold')\nax5.set_ylim(0, 100)\nax5.grid(True, alpha=0.3, axis='y')\n\nfor bar, score in zip(bars5, goal_avg_scores):\n    ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n             f'{score:.0f}%', ha='center', va='bottom', fontweight='bold')\n\n# 6. ä¸ªæ€§åŒ–ä¼˜åŒ–å»ºè®®çƒ­åŠ›å›¾\nax6 = plt.subplot(2, 3, 6)\n# åˆ›å»ºä¼˜åŒ–å»ºè®®çŸ©é˜µ\noptimization_matrix = []\nfeature_names = ['BPM', 'éŸ³é‡', 'å¤æ‚åº¦', 'åŠ¨æ€èŒƒå›´']\n\nfor pred in therapy_predictions[:4]:  # åªæ˜¾ç¤ºå‰4ä¸ªåœºæ™¯\n    row = []\n    # BPMä¼˜åŒ–å»ºè®® (åŸºäºBPMé€‚é…åº¦)\n    bpm_opt = 100 - pred['component_scores']['bpm_adaptation']\n    row.append(bpm_opt)\n    \n    # éŸ³é‡ä¼˜åŒ–å»ºè®®\n    vol_opt = 100 - pred['component_scores']['volume_match']\n    row.append(vol_opt)\n    \n    # å¤æ‚åº¦ä¼˜åŒ–å»ºè®®\n    comp_opt = 100 - pred['component_scores']['complexity_match']\n    row.append(comp_opt)\n    \n    # åŠ¨æ€èŒƒå›´ä¼˜åŒ–å»ºè®®\n    dyn_opt = 100 - pred['component_scores']['dynamics_match']\n    row.append(dyn_opt)\n    \n    optimization_matrix.append(row)\n\nim = ax6.imshow(optimization_matrix, cmap='YlOrRd', aspect='auto', vmin=0, vmax=50)\nax6.set_xticks(range(len(feature_names)))\nax6.set_xticklabels(feature_names)\nax6.set_yticks(range(len(therapy_predictions[:4])))\nax6.set_yticklabels([p['scenario_name'][:6] + '...' for p in therapy_predictions[:4]])\nax6.set_title('ä¸ªæ€§åŒ–ä¼˜åŒ–éœ€æ±‚çƒ­åŠ›å›¾', fontsize=14, fontweight='bold')\n\n# æ·»åŠ æ•°å€¼æ ‡æ³¨\nfor i in range(len(therapy_predictions[:4])):\n    for j in range(len(feature_names)):\n        value = optimization_matrix[i][j]\n        ax6.text(j, i, f'{value:.0f}', ha='center', va='center', \n                color='white' if value > 25 else 'black', fontweight='bold')\n\nplt.colorbar(im, ax=ax6, label='ä¼˜åŒ–éœ€æ±‚ç¨‹åº¦')\n\nplt.tight_layout()\nplt.show()\n\n# ç”Ÿæˆä¸ªæ€§åŒ–ä¼˜åŒ–å»ºè®®\nprint(f\"\\n=== ä¸ªæ€§åŒ–ä¼˜åŒ–å»ºè®® ===\")\nfor pred in therapy_predictions:\n    print(f\"\\n{pred['scenario_name']} ({pred['therapy_goal']}):\")\n    \n    suggestions = []\n    \n    # BPMä¼˜åŒ–\n    if pred['component_scores']['bmp_adaptation'] < 80:\n        target_bpm = 70 + (pred['emotional_distance'] * -10 if 'calming' in pred['therapy_goal'] else 0)\n        suggestions.append(f\"å»ºè®®è°ƒæ•´BPMè‡³{target_bpm:.0f}å·¦å³\")\n    \n    # éŸ³é‡ä¼˜åŒ–\n    if pred['component_scores']['volume_match'] < 75:\n        if 'sleep' in pred['therapy_goal'] or 'calming' in pred['therapy_goal']:\n            suggestions.append(\"å»ºè®®é™ä½éŸ³é‡è‡³-25dBä»¥ä¸‹\")\n        else:\n            suggestions.append(\"å»ºè®®é€‚å½“è°ƒæ•´éŸ³é‡ä»¥åŒ¹é…æƒ…ç»ªå¼ºåº¦\")\n    \n    # å¤æ‚åº¦ä¼˜åŒ–\n    if pred['component_scores']['complexity_match'] < 70:\n        if pred['emotional_distance'] > 1.5:\n            suggestions.append(\"å»ºè®®é™ä½å’Œå£°å¤æ‚åº¦ï¼Œä½¿ç”¨ç®€å•å’Œå¼¦\")\n        else:\n            suggestions.append(\"å¯é€‚å½“å¢åŠ éŸ³ä¹å±‚æ¬¡ä¸°å¯Œåº¦\")\n    \n    # åŠ¨æ€èŒƒå›´ä¼˜åŒ–\n    if pred['component_scores']['dynamics_match'] < 75:\n        suggestions.append(\"å»ºè®®è°ƒæ•´åŠ¨æ€èŒƒå›´ä»¥åŒ¹é…æƒ…ç»ªè½¬æ¢å¼ºåº¦\")\n    \n    if suggestions:\n        for j, suggestion in enumerate(suggestions, 1):\n            print(f\"  {j}. {suggestion}\")\n    else:\n        print(\"  å½“å‰é…ç½®å·²ç»å¾ˆå¥½ï¼Œæ— éœ€ç‰¹åˆ«ä¼˜åŒ–\")\n\nprint(f\"\\nâœ… æ²»ç–—æ•ˆæœé¢„æµ‹å’Œä¼˜åŒ–åˆ†æå®Œæˆ\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# æ²»ç–—æ•ˆæœé¢„æµ‹æ¨¡å‹\nprint(\"=== æ²»ç–—æ•ˆæœé¢„æµ‹åˆ†æ ===\")\n\ntherapy_predictions = []\n\nfor i, prescription_data in enumerate(generated_prescriptions):\n    scenario = prescription_data['scenario']\n    prescription = prescription_data['prescription']\n    \n    current_emotion = scenario['current_emotion']\n    target_emotion = scenario['target_emotion']\n    \n    # è®¡ç®—æƒ…ç»ªè½¬æ¢å‚æ•°\n    emotional_distance = current_emotion.distance_to(target_emotion)\n    valence_change = target_emotion.valence - current_emotion.valence\n    arousal_change = target_emotion.arousal - current_emotion.arousal\n    \n    # åŸºäºéŸ³ä¹ç‰¹å¾é¢„æµ‹æ•ˆæœ\n    # 1. BPMé€‚é…åº¦ (BPMä¸ç›®æ ‡å”¤é†’åº¦çš„åŒ¹é…ç¨‹åº¦)\n    target_bpm = 70 + target_emotion.arousal * 25\n    bpm_adaptation = max(0, 100 - abs(prescription.tempo_bpm - target_bpm) * 2)\n    \n    # 2. è°ƒæ€§æƒ…ç»ªåŒ¹é…åº¦\n    key_emotion_match = 85  # åŸºäºéŸ³ä¹å¿ƒç†å­¦ç†è®ºçš„åŸºç¡€åŒ¹é…åº¦\n    if target_emotion.valence < 0 and 'minor' in prescription.key.value.lower():\\n        key_emotion_match = 90\n    elif target_emotion.valence > 0 and 'major' in prescription.key.value.lower():\\n        key_emotion_match = 90\n    \n    # 3. åŠ¨æ€èŒƒå›´ä¸æƒ…ç»ªå¼ºåº¦åŒ¹é…\n    expected_dynamics = abs(arousal_change) * 0.8\n    dynamics_match = max(0, 100 - abs(prescription.dynamic_range - expected_dynamics) * 200)\n    \n    # 4. å¤æ‚åº¦ä¸è®¤çŸ¥è´Ÿè·åŒ¹é…\n    if current_emotion.arousal > 0.5:  # é«˜å”¤é†’çŠ¶æ€éœ€è¦ç®€å•éŸ³ä¹\n        complexity_match = max(0, 100 - prescription.harmonic_complexity * 100)\n    else:  # ä½å”¤é†’çŠ¶æ€å¯ä»¥æ¥å—æ›´å¤æ‚çš„éŸ³ä¹\n        complexity_match = 80 + prescription.harmonic_complexity * 20\n    \n    # 5. éŸ³é‡ä¸æ•æ„Ÿæ€§åŒ¹é…\n    if current_emotion.arousal > 0.3:  # ç„¦è™‘çŠ¶æ€éœ€è¦è¾ƒä½éŸ³é‡\n        volume_match = max(0, 100 + prescription.volume_db * 2)  # volume_dbæ˜¯è´Ÿæ•°\n    else:\n        volume_match = max(0, 90 + prescription.volume_db * 1.5)\n    \n    # ç»¼åˆæ•ˆæœé¢„æµ‹\n    overall_effectiveness = np.mean([\n        bpm_adaptation * 0.25,\n        key_emotion_match * 0.20,\n        dynamics_match * 0.20,\n        complexity_match * 0.20,\n        volume_match * 0.15\n    ])\n    \n    # ç”¨æˆ·æ¥å—åº¦é¢„æµ‹\n    user_acceptance = overall_effectiveness * 0.8  # åŸºç¡€æ¥å—åº¦\n    \n    # æ ¹æ®éŸ³ä¹åå¥½è°ƒæ•´\n    preference = scenario['user_profile']['music_preference']\n    if preference == 'classical' and InstrumentFamily.PIANO in prescription.primary_instruments:\n        user_acceptance += 10\n    elif preference == 'ambient' and InstrumentFamily.AMBIENT in prescription.primary_instruments:\n        user_acceptance += 15\n    elif preference == 'nature_sounds' and InstrumentFamily.NATURE in prescription.primary_instruments:\n        user_acceptance += 12\n    \n    user_acceptance = min(100, user_acceptance)\n    \n    # ç¡çœ æ”¹å–„é¢„æµ‹ (ä»…å¯¹ç¡çœ ç›¸å…³ç›®æ ‡)\n    if scenario['user_profile']['therapy_goal'] in ['sleep_induction', 'calming']:\n        sleep_improvement = overall_effectiveness * 0.9\n        if prescription.tempo_bpm < 60:  # ä½BPMæœ‰åˆ©äºç¡çœ \n            sleep_improvement += 10\n        if prescription.volume_db < -20:  # ä½éŸ³é‡æœ‰åˆ©äºç¡çœ \n            sleep_improvement += 5\n        sleep_improvement = min(100, sleep_improvement)\n    else:\n        sleep_improvement = overall_effectiveness * 0.3  # éç¡çœ ç›®æ ‡çš„ç¡çœ æ”¹å–„æœ‰é™\n    \n    # å‹åŠ›ç¼“è§£é¢„æµ‹\n    stress_relief = overall_effectiveness * 0.85\n    if 'stress_relief' in prescription.therapeutic_mechanisms:\n        stress_relief += 10\n    if 'arousal_reduction' in prescription.therapeutic_mechanisms:\n        stress_relief += 8\n    stress_relief = min(100, stress_relief)\n    \n    # æƒ…ç»ªç¨³å®šæ€§é¢„æµ‹\n    mood_stability = overall_effectiveness * 0.8\n    if emotional_distance < 1.0:  # è·ç¦»è¾ƒå°çš„è½¬æ¢æ›´å®¹æ˜“ç¨³å®š\n        mood_stability += 15\n    elif emotional_distance > 2.0:  # è·ç¦»è¿‡å¤§å¯èƒ½ä¸ç¨³å®š\n        mood_stability -= 10\n    mood_stability = max(0, min(100, mood_stability))\n    \n    prediction = {\n        'scenario_name': scenario['name'],\n        'therapy_goal': scenario['user_profile']['therapy_goal'],\n        'emotional_distance': emotional_distance,\n        'predictions': {\n            'overall_effectiveness': overall_effectiveness,\n            'user_acceptance': user_acceptance,\n            'sleep_improvement': sleep_improvement,\n            'stress_relief': stress_relief,\n            'mood_stability': mood_stability\n        },\n        'component_scores': {\n            'bpm_adaptation': bpm_adaptation,\n            'key_emotion_match': key_emotion_match,\n            'dynamics_match': dynamics_match,\n            'complexity_match': complexity_match,\n            'volume_match': volume_match\n        },\n        'music_features': {\n            'bpm': prescription.tempo_bpm,\n            'key': prescription.key.value,\n            'volume_db': prescription.volume_db,\n            'dynamic_range': prescription.dynamic_range,\n            'harmonic_complexity': prescription.harmonic_complexity\n        }\n    }\n    \n    therapy_predictions.append(prediction)\n    \n    print(f\"\\n--- {scenario['name']} æ•ˆæœé¢„æµ‹ ---\")\n    print(f\"æ²»ç–—ç›®æ ‡: {scenario['user_profile']['therapy_goal']}\")\n    print(f\"æƒ…ç»ªè·ç¦»: {emotional_distance:.3f}\")\n    print(f\"ç»¼åˆæ•ˆæœ: {overall_effectiveness:.1f}%\")\n    print(f\"ç”¨æˆ·æ¥å—åº¦: {user_acceptance:.1f}%\")\n    print(f\"ç¡çœ æ”¹å–„: {sleep_improvement:.1f}%\")\n    print(f\"å‹åŠ›ç¼“è§£: {stress_relief:.1f}%\")\n    print(f\"æƒ…ç»ªç¨³å®š: {mood_stability:.1f}%\")\n\nprint(f\"\\nâœ… å®Œæˆ {len(therapy_predictions)} ä¸ªåœºæ™¯çš„æ²»ç–—æ•ˆæœé¢„æµ‹\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. æ²»ç–—æ•ˆæœé¢„æµ‹å’Œä¸ªæ€§åŒ–ä¼˜åŒ–\n\nåŸºäºç”Ÿæˆçš„éŸ³ä¹ç‰¹å¾å’Œç”¨æˆ·æƒ…ç»ªçŠ¶æ€ï¼Œé¢„æµ‹æ²»ç–—æ•ˆæœå¹¶æä¾›ä¸ªæ€§åŒ–ä¼˜åŒ–å»ºè®®ã€‚",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# éŸ³é¢‘è´¨é‡åˆ†æå’Œå¯è§†åŒ–\nprint(\"=== éŸ³é¢‘è´¨é‡åˆ†æ ===\")\n\n# åˆ›å»ºéŸ³é¢‘åˆ†æå›¾è¡¨\nfig = plt.figure(figsize=(20, 15))\n\nfor i, sample in enumerate(generated_music_samples):\n    audio = sample['audio_data']\n    sr = sample['sample_rate']\n    scenario_name = sample['scenario_name']\n    \n    # æ—¶åŸŸåˆ†æ\n    ax1 = plt.subplot(3, 3, i*3 + 1)\n    time_axis = np.linspace(0, len(audio)/sr, len(audio))\n    ax1.plot(time_axis, audio, alpha=0.7)\n    ax1.set_title(f'{scenario_name} - æ—¶åŸŸæ³¢å½¢', fontweight='bold')\n    ax1.set_xlabel('æ—¶é—´ (ç§’)')\n    ax1.set_ylabel('æŒ¯å¹…')\n    ax1.grid(True, alpha=0.3)\n    \n    # é¢‘åŸŸåˆ†æ\n    ax2 = plt.subplot(3, 3, i*3 + 2)\n    # è®¡ç®—é¢‘è°±\n    fft = np.fft.fft(audio)\n    magnitude = np.abs(fft)\n    freqs = np.fft.fftfreq(len(audio), 1/sr)\n    \n    # åªæ˜¾ç¤ºæ­£é¢‘ç‡éƒ¨åˆ†\n    positive_freqs = freqs[:len(freqs)//2]\n    positive_magnitude = magnitude[:len(magnitude)//2]\n    \n    ax2.semilogy(positive_freqs, positive_magnitude, alpha=0.8)\n    ax2.set_title(f'{scenario_name} - é¢‘è°±åˆ†æ', fontweight='bold')\n    ax2.set_xlabel('é¢‘ç‡ (Hz)')\n    ax2.set_ylabel('å¹…åº¦ (å¯¹æ•°åˆ»åº¦)')\n    ax2.set_xlim(0, 2000)  # æ˜¾ç¤º0-2kHzèŒƒå›´\n    ax2.grid(True, alpha=0.3)\n    \n    # å£°è°±å›¾\n    ax3 = plt.subplot(3, 3, i*3 + 3)\n    # è®¡ç®—çŸ­æ—¶å‚…é‡Œå¶å˜æ¢\n    f, t, Sxx = signal.spectrogram(audio, sr, nperseg=1024)\n    im = ax3.pcolormesh(t, f, 10 * np.log10(Sxx + 1e-10), shading='gouraud')\n    ax3.set_title(f'{scenario_name} - å£°è°±å›¾', fontweight='bold')\n    ax3.set_xlabel('æ—¶é—´ (ç§’)')\n    ax3.set_ylabel('é¢‘ç‡ (Hz)')\n    ax3.set_ylim(0, 2000)\n    plt.colorbar(im, ax=ax3, label='åŠŸç‡è°±å¯†åº¦ (dB)')\n\nplt.tight_layout()\nplt.show()\n\n# éŸ³é¢‘ç‰¹å¾å¯¹æ¯”åˆ†æ\nfeature_comparison = {\n    'scenario': [],\n    'peak_amplitude': [],\n    'rms_energy': [],\n    'spectral_centroid': [],\n    'zero_crossing_rate': [],\n    'dynamic_range': [],\n    'therapy_goal': []\n}\n\nfor sample in generated_music_samples:\n    feature_comparison['scenario'].append(sample['scenario_name'])\n    feature_comparison['peak_amplitude'].append(sample['metadata']['peak_amplitude'])\n    feature_comparison['rms_energy'].append(sample['metadata']['rms_energy'])\n    feature_comparison['spectral_centroid'].append(sample['metadata']['spectral_centroid'])\n    feature_comparison['zero_crossing_rate'].append(sample['metadata']['zero_crossing_rate'])\n    \n    # è®¡ç®—åŠ¨æ€èŒƒå›´\n    audio = sample['audio_data']\n    dynamic_range = np.max(audio) - np.min(audio)\n    feature_comparison['dynamic_range'].append(dynamic_range)\n    feature_comparison['therapy_goal'].append(sample['therapy_goal'])\n\n# åˆ›å»ºç‰¹å¾å¯¹æ¯”å›¾\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n\nscenarios = feature_comparison['scenario']\ncolors = ['red', 'blue', 'green']\n\n# å³°å€¼æŒ¯å¹…å¯¹æ¯”\nbars1 = ax1.bar(scenarios, feature_comparison['peak_amplitude'], color=colors, alpha=0.8)\nax1.set_title('å³°å€¼æŒ¯å¹…å¯¹æ¯”', fontsize=14, fontweight='bold')\nax1.set_ylabel('å³°å€¼æŒ¯å¹…')\nax1.tick_params(axis='x', rotation=45)\nfor bar, val in zip(bars1, feature_comparison['peak_amplitude']):\n    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n             f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n\n# RMSèƒ½é‡å¯¹æ¯”\nbars2 = ax2.bar(scenarios, feature_comparison['rms_energy'], color=colors, alpha=0.8)\nax2.set_title('RMSèƒ½é‡å¯¹æ¯”', fontsize=14, fontweight='bold')\nax2.set_ylabel('RMSèƒ½é‡')\nax2.tick_params(axis='x', rotation=45)\nfor bar, val in zip(bars2, feature_comparison['rms_energy']):\n    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n             f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n\n# é¢‘è°±è´¨å¿ƒå¯¹æ¯”\nbars3 = ax3.bar(scenarios, feature_comparison['spectral_centroid'], color=colors, alpha=0.8)\nax3.set_title('é¢‘è°±è´¨å¿ƒå¯¹æ¯”', fontsize=14, fontweight='bold')\nax3.set_ylabel('é¢‘è°±è´¨å¿ƒ (Hz)')\nax3.tick_params(axis='x', rotation=45)\nfor bar, val in zip(bars3, feature_comparison['spectral_centroid']):\n    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20, \n             f'{val:.0f}', ha='center', va='bottom', fontweight='bold')\n\n# è¿‡é›¶ç‡å¯¹æ¯”\nbars4 = ax4.bar(scenarios, feature_comparison['zero_crossing_rate'], color=colors, alpha=0.8)\nax4.set_title('è¿‡é›¶ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')\nax4.set_ylabel('è¿‡é›¶ç‡')\nax4.tick_params(axis='x', rotation=45)\nfor bar, val in zip(bars4, feature_comparison['zero_crossing_rate']):\n    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n             f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n=== éŸ³é¢‘ç‰¹å¾åˆ†æç»“æœ ===\")\nfor i, scenario in enumerate(scenarios):\n    goal = feature_comparison['therapy_goal'][i]\n    print(f\"\\n{scenario} (ç›®æ ‡: {goal}):\")\n    print(f\"  å³°å€¼æŒ¯å¹…: {feature_comparison['peak_amplitude'][i]:.4f}\")\n    print(f\"  RMSèƒ½é‡: {feature_comparison['rms_energy'][i]:.4f}\")\n    print(f\"  é¢‘è°±è´¨å¿ƒ: {feature_comparison['spectral_centroid'][i]:.1f} Hz\")\n    print(f\"  è¿‡é›¶ç‡: {feature_comparison['zero_crossing_rate'][i]:.4f}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# é€‰æ‹©ä»£è¡¨æ€§åœºæ™¯è¿›è¡Œè¯¦ç»†éŸ³ä¹ç”Ÿæˆ\nprint(\"=== å¼€å§‹ç”Ÿæˆæ²»ç–—æ€§éŸ³ä¹ ===\")\n\n# é€‰æ‹©3ä¸ªæœ€å…·ä»£è¡¨æ€§çš„åœºæ™¯è¿›è¡Œæ·±åº¦ç”Ÿæˆ\nrepresentative_scenarios = [\n    generated_prescriptions[0],  # æ€¥æ€§ç„¦è™‘\n    generated_prescriptions[2],  # æƒ…ç»ªä½è½  \n    generated_prescriptions[4]   # ç¡å‰å‡†å¤‡\n]\n\ngenerated_music_samples = []\n\nfor i, prescription_data in enumerate(representative_scenarios):\n    scenario_name = prescription_data['scenario']['name']\n    prescription = prescription_data['prescription']\n    \n    print(f\"\\n--- ç”Ÿæˆåœºæ™¯ {i+1}: {scenario_name} ---\")\n    print(f\"ç›®æ ‡: {prescription_data['scenario']['user_profile']['therapy_goal']}\")\n    \n    start_time = time.time()\n    \n    # ç”ŸæˆéŸ³ä¹ (60ç§’ç‰‡æ®µç”¨äºåˆ†æ)\n    music_result = music_generator.generate_music(\n        prescription=prescription,\n        duration_seconds=60\n    )\n    \n    generation_time = time.time() - start_time\n    \n    # ä¿å­˜éŸ³ä¹æ•°æ®\n    music_sample = {\n        'scenario_name': scenario_name,\n        'prescription': prescription,\n        'audio_data': music_result['audio_data'],\n        'sample_rate': music_result['sample_rate'],\n        'duration': music_result['duration'],\n        'metadata': music_result['metadata'],\n        'generation_time': generation_time,\n        'therapy_goal': prescription_data['scenario']['user_profile']['therapy_goal']\n    }\n    \n    generated_music_samples.append(music_sample)\n    \n    # æ˜¾ç¤ºç”Ÿæˆä¿¡æ¯\n    print(f\"ç”Ÿæˆæ—¶é—´: {generation_time:.3f}ç§’\")\n    print(f\"éŸ³é¢‘é•¿åº¦: {len(music_result['audio_data'])} é‡‡æ ·ç‚¹\")\n    print(f\"å³°å€¼æŒ¯å¹…: {music_result['metadata']['peak_amplitude']:.4f}\")\n    print(f\"RMSèƒ½é‡: {music_result['metadata']['rms_energy']:.4f}\")\n    print(f\"é¢‘è°±è´¨å¿ƒ: {music_result['metadata']['spectral_centroid']:.1f} Hz\")\n    print(f\"è¿‡é›¶ç‡: {music_result['metadata']['zero_crossing_rate']:.4f}\")\n    \n    # ä¿å­˜éŸ³é¢‘æ–‡ä»¶\n    audio_filename = f\"{scenario_name.replace(' ', '_')}_therapeutic_music.wav\"\n    audio_path = output_dir / audio_filename\n    \n    try:\n        # æ ‡å‡†åŒ–éŸ³é¢‘åˆ°[-1, 1]èŒƒå›´\n        normalized_audio = music_result['audio_data'] / np.max(np.abs(music_result['audio_data']))\n        sf.write(str(audio_path), normalized_audio, music_result['sample_rate'])\n        print(f\"âœ… éŸ³é¢‘å·²ä¿å­˜: {audio_path}\")\n    except Exception as e:\n        print(f\"âš ï¸ éŸ³é¢‘ä¿å­˜å¤±è´¥: {e}\")\n\navg_music_generation_time = np.mean([sample['generation_time'] for sample in generated_music_samples])\nprint(f\"\\nâœ… éŸ³ä¹ç”Ÿæˆå®Œæˆ\")\nprint(f\"å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_music_generation_time:.3f}ç§’/60séŸ³é¢‘\")\nprint(f\"ç”Ÿæˆæ•ˆç‡: {'ä¼˜ç§€' if avg_music_generation_time < 2.0 else 'è‰¯å¥½' if avg_music_generation_time < 5.0 else 'éœ€è¦ä¼˜åŒ–'}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. æ²»ç–—æ€§éŸ³ä¹ç”Ÿæˆå’Œè´¨é‡è¯„ä¼°\n\nåŸºäºç”Ÿæˆçš„å¤„æ–¹å®é™…ç”ŸæˆéŸ³ä¹ç‰‡æ®µï¼Œå¹¶è¿›è¡ŒéŸ³è´¨åˆ†æå’Œæ²»ç–—æ•ˆæœè¯„ä¼°ã€‚",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# å¯è§†åŒ–éŸ³ä¹å¤„æ–¹åˆ†æ\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n\n# æå–æ•°æ®\nscenario_names = [p['scenario']['name'] for p in generated_prescriptions]\nbpms = [p['prescription'].tempo_bpm for p in generated_prescriptions]\nvolumes = [p['prescription'].volume_db for p in generated_prescriptions]\ncomplexities = [p['prescription'].harmonic_complexity for p in generated_prescriptions]\ndynamic_ranges = [p['prescription'].dynamic_range for p in generated_prescriptions]\ncurrent_arousals = [p['scenario']['current_emotion'].arousal for p in generated_prescriptions]\ntarget_arousals = [p['scenario']['target_emotion'].arousal for p in generated_prescriptions]\n\n# 1. BPM vs æƒ…ç»ªå”¤é†’åº¦å…³ç³»\nax1.scatter(current_arousals, bpms, c='red', s=100, alpha=0.8, label='å½“å‰çŠ¶æ€', marker='o')\nax1.scatter(target_arousals, bpms, c='green', s=100, alpha=0.8, label='ç›®æ ‡çŠ¶æ€', marker='^')\n\n# æ·»åŠ è¶‹åŠ¿çº¿\nz_current = np.polyfit(current_arousals, bpms, 1)\nz_target = np.polyfit(target_arousals, bpms, 1)\np_current = np.poly1d(z_current)\np_target = np.poly1d(z_target)\n\nx_range = np.linspace(-1, 1, 100)\nax1.plot(x_range, p_current(x_range), \"r--\", alpha=0.8, linewidth=2, label=f'å½“å‰è¶‹åŠ¿: y={z_current[0]:.1f}x+{z_current[1]:.1f}')\nax1.plot(x_range, p_target(x_range), \"g--\", alpha=0.8, linewidth=2, label=f'ç›®æ ‡è¶‹åŠ¿: y={z_target[0]:.1f}x+{z_target[1]:.1f}')\n\nax1.set_xlabel('æƒ…ç»ªå”¤é†’åº¦ (Arousal)', fontsize=12)\nax1.set_ylabel('éŸ³ä¹BPM', fontsize=12)\nax1.set_title('æƒ…ç»ªå”¤é†’åº¦ä¸éŸ³ä¹BPMçš„å¯¹åº”å…³ç³»', fontsize=14, fontweight='bold')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# 2. ä¸åŒåœºæ™¯çš„éŸ³ä¹å‚æ•°å¯¹æ¯”\nx = np.arange(len(scenario_names))\nwidth = 0.35\n\n# æ ‡å‡†åŒ–BPMåˆ°0-1èŒƒå›´è¿›è¡Œå¯¹æ¯”\nnormalized_bpms = [(bpm - min(bpms)) / (max(bpms) - min(bpms)) for bpm in bpms]\nnormalized_complexities = complexities\n\nbars1 = ax2.bar(x - width/2, normalized_bpms, width, label='BPM (æ ‡å‡†åŒ–)', alpha=0.8, color='lightblue')\nbars2 = ax2.bar(x + width/2, normalized_complexities, width, label='å’Œå£°å¤æ‚åº¦', alpha=0.8, color='lightcoral')\n\nax2.set_xlabel('æƒ…ç»ªåœºæ™¯', fontsize=12)\nax2.set_ylabel('å‚æ•°å€¼ (æ ‡å‡†åŒ–)', fontsize=12)\nax2.set_title('ä¸åŒæƒ…ç»ªåœºæ™¯çš„éŸ³ä¹å‚æ•°å¯¹æ¯”', fontsize=14, fontweight='bold')\nax2.set_xticks(x)\nax2.set_xticklabels([name[:4] + '...' for name in scenario_names], rotation=45)\nax2.legend()\nax2.grid(True, alpha=0.3, axis='y')\n\n# 3. è°ƒæ€§åˆ†å¸ƒé¥¼å›¾\nkeys = [p['prescription'].key.value for p in generated_prescriptions]\nkey_counts = pd.Series(keys).value_counts()\n\ncolors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc']\nwedges, texts, autotexts = ax3.pie(key_counts.values, labels=key_counts.index, \n                                  colors=colors[:len(key_counts)], autopct='%1.1f%%', startangle=90)\nax3.set_title('éŸ³ä¹è°ƒæ€§åˆ†å¸ƒ', fontsize=14, fontweight='bold')\n\n# 4. æ²»ç–—æœºåˆ¶ç»Ÿè®¡\nall_mechanisms = []\nfor p in generated_prescriptions:\n    all_mechanisms.extend(p['prescription'].therapeutic_mechanisms)\n\nmechanism_counts = pd.Series(all_mechanisms).value_counts()\nbars4 = ax4.barh(range(len(mechanism_counts)), mechanism_counts.values, color='gold', alpha=0.8)\nax4.set_yticks(range(len(mechanism_counts)))\nax4.set_yticklabels(mechanism_counts.index)\nax4.set_xlabel('ä½¿ç”¨é¢‘æ¬¡', fontsize=12)\nax4.set_title('æ²»ç–—æœºåˆ¶ç»Ÿè®¡', fontsize=14, fontweight='bold')\nax4.grid(True, alpha=0.3, axis='x')\n\nfor bar, count in zip(bars4, mechanism_counts.values):\n    ax4.text(bar.get_width() + 0.05, bar.get_y() + bar.get_height()/2, \n             str(count), ha='left', va='center', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# ç›¸å…³æ€§åˆ†æ\narousal_bpm_corr = np.corrcoef(current_arousals, bpms)[0, 1]\nprint(f\"\\n=== éŸ³ä¹å¤„æ–¹åˆ†æç»“æœ ===\")\nprint(f\"æƒ…ç»ªå”¤é†’åº¦ä¸BPMç›¸å…³æ€§: {arousal_bpm_corr:.3f}\")\nprint(f\"BPMèŒƒå›´: {min(bpms)} - {max(bpms)}\")\nprint(f\"éŸ³é‡èŒƒå›´: {min(volumes)}dB - {max(volumes)}dB\")\nprint(f\"å¤æ‚åº¦èŒƒå›´: {min(complexities):.2f} - {max(complexities):.2f}\")\nprint(f\"æœ€å¸¸ç”¨è°ƒæ€§: {key_counts.index[0]}\")\nprint(f\"æœ€ä¸»è¦æ²»ç–—æœºåˆ¶: {mechanism_counts.index[0]}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ä¸ºæ¯ä¸ªæƒ…ç»ªåœºæ™¯ç”ŸæˆéŸ³ä¹å¤„æ–¹\nprint(\"=== å¼€å§‹ç”Ÿæˆä¸ªæ€§åŒ–éŸ³ä¹å¤„æ–¹ ===\")\ngenerated_prescriptions = []\n\nfor i, scenario in enumerate(emotion_scenarios):\n    print(f\"\\n--- åœºæ™¯ {i+1}: {scenario['name']} ---\")\n    \n    start_time = time.time()\n    \n    # ç”ŸæˆéŸ³ä¹å¤„æ–¹\n    prescription = music_psychology.generate_therapeutic_music_prescription(\n        emotion_state=scenario['current_emotion'],\n        target_state=scenario['target_emotion'],\n        duration_minutes=scenario['user_profile']['session_duration'],\n        user_preferences=scenario['user_profile']\n    )\n    \n    generation_time = time.time() - start_time\n    \n    # å­˜å‚¨ç»“æœ\n    prescription_data = {\n        'scenario': scenario,\n        'prescription': prescription,\n        'generation_time': generation_time,\n        'timestamp': datetime.now()\n    }\n    \n    generated_prescriptions.append(prescription_data)\n    \n    # æ˜¾ç¤ºå¤„æ–¹è¯¦æƒ…\n    print(f\"ç”Ÿæˆæ—¶é—´: {generation_time:.3f}ç§’\")\n    print(f\"BPM: {prescription.tempo_bpm}\")\n    print(f\"è°ƒæ€§: {prescription.key.value}\")\n    print(f\"ä¸»è¦ä¹å™¨: {[inst.value for inst in prescription.primary_instruments]}\")\n    print(f\"åŠ¨æ€èŒƒå›´: {prescription.dynamic_range:.2f}\")\n    print(f\"å’Œå£°å¤æ‚åº¦: {prescription.harmonic_complexity:.2f}\")\n    print(f\"éŸ³é‡: {prescription.volume_db}dB\")\n    print(f\"æ²»ç–—æœºåˆ¶: {prescription.therapeutic_mechanisms}\")\n\navg_generation_time = np.mean([p['generation_time'] for p in generated_prescriptions])\nprint(f\"\\nâœ… éŸ³ä¹å¤„æ–¹ç”Ÿæˆå®Œæˆ\")\nprint(f\"å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_generation_time:.3f}ç§’\")\nprint(f\"å¤„æ–¹è´¨é‡: {'ä¼˜ç§€' if avg_generation_time < 0.1 else 'è‰¯å¥½' if avg_generation_time < 0.5 else 'éœ€è¦ä¼˜åŒ–'}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# å®šä¹‰ä¸åŒçš„æƒ…ç»ªæµ‹è¯•åœºæ™¯\nemotion_scenarios = [\n    {\n        'name': 'æ€¥æ€§ç„¦è™‘',\n        'description': 'é«˜å‹å·¥ä½œç¯å¢ƒå¯¼è‡´çš„æ€¥æ€§ç„¦è™‘çŠ¶æ€',\n        'current_emotion': EmotionState(valence=-0.7, arousal=0.9, dominance=0.2, confidence=0.9),\n        'target_emotion': EmotionState(valence=0.2, arousal=-0.6, dominance=0.6, confidence=0.9),\n        'user_profile': {\n            'music_preference': 'classical',\n            'therapy_goal': 'immediate_relief',\n            'session_duration': 15\n        }\n    },\n    {\n        'name': 'æ…¢æ€§ç–²åŠ³',\n        'description': 'é•¿æœŸå·¥ä½œå¯¼è‡´çš„èº«å¿ƒç–²æƒ«',\n        'current_emotion': EmotionState(valence=-0.4, arousal=-0.6, dominance=0.3, confidence=0.8),\n        'target_emotion': EmotionState(valence=0.4, arousal=-0.3, dominance=0.7, confidence=0.9),\n        'user_profile': {\n            'music_preference': 'ambient',\n            'therapy_goal': 'energy_restoration',\n            'session_duration': 25\n        }\n    },\n    {\n        'name': 'æƒ…ç»ªä½è½',\n        'description': 'è½»åº¦æŠ‘éƒæƒ…ç»ªï¼Œç¼ºä¹ç§¯ææ€§',\n        'current_emotion': EmotionState(valence=-0.8, arousal=-0.2, dominance=0.1, confidence=0.85),\n        'target_emotion': EmotionState(valence=0.1, arousal=0.2, dominance=0.5, confidence=0.8),\n        'user_profile': {\n            'music_preference': 'folk',\n            'therapy_goal': 'mood_elevation',\n            'session_duration': 20\n        }\n    },\n    {\n        'name': 'è¿‡åº¦å…´å¥‹',\n        'description': 'å…´å¥‹è¿‡åº¦ï¼Œéœ€è¦å¹³é™ä¸‹æ¥',\n        'current_emotion': EmotionState(valence=0.6, arousal=0.8, dominance=0.8, confidence=0.8),\n        'target_emotion': EmotionState(valence=0.4, arousal=-0.5, dominance=0.6, confidence=0.9),\n        'user_profile': {\n            'music_preference': 'nature_sounds',\n            'therapy_goal': 'calming',\n            'session_duration': 12\n        }\n    },\n    {\n        'name': 'ç¡å‰å‡†å¤‡',\n        'description': 'ä¸ºé«˜è´¨é‡ç¡çœ åšå‡†å¤‡',\n        'current_emotion': EmotionState(valence=0.1, arousal=0.3, dominance=0.5, confidence=0.7),\n        'target_emotion': EmotionState(valence=0.3, arousal=-0.9, dominance=0.6, confidence=0.9),\n        'user_profile': {\n            'music_preference': 'meditation',\n            'therapy_goal': 'sleep_induction',\n            'session_duration': 30\n        }\n    }\n]\n\nprint(f\"=== åˆ›å»ºäº† {len(emotion_scenarios)} ä¸ªæƒ…ç»ªæµ‹è¯•åœºæ™¯ ===\")\nfor i, scenario in enumerate(emotion_scenarios, 1):\n    current = scenario['current_emotion']\n    target = scenario['target_emotion']\n    distance = current.distance_to(target)\n    print(f\"{i}. {scenario['name']}: \"\n          f\"V({current.valence:.2f}â†’{target.valence:.2f}), \"\n          f\"A({current.arousal:.2f}â†’{target.arousal:.2f}), \"\n          f\"è·ç¦»={distance:.3f}, \"\n          f\"ç›®æ ‡={scenario['user_profile']['therapy_goal']}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. åŸºäºæƒ…ç»ªçŠ¶æ€çš„éŸ³ä¹å¤„æ–¹ç”Ÿæˆ\n\nä¸ºä¸åŒçš„æƒ…ç»ªçŠ¶æ€ç”Ÿæˆä¸ªæ€§åŒ–çš„éŸ³ä¹æ²»ç–—å¤„æ–¹ï¼Œå±•ç¤ºç³»ç»Ÿå¦‚ä½•å°†å¿ƒç†å­¦ç†è®ºè½¬åŒ–ä¸ºå…·ä½“çš„éŸ³ä¹å‚æ•°ã€‚",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# åˆå§‹åŒ–éŸ³ä¹ç”Ÿæˆç³»ç»Ÿ\nprint(\"æ­£åœ¨åˆå§‹åŒ–æ²»ç–—æ€§éŸ³ä¹ç”Ÿæˆç³»ç»Ÿ...\")\n\n# åˆ›å»ºæ ¸å¿ƒç»„ä»¶å®ä¾‹\nmusic_psychology = MusicPsychologyModel()\nmusic_generator = TherapeuticMusicGenerator()\n\nprint(f\"âœ… éŸ³ä¹å¿ƒç†å­¦æ¨¡å‹: å·²åŠ è½½\")\nprint(f\"âœ… éŸ³ä¹ç”Ÿæˆå™¨: {music_generator.model_name}\")\nprint(f\"âœ… é‡‡æ ·ç‡: {music_generator.sample_rate} Hz\")\nprint(f\"âœ… æ¨¡æ‹Ÿæ¨¡å¼: {'æ˜¯' if SIMULATION_MODE else 'å¦'}\")\n\n# åˆ›å»ºè¾“å‡ºç›®å½•\noutput_dir = Path('../outputs/music_generation')\noutput_dir.mkdir(parents=True, exist_ok=True)\nprint(f\"âœ… è¾“å‡ºç›®å½•: {output_dir}\")\n\nprint(\"\\nğŸµ éŸ³ä¹ç”Ÿæˆå·¥ä½œå®¤å‡†å¤‡å°±ç»ªï¼\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - æ²»ç–—æ€§éŸ³ä¹ç”Ÿæˆå·¥ä½œå®¤ (Therapeutic Music Generation Workshop)\n",
    "\n",
    "æœ¬notebookæ·±å…¥æ¢ç´¢ã€Œå¿ƒå¢ƒæµè½¬ã€ç³»ç»Ÿçš„æ²»ç–—æ€§éŸ³ä¹ç”ŸæˆåŠŸèƒ½:\n",
    "- åŸºäºæƒ…ç»ªçŠ¶æ€çš„éŸ³ä¹å‚æ•°è‡ªåŠ¨è°ƒæ•´\n",
    "- ä¸åŒæƒ…ç»ªè½¬æ¢çš„éŸ³ä¹å¤„æ–¹ç”Ÿæˆ\n",
    "- éŸ³ä¹è´¨é‡è¯„ä¼°å’Œæ²»ç–—æ•ˆæœåˆ†æ\n",
    "- ä¸ªæ€§åŒ–éŸ³ä¹é£æ ¼é€‚é…\n",
    "- å®æ—¶éŸ³ä¹ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–\n",
    "\n",
    "è¿™æ˜¯ç³»ç»Ÿä¸­æœ€æ ¸å¿ƒçš„åˆ›ä½œæ¨¡å—ï¼Œå°†å¿ƒç†å­¦ç†è®ºè½¬åŒ–ä¸ºå…·ä½“çš„éŸ³ä¹æ²»ç–—æ–¹æ¡ˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¡€è®¾ç½®å’Œå¯¼å…¥\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# æ·»åŠ é¡¹ç›®è·¯å¾„\n",
    "project_root = Path(os.getcwd()).parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"é¡¹ç›®æ ¹ç›®å½•: {project_root}\")\n",
    "print(f\"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}\")\n",
    "print(f\"éŸ³ä¹ç”Ÿæˆå·¥ä½œå®¤å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. éŸ³ä¹ç”Ÿæˆç³»ç»Ÿåˆå§‹åŒ–\n",
    "\n",
    "åˆå§‹åŒ–æ²»ç–—æ€§éŸ³ä¹ç”Ÿæˆçš„æ ¸å¿ƒç»„ä»¶å’Œç†è®ºæ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥éŸ³ä¹ç”Ÿæˆç›¸å…³æ¨¡å—\n",
    "try:\n",
    "    from src.models.music_generator import TherapeuticMusicGenerator, MusicGenerationConfig\n",
    "    from src.research.theory.music_psychology import (\n",
    "        MusicPsychologyModel, MusicalCharacteristics, InstrumentFamily, MusicalKey\n",
    "    )\n",
    "    from src.research.theory.iso_principle import EmotionState\n",
    "    from src.therapy.prescriptions import MusicPrescription\n",
    "    print(\"âœ… éŸ³ä¹ç”Ÿæˆæ¨¡å—å¯¼å…¥æˆåŠŸ\")\n",
    "    SIMULATION_MODE = False\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ éŸ³ä¹ç”Ÿæˆæ¨¡å—å¯¼å…¥å¤±è´¥: {e}\")\n",
    "    print(\"å¯ç”¨æ¨¡æ‹Ÿæ¨¡å¼...\")\n",
    "    SIMULATION_MODE = True\n",
    "    \n",
    "    # åˆ›å»ºæ¨¡æ‹ŸéŸ³ä¹ç”Ÿæˆç³»ç»Ÿ\n",
    "    from enum import Enum\n",
    "    from dataclasses import dataclass\n",
    "    from typing import Dict, List, Optional, Any\n",
    "    \n",
    "    class InstrumentFamily(Enum):\n",
    "        PIANO = \"piano\"\n",
    "        STRINGS = \"strings\"\n",
    "        WINDS = \"winds\"\n",
    "        AMBIENT = \"ambient\"\n",
    "        NATURE = \"nature_sounds\"\n",
    "        PERCUSSION = \"percussion\"\n",
    "    \n",
    "    class MusicalKey(Enum):\n",
    "        C_MAJOR = \"C_major\"\n",
    "        G_MAJOR = \"G_major\"\n",
    "        D_MAJOR = \"D_major\"\n",
    "        A_MAJOR = \"A_major\"\n",
    "        E_MAJOR = \"E_major\"\n",
    "        F_MAJOR = \"F_major\"\n",
    "        BB_MAJOR = \"Bb_major\"\n",
    "        EB_MAJOR = \"Eb_major\"\n",
    "        A_MINOR = \"A_minor\"\n",
    "        E_MINOR = \"E_minor\"\n",
    "        B_MINOR = \"B_minor\"\n",
    "        D_MINOR = \"D_minor\"\n",
    "        G_MINOR = \"G_minor\"\n",
    "        C_MINOR = \"C_minor\"\n",
    "        F_MINOR = \"F_minor\"\n",
    "    \n",
    "    class EmotionState:\n",
    "        def __init__(self, valence=0, arousal=0, dominance=0.5, confidence=0.8):\n",
    "            self.valence = valence\n",
    "            self.arousal = arousal\n",
    "            self.dominance = dominance\n",
    "            self.confidence = confidence\n",
    "        \n",
    "        def distance_to(self, other):\n",
    "            return np.sqrt((self.valence - other.valence)**2 + (self.arousal - other.arousal)**2)\n",
    "    \n",
    "    @dataclass\n",
    "    class MusicalCharacteristics:\n",
    "        tempo_bpm: int\n",
    "        key: MusicalKey\n",
    "        primary_instruments: List[InstrumentFamily]\n",
    "        dynamic_range: float\n",
    "        harmonic_complexity: float\n",
    "        volume_db: float\n",
    "        therapeutic_mechanisms: List[str]\n",
    "    \n",
    "    class MockMusicPsychologyModel:\n",
    "        def generate_therapeutic_music_prescription(self, emotion_state, target_state, duration_minutes, user_preferences=None):\n",
    "            # åŸºäºæƒ…ç»ªçŠ¶æ€ç”ŸæˆéŸ³ä¹ç‰¹å¾\n",
    "            arousal_diff = target_state.arousal - emotion_state.arousal\n",
    "            valence_diff = target_state.valence - emotion_state.valence\n",
    "            \n",
    "            # BPMè®¡ç®—\n",
    "            base_bpm = 70\n",
    "            arousal_adjustment = emotion_state.arousal * 25\n",
    "            target_adjustment = arousal_diff * 15\n",
    "            bpm = max(40, min(120, base_bpm + arousal_adjustment + target_adjustment))\n",
    "            \n",
    "            # è°ƒæ€§é€‰æ‹©\n",
    "            if emotion_state.valence < -0.3:\n",
    "                key = MusicalKey.D_MINOR if valence_diff < 0.3 else MusicalKey.F_MAJOR\n",
    "            else:\n",
    "                key = MusicalKey.C_MAJOR if target_state.valence > 0 else MusicalKey.A_MINOR\n",
    "            \n",
    "            # ä¹å™¨é€‰æ‹©\n",
    "            instruments = [InstrumentFamily.PIANO]\n",
    "            if emotion_state.arousal > 0.3:\n",
    "                instruments.append(InstrumentFamily.STRINGS)\n",
    "            if target_state.arousal < -0.5:\n",
    "                instruments.extend([InstrumentFamily.AMBIENT, InstrumentFamily.NATURE])\n",
    "            \n",
    "            return MusicalCharacteristics(\n",
    "                tempo_bpm=int(bpm),\n",
    "                key=key,\n",
    "                primary_instruments=instruments,\n",
    "                dynamic_range=0.3 if target_state.arousal < 0 else 0.5,\n",
    "                harmonic_complexity=0.4 if emotion_state.arousal > 0.5 else 0.2,\n",
    "                volume_db=-20 if target_state.arousal < 0 else -15,\n",
    "                therapeutic_mechanisms=self._get_mechanisms(emotion_state, target_state)\n",
    "            )\n",
    "        \n",
    "        def _get_mechanisms(self, current, target):\n",
    "            mechanisms = []\n",
    "            if current.arousal > 0.3:\n",
    "                mechanisms.append(\"arousal_reduction\")\n",
    "            if current.valence < -0.3:\n",
    "                mechanisms.append(\"mood_elevation\")\n",
    "            if target.arousal < -0.5:\n",
    "                mechanisms.append(\"sleep_induction\")\n",
    "            mechanisms.append(\"stress_relief\")\n",
    "            return mechanisms\n",
    "    \n",
    "    class MockTherapeuticMusicGenerator:\n",
    "        def __init__(self):\n",
    "            self.model_name = \"therapeutic_music_generator_v2\"\n",
    "            self.sample_rate = 22050\n",
    "        \n",
    "        def generate_music(self, prescription, duration_seconds=60):\n",
    "            # ç”ŸæˆåŸºäºå¤„æ–¹çš„éŸ³ä¹æ³¢å½¢\n",
    "            t = np.linspace(0, duration_seconds, int(self.sample_rate * duration_seconds))\n",
    "            \n",
    "            # åŸºç¡€é¢‘ç‡åŸºäºBPMå’Œè°ƒæ€§\n",
    "            base_freq = self._get_key_frequency(prescription.key)\n",
    "            rhythm_freq = prescription.tempo_bpm / 60.0\n",
    "            \n",
    "            # ç”Ÿæˆå¤šå±‚éŸ³ä¹\n",
    "            audio = np.zeros_like(t)\n",
    "            \n",
    "            # ä¸»æ—‹å¾‹\n",
    "            melody = self._generate_melody(t, base_freq, prescription)\n",
    "            audio += melody * 0.4\n",
    "            \n",
    "            # å’Œå£°\n",
    "            harmony = self._generate_harmony(t, base_freq, prescription)\n",
    "            audio += harmony * 0.3\n",
    "            \n",
    "            # èŠ‚æ‹\n",
    "            rhythm = self._generate_rhythm(t, rhythm_freq, prescription)\n",
    "            audio += rhythm * 0.2\n",
    "            \n",
    "            # ç¯å¢ƒéŸ³\n",
    "            if InstrumentFamily.AMBIENT in prescription.primary_instruments:\n",
    "                ambient = self._generate_ambient(t, prescription)\n",
    "                audio += ambient * 0.1\n",
    "            \n",
    "            # åº”ç”¨åŠ¨æ€èŒƒå›´\n",
    "            audio = self._apply_dynamics(audio, prescription.dynamic_range)\n",
    "            \n",
    "            # æ ‡å‡†åŒ–\n",
    "            audio = audio / np.max(np.abs(audio)) * 0.8\n",
    "            \n",
    "            return {\n",
    "                'audio_data': audio,\n",
    "                'sample_rate': self.sample_rate,\n",
    "                'duration': duration_seconds,\n",
    "                'prescription': prescription,\n",
    "                'metadata': {\n",
    "                    'peak_amplitude': np.max(np.abs(audio)),\n",
    "                    'rms_energy': np.sqrt(np.mean(audio**2)),\n",
    "                    'spectral_centroid': self._calculate_spectral_centroid(audio),\n",
    "                    'zero_crossing_rate': self._calculate_zcr(audio)\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        def _get_key_frequency(self, key):\n",
    "            key_frequencies = {\n",
    "                MusicalKey.C_MAJOR: 261.63,\n",
    "                MusicalKey.G_MAJOR: 196.00,\n",
    "                MusicalKey.D_MAJOR: 146.83,\n",
    "                MusicalKey.A_MAJOR: 220.00,\n",
    "                MusicalKey.F_MAJOR: 174.61,\n",
    "                MusicalKey.A_MINOR: 220.00,\n",
    "                MusicalKey.D_MINOR: 146.83,\n",
    "                MusicalKey.G_MINOR: 196.00,\n",
    "                MusicalKey.C_MINOR: 261.63\n",
    "            }\n",
    "            return key_frequencies.get(key, 261.63)\n",
    "        \n",
    "        def _generate_melody(self, t, base_freq, prescription):\n",
    "            # ç”Ÿæˆä¸»æ—‹å¾‹\n",
    "            melody = np.sin(2 * np.pi * base_freq * t)\n",
    "            \n",
    "            # æ·»åŠ è°æ³¢\n",
    "            if prescription.harmonic_complexity > 0.3:\n",
    "                melody += 0.3 * np.sin(2 * np.pi * base_freq * 1.5 * t)\n",
    "                melody += 0.2 * np.sin(2 * np.pi * base_freq * 2.0 * t)\n",
    "            \n",
    "            # BPMè°ƒåˆ¶\n",
    "            bpm_modulation = 1 + 0.1 * np.sin(2 * np.pi * prescription.tempo_bpm / 60.0 * t)\n",
    "            melody *= bpm_modulation\n",
    "            \n",
    "            return melody\n",
    "        \n",
    "        def _generate_harmony(self, t, base_freq, prescription):\n",
    "            # ç”Ÿæˆå’Œå£°\n",
    "            harmony = 0.5 * np.sin(2 * np.pi * base_freq * 0.75 * t)  # äº”åº¦\n",
    "            harmony += 0.3 * np.sin(2 * np.pi * base_freq * 0.6 * t)   # ä¸‰åº¦\n",
    "            return harmony\n",
    "        \n",
    "        def _generate_rhythm(self, t, rhythm_freq, prescription):\n",
    "            # ç”ŸæˆèŠ‚æ‹\n",
    "            rhythm = np.sin(2 * np.pi * rhythm_freq * t)\n",
    "            rhythm = np.where(rhythm > 0, 0.2, 0.0)  # è„‰å†²æ³¢\n",
    "            \n",
    "            # è½¯åŒ–èŠ‚æ‹è¾¹ç¼˜\n",
    "            rhythm = signal.savgol_filter(rhythm, 51, 3)\n",
    "            return rhythm\n",
    "        \n",
    "        def _generate_ambient(self, t, prescription):\n",
    "            # ç”Ÿæˆç¯å¢ƒéŸ³\n",
    "            # ç™½å™ªå£°åŸºç¡€\n",
    "            ambient = np.random.normal(0, 0.05, len(t))\n",
    "            \n",
    "            # ä½é€šæ»¤æ³¢æ¨¡æ‹Ÿè‡ªç„¶å£°éŸ³\n",
    "            b, a = signal.butter(4, 0.1, 'low')\n",
    "            ambient = signal.filtfilt(b, a, ambient)\n",
    "            \n",
    "            return ambient\n",
    "        \n",
    "        def _apply_dynamics(self, audio, dynamic_range):\n",
    "            # åº”ç”¨åŠ¨æ€èŒƒå›´è°ƒæ•´\n",
    "            envelope = 1.0 + dynamic_range * np.sin(2 * np.pi * 0.1 * np.arange(len(audio)) / len(audio))\n",
    "            return audio * envelope\n",
    "        \n",
    "        def _calculate_spectral_centroid(self, audio):\n",
    "            # è®¡ç®—é¢‘è°±è´¨å¿ƒ\n",
    "            fft = np.fft.fft(audio)\n",
    "            magnitude = np.abs(fft)\n",
    "            freqs = np.fft.fftfreq(len(audio), 1/self.sample_rate)\n",
    "            centroid = np.sum(freqs[:len(freqs)//2] * magnitude[:len(magnitude)//2]) / np.sum(magnitude[:len(magnitude)//2])\n",
    "            return abs(centroid)\n",
    "        \n",
    "        def _calculate_zcr(self, audio):\n",
    "            # è®¡ç®—è¿‡é›¶ç‡\n",
    "            return np.sum(np.diff(np.sign(audio)) != 0) / len(audio)\n",
    "    \n",
    "    MusicPsychologyModel = MockMusicPsychologyModel\n",
    "    TherapeuticMusicGenerator = MockTherapeuticMusicGenerator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}