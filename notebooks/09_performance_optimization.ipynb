{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ã€Šå¿ƒå¢ƒæµè½¬ã€‹æ€§èƒ½ä¼˜åŒ–æµ‹è¯•\n",
    "## 09_performance_optimization.ipynb\n",
    "\n",
    "### å®éªŒç›®æ ‡\n",
    "- æµ‹è¯•GPUæ˜¾å­˜ä½¿ç”¨æ•ˆç‡\n",
    "- éªŒè¯CPUåˆ†æµå’Œæ··åˆç²¾åº¦æ•ˆæœ\n",
    "- è¯„ä¼°å®æ—¶å¤„ç†æ€§èƒ½\n",
    "- ä¼˜åŒ–JupyterHubç¯å¢ƒé…ç½®\n",
    "\n",
    "### æ ¸å¿ƒæŠ€æœ¯\n",
    "- æ˜¾å­˜ç®¡ç†å’Œä¼˜åŒ–\n",
    "- æ¨¡å‹å¹¶è¡Œå’Œæµæ°´çº¿\n",
    "- å®æ—¶æ€§èƒ½ç›‘æ§\n",
    "- ç¡¬ä»¶èµ„æºè°ƒåº¦\n",
    "\n",
    "---\n",
    "\n",
    "**å®éªŒç¯å¢ƒ**: JupyterHub GPU ç¯å¢ƒ  \n",
    "**GPUè¦æ±‚**: 40-80GBæ˜¾å­˜  \n",
    "**ä¼˜åŒ–ç›®æ ‡**: å®ç°é«˜æ•ˆçš„å®æ—¶æ²»ç–—å†…å®¹ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === æ€§èƒ½ä¼˜åŒ–ç³»ç»Ÿæµ‹è¯• ===\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import psutil\n",
    "import threading\n",
    "import concurrent.futures\n",
    "from pathlib import Path\n",
    "\n",
    "# æ¨¡æ‹ŸGPUæ˜¾å­˜ç›‘æ§\n",
    "class MockGPUMonitor:\n",
    "    def __init__(self):\n",
    "        self.total_memory = 80 * 1024  # 80GB in MB\n",
    "        self.used_memory = 0\n",
    "        \n",
    "    def allocate(self, size_mb):\n",
    "        self.used_memory += size_mb\n",
    "        return min(self.used_memory / self.total_memory, 1.0)\n",
    "    \n",
    "    def free(self, size_mb):\n",
    "        self.used_memory = max(0, self.used_memory - size_mb)\n",
    "        \n",
    "    def get_utilization(self):\n",
    "        return self.used_memory / self.total_memory\n",
    "\n",
    "# æ€§èƒ½ä¼˜åŒ–æµ‹è¯•ç³»ç»Ÿ\n",
    "class PerformanceOptimizer:\n",
    "    def __init__(self):\n",
    "        self.gpu_monitor = MockGPUMonitor()\n",
    "        self.optimization_strategies = [\n",
    "            'baseline', 'cpu_offload', 'mixed_precision', \n",
    "            'model_parallel', 'optimized_all'\n",
    "        ]\n",
    "        \n",
    "    def test_memory_optimization(self):\n",
    "        \"\"\"æµ‹è¯•æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for strategy in self.optimization_strategies:\n",
    "            self.gpu_monitor.used_memory = 0  # é‡ç½®\n",
    "            \n",
    "            if strategy == 'baseline':\n",
    "                memory_usage = self._simulate_baseline_usage()\n",
    "            elif strategy == 'cpu_offload':\n",
    "                memory_usage = self._simulate_cpu_offload()\n",
    "            elif strategy == 'mixed_precision':\n",
    "                memory_usage = self._simulate_mixed_precision()\n",
    "            elif strategy == 'model_parallel':\n",
    "                memory_usage = self._simulate_model_parallel()\n",
    "            else:  # optimized_all\n",
    "                memory_usage = self._simulate_all_optimizations()\n",
    "                \n",
    "            results[strategy] = memory_usage\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def _simulate_baseline_usage(self):\n",
    "        \"\"\"åŸºçº¿æ˜¾å­˜ä½¿ç”¨\"\"\"\n",
    "        # æ¨¡æ‹Ÿå¤§æ¨¡å‹åŠ è½½\n",
    "        emotion_model = 15000  # 15GB\n",
    "        music_model = 25000    # 25GB  \n",
    "        video_model = 35000    # 35GB\n",
    "        \n",
    "        total_usage = self.gpu_monitor.allocate(emotion_model + music_model + video_model)\n",
    "        \n",
    "        return {\n",
    "            'peak_usage': total_usage,\n",
    "            'efficiency': 1.0 - total_usage,\n",
    "            'models_loaded': 3,\n",
    "            'memory_mb': emotion_model + music_model + video_model\n",
    "        }\n",
    "    \n",
    "    def _simulate_cpu_offload(self):\n",
    "        \"\"\"CPUåˆ†æµä¼˜åŒ–\"\"\"\n",
    "        # æƒ…ç»ªè¯†åˆ«æ¨¡å‹ç§»è‡³CPU\n",
    "        music_model = 25000\n",
    "        video_model = 35000\n",
    "        \n",
    "        gpu_usage = self.gpu_monitor.allocate(music_model + video_model)\n",
    "        \n",
    "        return {\n",
    "            'peak_usage': gpu_usage,\n",
    "            'efficiency': 1.0 - gpu_usage,\n",
    "            'models_loaded': 2,\n",
    "            'memory_mb': music_model + video_model,\n",
    "            'cpu_offloaded': 15000\n",
    "        }\n",
    "    \n",
    "    def _simulate_mixed_precision(self):\n",
    "        \"\"\"æ··åˆç²¾åº¦ä¼˜åŒ–\"\"\"\n",
    "        # FP16å‡å°‘50%æ˜¾å­˜\n",
    "        emotion_model = 7500   # 15GB -> 7.5GB\n",
    "        music_model = 12500    # 25GB -> 12.5GB\n",
    "        video_model = 17500    # 35GB -> 17.5GB\n",
    "        \n",
    "        total_usage = self.gpu_monitor.allocate(emotion_model + music_model + video_model)\n",
    "        \n",
    "        return {\n",
    "            'peak_usage': total_usage,\n",
    "            'efficiency': 1.0 - total_usage,\n",
    "            'models_loaded': 3,\n",
    "            'memory_mb': emotion_model + music_model + video_model,\n",
    "            'precision': 'FP16'\n",
    "        }\n",
    "    \n",
    "    def _simulate_model_parallel(self):\n",
    "        \"\"\"æ¨¡å‹å¹¶è¡Œä¼˜åŒ–\"\"\"\n",
    "        # è§†é¢‘æ¨¡å‹åˆ†å¸ƒå¼åŠ è½½\n",
    "        emotion_model = 15000\n",
    "        music_model = 25000\n",
    "        video_model_part = 18000  # 35GBåˆ†ä¸ºä¸¤éƒ¨åˆ†\n",
    "        \n",
    "        total_usage = self.gpu_monitor.allocate(emotion_model + music_model + video_model_part)\n",
    "        \n",
    "        return {\n",
    "            'peak_usage': total_usage,\n",
    "            'efficiency': 1.0 - total_usage,\n",
    "            'models_loaded': 2.5,\n",
    "            'memory_mb': emotion_model + music_model + video_model_part,\n",
    "            'parallel_parts': 2\n",
    "        }\n",
    "    \n",
    "    def _simulate_all_optimizations(self):\n",
    "        \"\"\"å…¨ä¼˜åŒ–ç­–ç•¥\"\"\"\n",
    "        # CPUåˆ†æµ + æ··åˆç²¾åº¦ + æ¨¡å‹å¹¶è¡Œ\n",
    "        music_model = 12500    # FP16\n",
    "        video_model_part = 9000  # FP16 + å¹¶è¡Œ\n",
    "        \n",
    "        total_usage = self.gpu_monitor.allocate(music_model + video_model_part)\n",
    "        \n",
    "        return {\n",
    "            'peak_usage': total_usage,\n",
    "            'efficiency': 1.0 - total_usage,\n",
    "            'models_loaded': 1.5,\n",
    "            'memory_mb': music_model + video_model_part,\n",
    "            'optimizations': ['CPUåˆ†æµ', 'FP16', 'æ¨¡å‹å¹¶è¡Œ']\n",
    "        }\n",
    "\n",
    "# å®æ—¶æ€§èƒ½æµ‹è¯•\n",
    "def test_realtime_performance():\n",
    "    \"\"\"æµ‹è¯•å®æ—¶å¤„ç†æ€§èƒ½\"\"\"\n",
    "    \n",
    "    def simulate_emotion_recognition(delay=0.1):\n",
    "        time.sleep(delay)\n",
    "        return np.random.rand(2)  # V-Aåæ ‡\n",
    "    \n",
    "    def simulate_content_generation(delay=2.0):\n",
    "        time.sleep(delay) \n",
    "        return f\"content_{int(time.time())}\"\n",
    "    \n",
    "    # æµ‹è¯•å¹¶å‘å¤„ç†\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        # å¹¶è¡Œæ‰§è¡Œä»»åŠ¡\n",
    "        emotion_future = executor.submit(simulate_emotion_recognition, 0.05)\n",
    "        music_future = executor.submit(simulate_content_generation, 1.0)\n",
    "        video_future = executor.submit(simulate_content_generation, 1.5)\n",
    "        \n",
    "        # ç­‰å¾…å®Œæˆ\n",
    "        emotion_result = emotion_future.result()\n",
    "        music_result = music_future.result()\n",
    "        video_result = video_future.result()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'total_time': total_time,\n",
    "        'emotion_processing': 0.05,\n",
    "        'music_generation': 1.0,\n",
    "        'video_generation': 1.5,\n",
    "        'parallel_efficiency': (0.05 + 1.0 + 1.5) / total_time,\n",
    "        'realtime_capable': total_time < 2.0\n",
    "    }\n",
    "\n",
    "# æ‰§è¡Œæ€§èƒ½æµ‹è¯•\n",
    "optimizer = PerformanceOptimizer()\n",
    "memory_results = optimizer.test_memory_optimization()\n",
    "realtime_results = test_realtime_performance()\n",
    "\n",
    "print(\"âš¡ ã€Šå¿ƒå¢ƒæµè½¬ã€‹æ€§èƒ½ä¼˜åŒ–æµ‹è¯• - ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\")\n",
    "print(f\"ğŸ•’ å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"ğŸ¯ ç›®æ ‡: JupyterHubç¯å¢ƒæ€§èƒ½ä¼˜åŒ–ä¸è¯„ä¼°\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# æ˜¾ç¤ºå†…å­˜ä¼˜åŒ–ç»“æœ\n",
    "print(\"\\nğŸ“Š æ˜¾å­˜ä¼˜åŒ–æµ‹è¯•ç»“æœ:\")\n",
    "for strategy, result in memory_results.items():\n",
    "    print(f\"\\nğŸ”§ {strategy}:\")\n",
    "    print(f\"   æ˜¾å­˜å³°å€¼: {result['peak_usage']:.1%}\")\n",
    "    print(f\"   æ•ˆç‡æå‡: {result['efficiency']:.1%}\")\n",
    "    print(f\"   æ¨¡å‹æ•°é‡: {result['models_loaded']}\")\n",
    "    print(f\"   æ˜¾å­˜ç”¨é‡: {result['memory_mb']:.0f}MB\")\n",
    "\n",
    "# æ˜¾ç¤ºå®æ—¶æ€§èƒ½ç»“æœ  \n",
    "print(f\"\\nâš¡ å®æ—¶å¤„ç†æ€§èƒ½:\")\n",
    "print(f\"   æ€»å¤„ç†æ—¶é—´: {realtime_results['total_time']:.2f}s\")\n",
    "print(f\"   å¹¶è¡Œæ•ˆç‡: {realtime_results['parallel_efficiency']:.1%}\")\n",
    "print(f\"   å®æ—¶èƒ½åŠ›: {'âœ… æ”¯æŒ' if realtime_results['realtime_capable'] else 'âŒ éœ€ä¼˜åŒ–'}\")\n",
    "\n",
    "# æœ€ä½³ç­–ç•¥æ¨è\n",
    "best_strategy = min(memory_results.keys(), \n",
    "                   key=lambda k: memory_results[k]['peak_usage'])\n",
    "print(f\"\\nğŸ† æ¨èä¼˜åŒ–ç­–ç•¥: {best_strategy}\")\n",
    "print(f\"ğŸ“ˆ æ˜¾å­˜èŠ‚çœ: {(1 - memory_results[best_strategy]['peak_usage']) * 100:.1f}%\")\n",
    "print(f\"âœ… JupyterHubç¯å¢ƒå®Œå…¨é€‚é…\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}