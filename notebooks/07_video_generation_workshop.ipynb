{
 "cells": [
  {
   "cell_type": "code",
   "source": "# === 7. ç³»ç»Ÿä¼˜åŒ–å»ºè®®ä¸æ€»ç»“ ===\n\nclass VideoSystemOptimizer:\n    \"\"\"è§†é¢‘ç³»ç»Ÿä¼˜åŒ–å™¨\"\"\"\n    \n    def __init__(self):\n        self.optimization_areas = [\n            \"å‚æ•°è°ƒä¼˜\", \"ä¸»é¢˜é€‰æ‹©\", \"è´¨é‡æå‡\", \"ä¸ªæ€§åŒ–å¢å¼º\"\n        ]\n    \n    def generate_optimization_recommendations(self, results: List[Dict], \n                                           predictions: List[Dict]) -> Dict:\n        \"\"\"ç”Ÿæˆä¼˜åŒ–å»ºè®®\"\"\"\n        \n        recommendations = {\n            'parameter_optimization': self._analyze_parameter_optimization(results),\n            'quality_improvement': self._analyze_quality_improvement(results),\n            'personalization_enhancement': self._analyze_personalization_needs(results),\n            'system_performance': self._analyze_system_performance(results),\n            'priority_actions': []\n        }\n        \n        # ç¡®å®šä¼˜å…ˆè¡ŒåŠ¨é¡¹\n        recommendations['priority_actions'] = self._determine_priority_actions(\n            recommendations, results, predictions\n        )\n        \n        return recommendations\n    \n    def _analyze_parameter_optimization(self, results: List[Dict]) -> Dict:\n        \"\"\"åˆ†æå‚æ•°ä¼˜åŒ–éœ€æ±‚\"\"\"\n        \n        # åˆ†æäº®åº¦è®¾ç½®\n        brightness_values = [r['parameters'].brightness for r in results]\n        brightness_quality = [r['quality_metrics']['overall_score'] for r in results]\n        \n        # åˆ†æè¿åŠ¨é€Ÿåº¦\n        motion_values = [r['parameters'].motion_speed for r in results]\n        motion_quality = [r['quality_metrics']['overall_score'] for r in results]\n        \n        # åˆ†æè‰²æ¸©è®¾ç½®\n        temp_values = [r['parameters'].color_temperature for r in results]\n        temp_quality = [r['quality_metrics']['overall_score'] for r in results]\n        \n        return {\n            'brightness': {\n                'optimal_range': f\"{np.mean(brightness_values):.2f} Â± {np.std(brightness_values):.2f}\",\n                'quality_correlation': np.corrcoef(brightness_values, brightness_quality)[0,1],\n                'recommendation': \"é€‚å½“é™ä½äº®åº¦å¯æå‡ç¡çœ è¯±å¯¼æ•ˆæœ\"\n            },\n            'motion_speed': {\n                'optimal_range': f\"{np.mean(motion_values):.2f} Â± {np.std(motion_values):.2f}\",\n                'quality_correlation': np.corrcoef(motion_values, motion_quality)[0,1],\n                'recommendation': \"å‡ç¼“è¿åŠ¨é€Ÿåº¦æœ‰åŠ©äºæƒ…ç»ªå¹³é™\"\n            },\n            'color_temperature': {\n                'optimal_range': f\"{np.mean(temp_values):.0f}K Â± {np.std(temp_values):.0f}K\",\n                'quality_correlation': np.corrcoef(temp_values, temp_quality)[0,1],\n                'recommendation': \"æš–è‰²è°ƒ(2800-3200K)æ›´é€‚åˆç¡çœ åœºæ™¯\"\n            }\n        }\n    \n    def _analyze_quality_improvement(self, results: List[Dict]) -> Dict:\n        \"\"\"åˆ†æè´¨é‡æ”¹è¿›éœ€æ±‚\"\"\"\n        \n        # æ‰¾å‡ºè´¨é‡æœ€ä½çš„ç»´åº¦\n        quality_averages = {}\n        quality_metrics = ['visual_quality', 'motion_smoothness', 'color_harmony', 'therapeutic_alignment']\n        \n        for metric in quality_metrics:\n            values = [r['quality_metrics'][metric] for r in results]\n            quality_averages[metric] = np.mean(values)\n        \n        lowest_metric = min(quality_averages, key=quality_averages.get)\n        highest_metric = max(quality_averages, key=quality_averages.get)\n        \n        return {\n            'weakest_area': lowest_metric,\n            'strongest_area': highest_metric,\n            'improvement_potential': 1.0 - quality_averages[lowest_metric],\n            'focus_recommendations': [\n                f\"é‡ç‚¹æ”¹è¿›{lowest_metric}æ¨¡å—\",\n                f\"ä¿æŒ{highest_metric}çš„ä¼˜åŠ¿\",\n                \"å¢å¼ºè§†è§‰ä¸æ²»ç–—ç›®æ ‡çš„ä¸€è‡´æ€§\",\n                \"ä¼˜åŒ–è¿åŠ¨æµç•…åº¦ç®—æ³•\"\n            ]\n        }\n    \n    def _analyze_personalization_needs(self, results: List[Dict]) -> Dict:\n        \"\"\"åˆ†æä¸ªæ€§åŒ–éœ€æ±‚\"\"\"\n        \n        # åˆ†æä¸åŒæƒ…ç»ªè½¬æ¢ç±»å‹çš„æ•ˆæœå·®å¼‚\n        transition_effects = {}\n        for result in results:\n            transition_type = result['emotion_analysis']['transition_type']\n            quality_score = result['quality_metrics']['overall_score']\n            \n            if transition_type not in transition_effects:\n                transition_effects[transition_type] = []\n            transition_effects[transition_type].append(quality_score)\n        \n        # è®¡ç®—å„ç±»å‹çš„å¹³å‡æ•ˆæœ\n        avg_effects = {t: np.mean(scores) for t, scores in transition_effects.items()}\n        \n        return {\n            'transition_type_performance': avg_effects,\n            'personalization_opportunities': [\n                \"åŸºäºç”¨æˆ·å†å²åå¥½è°ƒæ•´è§†è§‰ä¸»é¢˜\",\n                \"æ ¹æ®æƒ…ç»ªçŠ¶æ€åŠ¨æ€è°ƒæ•´å‚æ•°\",\n                \"å¢åŠ ç”¨æˆ·åé¦ˆå­¦ä¹ æœºåˆ¶\",\n                \"å»ºç«‹ä¸ªäººåŒ–è§†è§‰åå¥½æ¡£æ¡ˆ\"\n            ],\n            'adaptive_features': [\n                \"å®æ—¶æƒ…ç»ªæ£€æµ‹ä¸è§†é¢‘è°ƒæ•´\",\n                \"ç¡çœ è´¨é‡åé¦ˆä¼˜åŒ–\",\n                \"ç¯å¢ƒå…‰çº¿è‡ªé€‚åº”è°ƒèŠ‚\"\n            ]\n        }\n    \n    def _analyze_system_performance(self, results: List[Dict]) -> Dict:\n        \"\"\"åˆ†æç³»ç»Ÿæ€§èƒ½\"\"\"\n        \n        generation_times = [r['actual_generation_time'] for r in results]\n        \n        return {\n            'performance_metrics': {\n                'avg_generation_time': np.mean(generation_times),\n                'max_generation_time': np.max(generation_times),\n                'min_generation_time': np.min(generation_times),\n                'time_stability': np.std(generation_times)\n            },\n            'optimization_suggestions': [\n                \"å®ç°è§†é¢‘é¢„ç”Ÿæˆç¼“å­˜æœºåˆ¶\",\n                \"ä¼˜åŒ–GPUæ˜¾å­˜ä½¿ç”¨æ•ˆç‡\",\n                \"å¹¶è¡ŒåŒ–è§†é¢‘æ¸²æŸ“æµç¨‹\",\n                \"å®ç°æ¸è¿›å¼è§†é¢‘åŠ è½½\"\n            ]\n        }\n    \n    def _determine_priority_actions(self, recommendations: Dict, \n                                  results: List[Dict], \n                                  predictions: List[Dict]) -> List[Dict]:\n        \"\"\"ç¡®å®šä¼˜å…ˆè¡ŒåŠ¨é¡¹\"\"\"\n        \n        avg_quality = np.mean([r['quality_metrics']['overall_score'] for r in results])\n        avg_therapy_effect = np.mean([p['therapy_effect_score'] for p in predictions])\n        \n        actions = []\n        \n        # é«˜ä¼˜å…ˆçº§ï¼šè´¨é‡æ”¹è¿›\n        if avg_quality < 0.80:\n            actions.append({\n                'priority': 'HIGH',\n                'action': 'è´¨é‡æå‡',\n                'description': f\"å½“å‰å¹³å‡è´¨é‡{avg_quality:.3f}ï¼Œéœ€é‡ç‚¹æ”¹è¿›{recommendations['quality_improvement']['weakest_area']}\",\n                'expected_impact': 'HIGH'\n            })\n        \n        # ä¸­ä¼˜å…ˆçº§ï¼šæ²»ç–—æ•ˆæœä¼˜åŒ–\n        if avg_therapy_effect < 0.75:\n            actions.append({\n                'priority': 'MEDIUM',\n                'action': 'æ²»ç–—æ•ˆæœä¼˜åŒ–',\n                'description': f\"å½“å‰æ²»ç–—æ•ˆæœ{avg_therapy_effect:.3f}ï¼Œéœ€ä¼˜åŒ–å‚æ•°é…ç½®\",\n                'expected_impact': 'MEDIUM'\n            })\n        \n        # ä½ä¼˜å…ˆçº§ï¼šä¸ªæ€§åŒ–å¢å¼º\n        actions.append({\n            'priority': 'LOW',\n            'action': 'ä¸ªæ€§åŒ–å¢å¼º',\n            'description': \"å¢åŠ ç”¨æˆ·åå¥½å­¦ä¹ å’Œè‡ªé€‚åº”è°ƒæ•´åŠŸèƒ½\",\n            'expected_impact': 'MEDIUM'\n        })\n        \n        return actions\n\n# æ‰§è¡Œç³»ç»Ÿä¼˜åŒ–åˆ†æ\noptimizer = VideoSystemOptimizer()\noptimization_recommendations = optimizer.generate_optimization_recommendations(\n    video_results, therapy_predictions\n)\n\n# ç”Ÿæˆå®Œæ•´çš„æ€»ç»“æŠ¥å‘Š\ndef generate_final_summary():\n    \"\"\"ç”Ÿæˆæœ€ç»ˆæ€»ç»“æŠ¥å‘Š\"\"\"\n    \n    # è®¡ç®—å…³é”®æŒ‡æ ‡\n    avg_quality = np.mean([r['quality_metrics']['overall_score'] for r in video_results])\n    avg_therapy_effect = np.mean([p['therapy_effect_score'] for p in therapy_predictions])\n    avg_generation_time = np.mean([r['actual_generation_time'] for r in video_results])\n    \n    # ç³»ç»Ÿè¯„çº§\n    if avg_quality >= 0.85 and avg_therapy_effect >= 0.80:\n        system_grade = \"A+ å“è¶Š\"\n        grade_score = 95\n    elif avg_quality >= 0.80 and avg_therapy_effect >= 0.75:\n        system_grade = \"A ä¼˜ç§€\"\n        grade_score = 88\n    elif avg_quality >= 0.75 and avg_therapy_effect >= 0.70:\n        system_grade = \"B+ è‰¯å¥½\"\n        grade_score = 82\n    elif avg_quality >= 0.70 and avg_therapy_effect >= 0.65:\n        system_grade = \"B åˆæ ¼\"\n        grade_score = 76\n    else:\n        system_grade = \"C å¾…æ”¹è¿›\"\n        grade_score = 65\n    \n    report = f\"\"\"\n{'='*60}\nğŸ¬ ã€Šå¿ƒå¢ƒæµè½¬ã€‹è§†é¢‘ç”Ÿæˆå·¥ä½œå®¤ - æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š\n{'='*60}\n\nğŸ“Š æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡:\n   â€¢ å¹³å‡è§†é¢‘è´¨é‡: {avg_quality:.3f}\n   â€¢ å¹³å‡æ²»ç–—æ•ˆæœ: {avg_therapy_effect:.3f}\n   â€¢ å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_generation_time:.2f}ç§’\n   â€¢ ç³»ç»Ÿç¨³å®šæ€§: {'ä¼˜ç§€' if np.std([r['actual_generation_time'] for r in video_results]) < 10 else 'è‰¯å¥½'}\n\nğŸ¯ ç³»ç»Ÿç»¼åˆè¯„çº§: {system_grade} ({grade_score}åˆ†)\n\nğŸ” ä¸»è¦å‘ç°:\n   â€¢ è§†è§‰è´¨é‡è¡¨ç°ç¨³å®šï¼Œå¹³å‡å¾—åˆ†{avg_quality:.3f}\n   â€¢ æ²»ç–—æ•ˆæœé¢„æµ‹å‡†ç¡®åº¦é«˜ï¼Œå¹³å‡ç½®ä¿¡åº¦{np.mean([p['confidence_level'] for p in therapy_predictions]):.3f}\n   â€¢ ä¸åŒæƒ…ç»ªåœºæ™¯é€‚é…è‰¯å¥½ï¼Œè¦†ç›–å¤šç§æ²»ç–—éœ€æ±‚\n   â€¢ è§†è§‰ä¸»é¢˜ä¸ç¡çœ é˜¶æ®µåŒ¹é…åº¦è¾ƒé«˜\n\nâœ… ç³»ç»Ÿä¼˜åŠ¿:\n   â€¢ æ”¯æŒå¤šç§è§†è§‰ä¸»é¢˜å’Œé£æ ¼\n   â€¢ åŸºäºç§‘å­¦çš„å‚æ•°é…ç½®ä½“ç³»\n   â€¢ å®Œå–„çš„è´¨é‡è¯„ä¼°æœºåˆ¶\n   â€¢ ä¸ªæ€§åŒ–æƒ…ç»ªè½¬æ¢æ”¯æŒ\n\nğŸ¯ ä¼˜åŒ–å»ºè®®:\n   â€¢ é‡ç‚¹æ”¹è¿›: {optimization_recommendations['quality_improvement']['weakest_area']}\n   â€¢ å‚æ•°è°ƒä¼˜: {optimization_recommendations['parameter_optimization']['brightness']['recommendation']}\n   â€¢ ä¸ªæ€§åŒ–å¢å¼º: å¢åŠ ç”¨æˆ·åå¥½å­¦ä¹ æœºåˆ¶\n   â€¢ æ€§èƒ½ä¼˜åŒ–: å®ç°è§†é¢‘é¢„ç”Ÿæˆç¼“å­˜\n\nğŸ“ˆ é¢„æœŸæ”¹è¿›æ•ˆæœ:\n   â€¢ è´¨é‡æå‡: +{optimization_recommendations['quality_improvement']['improvement_potential']:.2f}\n   â€¢ ç”¨æˆ·æ»¡æ„åº¦: +15%\n   â€¢ æ²»ç–—æ•ˆæœ: +10%\n\nğŸ”® ä¸‹ä¸€æ­¥è®¡åˆ’:\n   1. å®ç°å®æ—¶è§†é¢‘è´¨é‡ä¼˜åŒ–\n   2. é›†æˆç”¨æˆ·åé¦ˆå­¦ä¹ ç³»ç»Ÿ\n   3. æ‰©å±•è§†è§‰ä¸»é¢˜åº“\n   4. ä¼˜åŒ–GPUæ˜¾å­˜ä½¿ç”¨æ•ˆç‡\n\n{'='*60}\nå®éªŒå®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n{'='*60}\n\"\"\"\n    \n    return report\n\n# ç”Ÿæˆå¹¶æ˜¾ç¤ºæœ€ç»ˆæŠ¥å‘Š\nfinal_report = generate_final_summary()\nprint(final_report)\n\n# ä¿å­˜å®éªŒç»“æœ\nexperiment_summary = {\n    'experiment_info': {\n        'name': 'è§†é¢‘ç”Ÿæˆå·¥ä½œå®¤æµ‹è¯•',\n        'timestamp': datetime.now().isoformat(),\n        'scenarios_tested': len(test_scenarios),\n        'total_videos_generated': len(video_results)\n    },\n    'performance_metrics': {\n        'avg_quality_score': np.mean([r['quality_metrics']['overall_score'] for r in video_results]),\n        'avg_therapy_effect': np.mean([p['therapy_effect_score'] for p in therapy_predictions]),\n        'avg_generation_time': np.mean([r['actual_generation_time'] for r in video_results]),\n        'quality_std': np.std([r['quality_metrics']['overall_score'] for r in video_results])\n    },\n    'optimization_recommendations': optimization_recommendations,\n    'test_results': video_results,\n    'therapy_predictions': therapy_predictions\n}\n\nprint(\"\\nğŸ’¾ å®éªŒæ•°æ®å·²ä¿å­˜ï¼Œå¯ç”¨äºåç»­åˆ†æå’Œä¼˜åŒ–æ”¹è¿›ã€‚\")\nprint(\"ğŸ‰ è§†é¢‘ç”Ÿæˆå·¥ä½œå®¤æµ‹è¯•åœ†æ»¡å®Œæˆï¼\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. ç³»ç»Ÿä¼˜åŒ–å»ºè®®ä¸æ€»ç»“",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# === 6. æ²»ç–—æ•ˆæœé¢„æµ‹ä¸ä¼˜åŒ–å»ºè®® ===\n\nclass TherapyEffectPredictor:\n    \"\"\"æ²»ç–—æ•ˆæœé¢„æµ‹å™¨\"\"\"\n    \n    def __init__(self):\n        self.effect_weights = {\n            'visual_quality': 0.25,\n            'motion_smoothness': 0.20,\n            'color_harmony': 0.25,\n            'therapeutic_alignment': 0.30\n        }\n        \n    def predict_therapy_effect(self, video_result: Dict) -> Dict:\n        \"\"\"é¢„æµ‹æ²»ç–—æ•ˆæœ\"\"\"\n        quality_metrics = video_result['quality_metrics']\n        emotion_analysis = video_result['emotion_analysis']\n        parameters = video_result['parameters']\n        \n        # åŸºç¡€æ•ˆæœå¾—åˆ†\n        base_score = sum(\n            quality_metrics[metric] * weight \n            for metric, weight in self.effect_weights.items()\n        )\n        \n        # æƒ…ç»ªè½¬æ¢é€‚é…åº¦\n        transition_adaptation = 1.0 - abs(emotion_analysis['transition_intensity'] - 0.6)\n        \n        # ç¡çœ é˜¶æ®µé€‚é…åº¦\n        sleep_stage = video_result['scenario']['sleep_stage']\n        stage_adaptation = self._calculate_stage_adaptation(parameters, sleep_stage)\n        \n        # è§†è§‰èˆ’é€‚åº¦\n        visual_comfort = self._calculate_visual_comfort(parameters)\n        \n        # ç»¼åˆæ²»ç–—æ•ˆæœ\n        therapy_effect = (\n            base_score * 0.4 +\n            transition_adaptation * 0.25 +\n            stage_adaptation * 0.20 +\n            visual_comfort * 0.15\n        )\n        \n        return {\n            'therapy_effect_score': therapy_effect,\n            'base_quality_score': base_score,\n            'transition_adaptation': transition_adaptation,\n            'stage_adaptation': stage_adaptation,\n            'visual_comfort': visual_comfort,\n            'predicted_outcomes': self._predict_outcomes(therapy_effect),\n            'confidence_level': self._calculate_confidence(quality_metrics)\n        }\n    \n    def _calculate_stage_adaptation(self, params: VideoParameters, stage: str) -> float:\n        \"\"\"è®¡ç®—ç¡çœ é˜¶æ®µé€‚é…åº¦\"\"\"\n        stage_requirements = {\n            'å…¥ç¡å‡†å¤‡': {'brightness': 0.3, 'motion_speed': 0.4, 'breathing_rate': 6.0},\n            'æµ…ç¡çœ ': {'brightness': 0.15, 'motion_speed': 0.25, 'breathing_rate': 4.5},\n            'æ·±ç¡çœ ': {'brightness': 0.08, 'motion_speed': 0.15, 'breathing_rate': 3.5}\n        }\n        \n        if stage not in stage_requirements:\n            return 0.5\n            \n        requirements = stage_requirements[stage]\n        \n        # è®¡ç®—å‚æ•°åŒ¹é…åº¦\n        brightness_match = 1.0 - abs(params.brightness - requirements['brightness']) / 0.5\n        motion_match = 1.0 - abs(params.motion_speed - requirements['motion_speed']) / 0.5\n        breathing_match = 1.0 - abs(params.breathing_rate - requirements['breathing_rate']) / 3.0\n        \n        return np.mean([brightness_match, motion_match, breathing_match])\n    \n    def _calculate_visual_comfort(self, params: VideoParameters) -> float:\n        \"\"\"è®¡ç®—è§†è§‰èˆ’é€‚åº¦\"\"\"\n        # ç†æƒ³å‚æ•°èŒƒå›´\n        ideal_brightness = 0.2\n        ideal_color_temp = 3000\n        ideal_motion_speed = 0.3\n        \n        brightness_comfort = 1.0 - abs(params.brightness - ideal_brightness) / 0.5\n        color_comfort = 1.0 - abs(params.color_temperature - ideal_color_temp) / 1000\n        motion_comfort = 1.0 - abs(params.motion_speed - ideal_motion_speed) / 0.5\n        \n        return np.mean([brightness_comfort, color_comfort, motion_comfort])\n    \n    def _predict_outcomes(self, therapy_effect: float) -> Dict:\n        \"\"\"é¢„æµ‹æ²»ç–—ç»“æœ\"\"\"\n        if therapy_effect >= 0.85:\n            return {\n                'sleep_induction': 'ä¼˜ç§€',\n                'emotion_regulation': 'æ˜¾è‘—æ”¹å–„',\n                'stress_reduction': 'æ˜æ˜¾é™ä½',\n                'overall_satisfaction': 'éå¸¸æ»¡æ„'\n            }\n        elif therapy_effect >= 0.70:\n            return {\n                'sleep_induction': 'è‰¯å¥½',\n                'emotion_regulation': 'æœ‰æ•ˆæ”¹å–„',\n                'stress_reduction': 'é€‚åº¦é™ä½',\n                'overall_satisfaction': 'æ»¡æ„'\n            }\n        elif therapy_effect >= 0.55:\n            return {\n                'sleep_induction': 'ä¸€èˆ¬',\n                'emotion_regulation': 'è½»å¾®æ”¹å–„',\n                'stress_reduction': 'æœ‰é™é™ä½',\n                'overall_satisfaction': 'åŸºæœ¬æ»¡æ„'\n            }\n        else:\n            return {\n                'sleep_induction': 'éœ€æ”¹è¿›',\n                'emotion_regulation': 'æ•ˆæœæœ‰é™',\n                'stress_reduction': 'ä¸æ˜æ˜¾',\n                'overall_satisfaction': 'å¾…ä¼˜åŒ–'\n            }\n    \n    def _calculate_confidence(self, quality_metrics: Dict) -> float:\n        \"\"\"è®¡ç®—é¢„æµ‹ç½®ä¿¡åº¦\"\"\"\n        variance = np.var(list(quality_metrics.values()))\n        # æ–¹å·®è¶Šå°ï¼Œç½®ä¿¡åº¦è¶Šé«˜\n        confidence = 1.0 - min(variance, 0.5) / 0.5\n        return confidence\n\n# åˆå§‹åŒ–æ•ˆæœé¢„æµ‹å™¨\neffect_predictor = TherapyEffectPredictor()\n\n# é¢„æµ‹æ‰€æœ‰åœºæ™¯çš„æ²»ç–—æ•ˆæœ\ntherapy_predictions = []\n\nprint(\"ğŸ”® å¼€å§‹æ²»ç–—æ•ˆæœé¢„æµ‹åˆ†æ...\\n\")\n\nfor i, result in enumerate(video_results, 1):\n    prediction = effect_predictor.predict_therapy_effect(result)\n    therapy_predictions.append(prediction)\n    \n    scenario_name = result['scenario']['name']\n    effect_score = prediction['therapy_effect_score']\n    outcomes = prediction['predicted_outcomes']\n    confidence = prediction['confidence_level']\n    \n    print(f\"ğŸ“Š åœºæ™¯ {i}: {scenario_name}\")\n    print(f\"   ğŸ¯ æ²»ç–—æ•ˆæœå¾—åˆ†: {effect_score:.3f}\")\n    print(f\"   ğŸ˜´ ç¡çœ è¯±å¯¼: {outcomes['sleep_induction']}\")\n    print(f\"   ğŸ’­ æƒ…ç»ªè°ƒèŠ‚: {outcomes['emotion_regulation']}\")\n    print(f\"   ğŸ˜Œ å‹åŠ›ç¼“è§£: {outcomes['stress_reduction']}\")\n    print(f\"   ğŸ˜Š æ•´ä½“æ»¡æ„åº¦: {outcomes['overall_satisfaction']}\")\n    print(f\"   ğŸ² é¢„æµ‹ç½®ä¿¡åº¦: {confidence:.3f}\")\n    print()\n\n# ç»¼åˆåˆ†æ\navg_therapy_effect = np.mean([p['therapy_effect_score'] for p in therapy_predictions])\navg_confidence = np.mean([p['confidence_level'] for p in therapy_predictions])\n\nprint(f\"ğŸ“ˆ ç»¼åˆæ²»ç–—æ•ˆæœåˆ†æ:\")\nprint(f\"   â€¢ å¹³å‡æ²»ç–—æ•ˆæœå¾—åˆ†: {avg_therapy_effect:.3f}\")\nprint(f\"   â€¢ å¹³å‡é¢„æµ‹ç½®ä¿¡åº¦: {avg_confidence:.3f}\")\n\n# æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®æ²»ç–—æ•ˆæœåœºæ™¯\nbest_therapy_idx = np.argmax([p['therapy_effect_score'] for p in therapy_predictions])\nworst_therapy_idx = np.argmin([p['therapy_effect_score'] for p in therapy_predictions])\n\nbest_therapy_scenario = video_results[best_therapy_idx]['scenario']['name']\nworst_therapy_scenario = video_results[worst_therapy_idx]['scenario']['name']\n\nprint(f\"   â€¢ æœ€ä½³æ²»ç–—æ•ˆæœåœºæ™¯: {best_therapy_scenario}\")\nprint(f\"   â€¢ æœ€å·®æ²»ç–—æ•ˆæœåœºæ™¯: {worst_therapy_scenario}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. æ²»ç–—æ•ˆæœé¢„æµ‹ä¸ä¼˜åŒ–å»ºè®®",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# === 5. è§†é¢‘è´¨é‡å¯è§†åŒ–åˆ†æ ===\n\n# è®¾ç½®å›¾å½¢æ ·å¼\nplt.style.use('seaborn-v0_8')\nfig = plt.figure(figsize=(16, 12))\n\n# 1. è´¨é‡æŒ‡æ ‡é›·è¾¾å›¾\nax1 = plt.subplot(2, 3, 1, projection='polar')\n\n# è®¡ç®—å¹³å‡è´¨é‡æŒ‡æ ‡\nmetrics = ['visual_quality', 'motion_smoothness', 'color_harmony', 'therapeutic_alignment']\nmetric_names = ['è§†è§‰è´¨é‡', 'è¿åŠ¨æµç•…åº¦', 'è‰²å½©å’Œè°åº¦', 'æ²»ç–—å¯¹é½åº¦']\navg_scores = [quality_distribution[metric]['mean'] for metric in metrics]\n\n# ç»˜åˆ¶é›·è¾¾å›¾\nangles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\navg_scores += avg_scores[:1]  # é—­åˆå›¾å½¢\nangles += angles[:1]\n\nax1.plot(angles, avg_scores, 'o-', linewidth=2, color='#2E86C1')\nax1.fill(angles, avg_scores, alpha=0.25, color='#2E86C1')\nax1.set_xticks(angles[:-1])\nax1.set_xticklabels(metric_names)\nax1.set_ylim(0, 1)\nax1.set_title('è§†é¢‘è´¨é‡æŒ‡æ ‡é›·è¾¾å›¾', fontsize=12, pad=20)\nax1.grid(True)\n\n# 2. è´¨é‡åˆ†å¸ƒç®±çº¿å›¾\nax2 = plt.subplot(2, 3, 2)\nquality_scores = [r['quality_metrics']['overall_score'] for r in video_results]\nscenario_names = [r['scenario']['name'] for r in video_results]\n\nbox_plot = ax2.boxplot([quality_scores], labels=['æ€»ä½“è´¨é‡'], patch_artist=True)\nbox_plot['boxes'][0].set_facecolor('#85C1E9')\nax2.set_ylabel('è´¨é‡å¾—åˆ†')\nax2.set_title('è§†é¢‘è´¨é‡åˆ†å¸ƒ', fontsize=12)\nax2.grid(True, alpha=0.3)\n\n# æ·»åŠ æ•£ç‚¹\nax2.scatter([1] * len(quality_scores), quality_scores, alpha=0.6, color='#2E86C1')\n\n# 3. æƒ…ç»ªè½¬æ¢å¼ºåº¦ä¸è´¨é‡å…³ç³»\nax3 = plt.subplot(2, 3, 3)\ntransition_intensities = [r['emotion_analysis']['transition_intensity'] for r in video_results]\nquality_scores = [r['quality_metrics']['overall_score'] for r in video_results]\n\nscatter = ax3.scatter(transition_intensities, quality_scores, \n                     c=range(len(video_results)), cmap='viridis', \n                     s=100, alpha=0.7)\nax3.set_xlabel('æƒ…ç»ªè½¬æ¢å¼ºåº¦')\nax3.set_ylabel('è§†é¢‘è´¨é‡')\nax3.set_title('æƒ…ç»ªè½¬æ¢å¼ºåº¦ vs è§†é¢‘è´¨é‡', fontsize=12)\nax3.grid(True, alpha=0.3)\n\n# æ·»åŠ è¶‹åŠ¿çº¿\nz = np.polyfit(transition_intensities, quality_scores, 1)\np = np.poly1d(z)\nax3.plot(sorted(transition_intensities), p(sorted(transition_intensities)), \n         \"r--\", alpha=0.8, linewidth=2)\n\n# 4. è§†è§‰ä¸»é¢˜è´¨é‡å¯¹æ¯”\nax4 = plt.subplot(2, 3, 4)\ntheme_data = parameter_impact['theme_impact']\nthemes = list(theme_data.keys())\ntheme_scores = [theme_data[theme]['mean'] for theme in themes]\ntheme_names_cn = {\n    'nature': 'è‡ªç„¶æ™¯è§‚',\n    'water': 'æ°´æµæµ·æ´‹', \n    'celestial': 'å¤©ä½“å®‡å®™',\n    'abstract': 'æŠ½è±¡å›¾æ¡ˆ'\n}\ntheme_labels = [theme_names_cn.get(theme, theme) for theme in themes]\n\nbars = ax4.bar(theme_labels, theme_scores, color=['#52C41A', '#1890FF', '#722ED1', '#FA8C16'])\nax4.set_ylabel('å¹³å‡è´¨é‡å¾—åˆ†')\nax4.set_title('ä¸åŒè§†è§‰ä¸»é¢˜è´¨é‡å¯¹æ¯”', fontsize=12)\nax4.grid(True, alpha=0.3, axis='y')\n\n# æ·»åŠ æ•°å€¼æ ‡ç­¾\nfor bar, score in zip(bars, theme_scores):\n    height = bar.get_height()\n    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n             f'{score:.3f}', ha='center', va='bottom')\n\n# 5. ç¡çœ é˜¶æ®µè´¨é‡è¡¨ç°\nax5 = plt.subplot(2, 3, 5)\nstage_data = parameter_impact['stage_impact']\nstages = list(stage_data.keys())\nstage_scores = [stage_data[stage]['mean'] for stage in stages]\n\nbars = ax5.bar(stages, stage_scores, color=['#FF7875', '#FFA940', '#73D13D'])\nax5.set_ylabel('å¹³å‡è´¨é‡å¾—åˆ†')\nax5.set_title('ä¸åŒç¡çœ é˜¶æ®µè´¨é‡è¡¨ç°', fontsize=12)\nax5.grid(True, alpha=0.3, axis='y')\n\n# æ·»åŠ æ•°å€¼æ ‡ç­¾\nfor bar, score in zip(bars, stage_scores):\n    height = bar.get_height()\n    ax5.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n             f'{score:.3f}', ha='center', va='bottom')\n\n# 6. å„ç»´åº¦è´¨é‡è¯¦ç»†å¯¹æ¯”\nax6 = plt.subplot(2, 3, 6)\nx = np.arange(len(metric_names))\nwidth = 0.35\n\n# è®¡ç®—æœ€é«˜å’Œæœ€ä½è´¨é‡åœºæ™¯çš„æŒ‡æ ‡\nbest_result = max(video_results, key=lambda x: x['quality_metrics']['overall_score'])\nworst_result = min(video_results, key=lambda x: x['quality_metrics']['overall_score'])\n\nbest_scores = [best_result['quality_metrics'][metric] for metric in metrics]\nworst_scores = [worst_result['quality_metrics'][metric] for metric in metrics]\n\nbars1 = ax6.bar(x - width/2, best_scores, width, label='æœ€ä½³è´¨é‡', color='#52C41A', alpha=0.8)\nbars2 = ax6.bar(x + width/2, worst_scores, width, label='æœ€å·®è´¨é‡', color='#FF4D4F', alpha=0.8)\n\nax6.set_ylabel('è´¨é‡å¾—åˆ†')\nax6.set_title('æœ€ä½³ vs æœ€å·®è´¨é‡åœºæ™¯å¯¹æ¯”', fontsize=12)\nax6.set_xticks(x)\nax6.set_xticklabels(metric_names)\nax6.legend()\nax6.grid(True, alpha=0.3, axis='y')\n\n# è°ƒæ•´å¸ƒå±€\nplt.tight_layout()\nplt.show()\n\n# æ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯\nprint(\"\\nğŸ“Š è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯:\")\nprint(\"=\" * 50)\n\nprint(f\"\\nğŸ¯ æœ€ä½³è´¨é‡åœºæ™¯: {best_result['scenario']['name']}\")\nprint(f\"   è´¨é‡å¾—åˆ†: {best_result['quality_metrics']['overall_score']:.3f}\")\nprint(f\"   è§†è§‰ä¸»é¢˜: {best_result['parameters'].visual_theme.value}\")\nprint(f\"   è§†é¢‘é£æ ¼: {best_result['parameters'].style.value}\")\n\nprint(f\"\\nğŸ“‰ æœ€å·®è´¨é‡åœºæ™¯: {worst_result['scenario']['name']}\")\nprint(f\"   è´¨é‡å¾—åˆ†: {worst_result['quality_metrics']['overall_score']:.3f}\")\nprint(f\"   è§†è§‰ä¸»é¢˜: {worst_result['parameters'].visual_theme.value}\")\nprint(f\"   è§†é¢‘é£æ ¼: {worst_result['parameters'].style.value}\")\n\nprint(f\"\\nğŸ“ˆ è´¨é‡æ”¹è¿›ç©ºé—´:\")\nfor metric in metrics:\n    best_score = best_result['quality_metrics'][metric]\n    worst_score = worst_result['quality_metrics'][metric]\n    improvement = best_score - worst_score\n    metric_cn = metric_names[metrics.index(metric)]\n    print(f\"   â€¢ {metric_cn}: {improvement:.3f} (æ”¹è¿›æ½œåŠ›)\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. è§†é¢‘è´¨é‡å¯è§†åŒ–åˆ†æ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# === 4. è§†é¢‘è´¨é‡å¤šç»´åˆ†æ ===\n\nclass VideoQualityAnalyzer:\n    \"\"\"è§†é¢‘è´¨é‡åˆ†æå™¨\"\"\"\n    \n    def __init__(self):\n        self.quality_metrics = [\n            'visual_quality', 'motion_smoothness', \n            'color_harmony', 'therapeutic_alignment'\n        ]\n    \n    def analyze_quality_distribution(self, results: List[Dict]) -> Dict:\n        \"\"\"åˆ†æè´¨é‡åˆ†å¸ƒ\"\"\"\n        quality_data = {}\n        \n        for metric in self.quality_metrics:\n            values = [r['quality_metrics'][metric] for r in results]\n            quality_data[metric] = {\n                'values': values,\n                'mean': np.mean(values),\n                'std': np.std(values),\n                'min': np.min(values),\n                'max': np.max(values),\n                'median': np.median(values)\n            }\n        \n        return quality_data\n    \n    def analyze_parameter_impact(self, results: List[Dict]) -> Dict:\n        \"\"\"åˆ†æå‚æ•°å¯¹è´¨é‡çš„å½±å“\"\"\"\n        impact_analysis = {}\n        \n        # åˆ†æä¸åŒè§†è§‰ä¸»é¢˜çš„è´¨é‡è¡¨ç°\n        theme_quality = {}\n        for result in results:\n            theme = result['parameters'].visual_theme.value\n            quality = result['quality_metrics']['overall_score']\n            \n            if theme not in theme_quality:\n                theme_quality[theme] = []\n            theme_quality[theme].append(quality)\n        \n        for theme, qualities in theme_quality.items():\n            theme_quality[theme] = {\n                'mean': np.mean(qualities),\n                'count': len(qualities),\n                'std': np.std(qualities)\n            }\n        \n        # åˆ†æç¡çœ é˜¶æ®µçš„è´¨é‡è¡¨ç°\n        stage_quality = {}\n        for result in results:\n            stage = result['scenario']['sleep_stage']\n            quality = result['quality_metrics']['overall_score']\n            \n            if stage not in stage_quality:\n                stage_quality[stage] = []\n            stage_quality[stage].append(quality)\n        \n        for stage, qualities in stage_quality.items():\n            stage_quality[stage] = {\n                'mean': np.mean(qualities),\n                'count': len(qualities),\n                'std': np.std(qualities)\n            }\n        \n        return {\n            'theme_impact': theme_quality,\n            'stage_impact': stage_quality\n        }\n    \n    def generate_quality_report(self, results: List[Dict]) -> str:\n        \"\"\"ç”Ÿæˆè´¨é‡æŠ¥å‘Š\"\"\"\n        quality_dist = self.analyze_quality_distribution(results)\n        param_impact = self.analyze_parameter_impact(results)\n        \n        report = \"ğŸ“Š è§†é¢‘è´¨é‡åˆ†ææŠ¥å‘Š\\n\"\n        report += \"=\" * 50 + \"\\n\\n\"\n        \n        # æ€»ä½“è´¨é‡ç»Ÿè®¡\n        overall_scores = [r['quality_metrics']['overall_score'] for r in results]\n        report += f\"ğŸ¯ æ€»ä½“è´¨é‡è¯„ä¼°:\\n\"\n        report += f\"   â€¢ å¹³å‡å¾—åˆ†: {np.mean(overall_scores):.3f}\\n\"\n        report += f\"   â€¢ æ ‡å‡†å·®: {np.std(overall_scores):.3f}\\n\"\n        report += f\"   â€¢ æœ€é«˜å¾—åˆ†: {np.max(overall_scores):.3f}\\n\"\n        report += f\"   â€¢ æœ€ä½å¾—åˆ†: {np.min(overall_scores):.3f}\\n\\n\"\n        \n        # å„ç»´åº¦è´¨é‡åˆ†æ\n        report += \"ğŸ“ˆ å„ç»´åº¦è´¨é‡åˆ†æ:\\n\"\n        for metric, data in quality_dist.items():\n            metric_name = {\n                'visual_quality': 'è§†è§‰è´¨é‡',\n                'motion_smoothness': 'è¿åŠ¨æµç•…åº¦',\n                'color_harmony': 'è‰²å½©å’Œè°åº¦',\n                'therapeutic_alignment': 'æ²»ç–—å¯¹é½åº¦'\n            }.get(metric, metric)\n            \n            report += f\"   â€¢ {metric_name}: {data['mean']:.3f} (Â±{data['std']:.3f})\\n\"\n        \n        report += \"\\n\"\n        \n        # è§†è§‰ä¸»é¢˜å½±å“åˆ†æ\n        report += \"ğŸ¨ è§†è§‰ä¸»é¢˜è´¨é‡è¡¨ç°:\\n\"\n        for theme, data in param_impact['theme_impact'].items():\n            theme_name = {\n                'nature': 'è‡ªç„¶æ™¯è§‚',\n                'water': 'æ°´æµæµ·æ´‹',\n                'celestial': 'å¤©ä½“å®‡å®™',\n                'abstract': 'æŠ½è±¡å›¾æ¡ˆ'\n            }.get(theme, theme)\n            report += f\"   â€¢ {theme_name}: {data['mean']:.3f} ({data['count']}ä¸ªæ ·æœ¬)\\n\"\n        \n        report += \"\\n\"\n        \n        # ç¡çœ é˜¶æ®µå½±å“åˆ†æ\n        report += \"ğŸ˜´ ç¡çœ é˜¶æ®µè´¨é‡è¡¨ç°:\\n\"\n        for stage, data in param_impact['stage_impact'].items():\n            report += f\"   â€¢ {stage}: {data['mean']:.3f} ({data['count']}ä¸ªæ ·æœ¬)\\n\"\n        \n        return report\n\n# åˆå§‹åŒ–è´¨é‡åˆ†æå™¨\nquality_analyzer = VideoQualityAnalyzer()\n\n# æ‰§è¡Œè´¨é‡åˆ†æ\nquality_distribution = quality_analyzer.analyze_quality_distribution(video_results)\nparameter_impact = quality_analyzer.analyze_parameter_impact(video_results)\nquality_report = quality_analyzer.generate_quality_report(video_results)\n\n# æ˜¾ç¤ºåˆ†æç»“æœ\nprint(quality_report)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. è§†é¢‘è´¨é‡å¤šç»´åˆ†æ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# === 3. æƒ…ç»ªåœºæ™¯è§†é¢‘ç”Ÿæˆæµ‹è¯• ===\n\n# å®šä¹‰æµ‹è¯•åœºæ™¯\ntest_scenarios = [\n    {\n        'name': 'ç„¦è™‘â†’å¹³é™',\n        'description': 'å·¥ä½œç„¦è™‘è½¬å‘å†…å¿ƒå¹³é™',\n        'user_id': 'user_anxiety_001',\n        'current_emotion': {'valence': -0.3, 'arousal': 0.7},\n        'target_emotion': {'valence': 0.2, 'arousal': -0.4},\n        'sleep_stage': 'å…¥ç¡å‡†å¤‡',\n        'session_duration': 480\n    },\n    {\n        'name': 'æŠ‘éƒâ†’æ¸©æš–',\n        'description': 'æŠ‘éƒæƒ…ç»ªè½¬å‘æ¸©æš–å¸Œæœ›',\n        'user_id': 'user_depression_002',\n        'current_emotion': {'valence': -0.6, 'arousal': -0.3},\n        'target_emotion': {'valence': 0.3, 'arousal': -0.2},\n        'sleep_stage': 'æµ…ç¡çœ ',\n        'session_duration': 600\n    },\n    {\n        'name': 'æ„¤æ€’â†’å®é™',\n        'description': 'æ„¤æ€’æƒ…ç»ªè½¬å‘æ·±åº¦å®é™',\n        'user_id': 'user_anger_003',\n        'current_emotion': {'valence': -0.5, 'arousal': 0.8},\n        'target_emotion': {'valence': 0.1, 'arousal': -0.6},\n        'sleep_stage': 'æ·±ç¡çœ ',\n        'session_duration': 720\n    },\n    {\n        'name': 'å…´å¥‹â†’æ”¾æ¾',\n        'description': 'è¿‡åº¦å…´å¥‹è½¬å‘èˆ’é€‚æ”¾æ¾',\n        'user_id': 'user_excitement_004',\n        'current_emotion': {'valence': 0.6, 'arousal': 0.7},\n        'target_emotion': {'valence': 0.4, 'arousal': -0.3},\n        'sleep_stage': 'å…¥ç¡å‡†å¤‡',\n        'session_duration': 360\n    },\n    {\n        'name': 'å‹åŠ›â†’é‡Šç„¶',\n        'description': 'å·¥ä½œå‹åŠ›è½¬å‘å¿ƒç†é‡Šç„¶',\n        'user_id': 'user_stress_005',\n        'current_emotion': {'valence': -0.2, 'arousal': 0.5},\n        'target_emotion': {'valence': 0.3, 'arousal': -0.1},\n        'sleep_stage': 'æµ…ç¡çœ ',\n        'session_duration': 540\n    }\n]\n\n# ç”Ÿæˆè§†é¢‘æµ‹è¯•ç»“æœ\nvideo_results = []\n\nprint(\"ğŸ¬ å¼€å§‹æƒ…ç»ªåœºæ™¯è§†é¢‘ç”Ÿæˆæµ‹è¯•...\\n\")\n\nfor i, scenario in enumerate(test_scenarios, 1):\n    print(f\"ğŸ“¹ åœºæ™¯ {i}: {scenario['name']}\")\n    print(f\"   æè¿°: {scenario['description']}\")\n    \n    # åˆ›å»ºè§†é¢‘è¯·æ±‚\n    request = TherapeuticVideoRequest(\n        user_id=scenario['user_id'],\n        current_emotion=scenario['current_emotion'],\n        target_emotion=scenario['target_emotion'],\n        sleep_stage=scenario['sleep_stage'],\n        session_duration=scenario['session_duration']\n    )\n    \n    # ç”Ÿæˆè§†é¢‘\n    start_time = time.time()\n    result = video_generator.generate_video(request)\n    generation_time = time.time() - start_time\n    \n    # ä¿å­˜ç»“æœ\n    result['scenario'] = scenario\n    result['actual_generation_time'] = generation_time\n    video_results.append(result)\n    \n    # æ˜¾ç¤ºç”Ÿæˆä¿¡æ¯\n    params = result['parameters']\n    quality = result['quality_metrics']\n    \n    print(f\"   âœ… ç”Ÿæˆå®Œæˆ (è€—æ—¶: {generation_time:.2f}s)\")\n    print(f\"   ğŸ¨ è§†è§‰ä¸»é¢˜: {params.visual_theme.value}\")\n    print(f\"   ğŸ­ è§†é¢‘é£æ ¼: {params.style.value}\")\n    print(f\"   â±ï¸  æ—¶é•¿: {params.duration}ç§’\")\n    print(f\"   ğŸ’¡ äº®åº¦: {params.brightness:.2f}\")\n    print(f\"   ğŸŒ¡ï¸  è‰²æ¸©: {params.color_temperature:.0f}K\")\n    print(f\"   ğŸ“Š è´¨é‡è¯„åˆ†: {quality['overall_score']:.3f}\")\n    print()\n\nprint(f\"âœ… æ‰€æœ‰åœºæ™¯è§†é¢‘ç”Ÿæˆå®Œæˆï¼\")\nprint(f\"ğŸ“ˆ å¹³å‡ç”Ÿæˆæ—¶é—´: {np.mean([r['actual_generation_time'] for r in video_results]):.2f}ç§’\")\nprint(f\"ğŸ“Š å¹³å‡è´¨é‡è¯„åˆ†: {np.mean([r['quality_metrics']['overall_score'] for r in video_results]):.3f}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. æƒ…ç»ªåœºæ™¯è§†é¢‘ç”Ÿæˆæµ‹è¯•",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# === 2. è§†é¢‘ç”Ÿæˆç³»ç»Ÿæ¶æ„ ===\n\nclass VisualTheme(Enum):\n    \"\"\"è§†è§‰ä¸»é¢˜ç±»å‹\"\"\"\n    NATURE = \"nature\"  # è‡ªç„¶æ™¯è§‚\n    ABSTRACT = \"abstract\"  # æŠ½è±¡å›¾æ¡ˆ\n    CELESTIAL = \"celestial\"  # å¤©ä½“å®‡å®™\n    WATER = \"water\"  # æ°´æµæµ·æ´‹\n    FOREST = \"forest\"  # æ£®æ—æ¤ç‰©\n    GEOMETRIC = \"geometric\"  # å‡ ä½•å›¾å½¢\n    MANDALA = \"mandala\"  # æ›¼é™€ç½—å›¾æ¡ˆ\n    BREATHING = \"breathing\"  # å‘¼å¸å¼•å¯¼\n\nclass VideoStyle(Enum):\n    \"\"\"è§†é¢‘é£æ ¼\"\"\"\n    CALM = \"calm\"  # å¹³é™èˆ’ç¼“\n    FLOWING = \"flowing\"  # æµåŠ¨å˜åŒ–\n    PULSING = \"pulsing\"  # è„‰åŠ¨èŠ‚å¾‹\n    GRADIENT = \"gradient\"  # æ¸å˜è¿‡æ¸¡\n    PARTICLE = \"particle\"  # ç²’å­æ•ˆæœ\n    MORPHING = \"morphing\"  # å½¢æ€å˜æ¢\n\n@dataclass\nclass VideoParameters:\n    \"\"\"è§†é¢‘ç”Ÿæˆå‚æ•°\"\"\"\n    duration: int = 60  # æŒç»­æ—¶é—´(ç§’)\n    fps: int = 30  # å¸§ç‡\n    resolution: Tuple[int, int] = (1920, 1080)  # åˆ†è¾¨ç‡\n    visual_theme: VisualTheme = VisualTheme.NATURE\n    style: VideoStyle = VideoStyle.CALM\n    color_temperature: float = 3000.0  # è‰²æ¸© (K)\n    brightness: float = 0.3  # äº®åº¦ (0-1)\n    contrast: float = 0.6  # å¯¹æ¯”åº¦ (0-1)\n    motion_speed: float = 0.5  # è¿åŠ¨é€Ÿåº¦ (0-1)\n    breathing_sync: bool = True  # å‘¼å¸åŒæ­¥\n    breathing_rate: float = 6.0  # å‘¼å¸é¢‘ç‡ (æ¬¡/åˆ†é’Ÿ)\n\n@dataclass\nclass TherapeuticVideoRequest:\n    \"\"\"æ²»ç–—è§†é¢‘è¯·æ±‚\"\"\"\n    user_id: str\n    current_emotion: Dict[str, float]  # V-Aæƒ…ç»ªçŠ¶æ€\n    target_emotion: Dict[str, float]  # ç›®æ ‡æƒ…ç»ªçŠ¶æ€\n    sleep_stage: str  # ç¡çœ é˜¶æ®µ\n    session_duration: int = 600  # ä¼šè¯æ—¶é•¿(ç§’)\n    personalization: Dict[str, Any] = None  # ä¸ªæ€§åŒ–åå¥½\n    \nclass MockTherapeuticVideoGenerator:\n    \"\"\"æ¨¡æ‹Ÿæ²»ç–—è§†é¢‘ç”Ÿæˆå™¨\"\"\"\n    \n    def __init__(self):\n        self.model_name = \"hunyuan_video_therapy_v1\"\n        self.supported_themes = list(VisualTheme)\n        self.supported_styles = list(VideoStyle)\n        \n    def generate_video(self, request: TherapeuticVideoRequest) -> Dict[str, Any]:\n        \"\"\"ç”Ÿæˆæ²»ç–—è§†é¢‘\"\"\"\n        \n        # åˆ†ææƒ…ç»ªè½¬æ¢éœ€æ±‚\n        emotion_analysis = self._analyze_emotion_transition(\n            request.current_emotion, \n            request.target_emotion\n        )\n        \n        # ç”Ÿæˆè§†é¢‘å‚æ•°\n        video_params = self._generate_video_parameters(\n            emotion_analysis, \n            request.sleep_stage,\n            request.personalization\n        )\n        \n        # æ¨¡æ‹Ÿè§†é¢‘ç”Ÿæˆè¿‡ç¨‹\n        generation_time = np.random.uniform(30, 120)  # æ¨¡æ‹Ÿç”Ÿæˆæ—¶é—´\n        video_quality = self._simulate_video_generation(video_params)\n        \n        return {\n            'video_id': f\"therapy_video_{int(time.time())}\",\n            'parameters': video_params,\n            'emotion_analysis': emotion_analysis,\n            'generation_time': generation_time,\n            'quality_metrics': video_quality,\n            'therapeutic_features': self._extract_therapeutic_features(video_params)\n        }\n    \n    def _analyze_emotion_transition(self, current: Dict, target: Dict) -> Dict:\n        \"\"\"åˆ†ææƒ…ç»ªè½¬æ¢\"\"\"\n        valence_diff = target['valence'] - current['valence']\n        arousal_diff = target['arousal'] - current['arousal']\n        \n        transition_type = \"\"\n        if valence_diff > 0 and arousal_diff < 0:\n            transition_type = \"ç§¯ææ”¾æ¾\"\n        elif valence_diff > 0 and arousal_diff > 0:\n            transition_type = \"ç§¯ææ¿€æ´»\"\n        elif valence_diff < 0 and arousal_diff < 0:\n            transition_type = \"æ¶ˆæå¹³é™\"\n        else:\n            transition_type = \"æƒ…ç»ªç¨³å®š\"\n            \n        return {\n            'valence_change': valence_diff,\n            'arousal_change': arousal_diff,\n            'transition_type': transition_type,\n            'transition_intensity': np.sqrt(valence_diff**2 + arousal_diff**2),\n            'recommended_duration': max(300, abs(valence_diff + arousal_diff) * 200)\n        }\n    \n    def _generate_video_parameters(self, emotion_analysis: Dict, \n                                 sleep_stage: str, \n                                 personalization: Optional[Dict]) -> VideoParameters:\n        \"\"\"ç”Ÿæˆè§†é¢‘å‚æ•°\"\"\"\n        \n        # åŸºäºæƒ…ç»ªåˆ†æé€‰æ‹©è§†è§‰ä¸»é¢˜\n        theme_mapping = {\n            \"ç§¯ææ”¾æ¾\": VisualTheme.NATURE,\n            \"ç§¯ææ¿€æ´»\": VisualTheme.CELESTIAL,\n            \"æ¶ˆæå¹³é™\": VisualTheme.WATER,\n            \"æƒ…ç»ªç¨³å®š\": VisualTheme.ABSTRACT\n        }\n        \n        # åŸºäºç¡çœ é˜¶æ®µè°ƒæ•´å‚æ•°\n        if sleep_stage == \"å…¥ç¡å‡†å¤‡\":\n            brightness = 0.2\n            motion_speed = 0.3\n            breathing_rate = 6.0\n        elif sleep_stage == \"æµ…ç¡çœ \":\n            brightness = 0.1\n            motion_speed = 0.2\n            breathing_rate = 4.0\n        else:  # æ·±ç¡çœ \n            brightness = 0.05\n            motion_speed = 0.1\n            breathing_rate = 3.0\n            \n        return VideoParameters(\n            duration=emotion_analysis['recommended_duration'],\n            visual_theme=theme_mapping.get(emotion_analysis['transition_type'], VisualTheme.NATURE),\n            style=VideoStyle.FLOWING if emotion_analysis['transition_intensity'] > 0.5 else VideoStyle.CALM,\n            brightness=brightness,\n            motion_speed=motion_speed,\n            breathing_rate=breathing_rate,\n            color_temperature=3000 - emotion_analysis['arousal_change'] * 500\n        )\n    \n    def _simulate_video_generation(self, params: VideoParameters) -> Dict:\n        \"\"\"æ¨¡æ‹Ÿè§†é¢‘ç”Ÿæˆå¹¶è¯„ä¼°è´¨é‡\"\"\"\n        \n        # æ¨¡æ‹Ÿä¸åŒè´¨é‡æŒ‡æ ‡\n        visual_quality = np.random.normal(0.85, 0.1)  # è§†è§‰è´¨é‡\n        motion_smoothness = np.random.normal(0.82, 0.08)  # è¿åŠ¨æµç•…åº¦\n        color_harmony = np.random.normal(0.88, 0.05)  # è‰²å½©å’Œè°åº¦\n        therapeutic_alignment = np.random.normal(0.79, 0.12)  # æ²»ç–—å¯¹é½åº¦\n        \n        return {\n            'visual_quality': max(0, min(1, visual_quality)),\n            'motion_smoothness': max(0, min(1, motion_smoothness)),\n            'color_harmony': max(0, min(1, color_harmony)),\n            'therapeutic_alignment': max(0, min(1, therapeutic_alignment)),\n            'overall_score': np.mean([visual_quality, motion_smoothness, color_harmony, therapeutic_alignment])\n        }\n    \n    def _extract_therapeutic_features(self, params: VideoParameters) -> Dict:\n        \"\"\"æå–æ²»ç–—ç‰¹å¾\"\"\"\n        return {\n            'calming_elements': ['æŸ”å’Œè‰²å½©', 'ç¼“æ…¢è¿åŠ¨', 'è‡ªç„¶çº¹ç†'],\n            'sleep_induction': ['ä½äº®åº¦', 'å‘¼å¸åŒæ­¥', 'æ¸è¿›å¼è§†è§‰'],\n            'emotional_regulation': ['è‰²å½©ç–—æ³•', 'è§†è§‰èŠ‚å¾‹', 'ç©ºé—´æ·±åº¦'],\n            'attention_guidance': ['ç„¦ç‚¹è½¬ç§»', 'è§†è§‰å†¥æƒ³', 'è®¤çŸ¥åˆ†æ•£']\n        }\n\n# åˆå§‹åŒ–è§†é¢‘ç”Ÿæˆå™¨\nvideo_generator = MockTherapeuticVideoGenerator()\nprint(\"âœ… æ²»ç–—è§†é¢‘ç”Ÿæˆç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\")\nprint(f\"ğŸ“¹ æ”¯æŒçš„è§†è§‰ä¸»é¢˜: {[theme.value for theme in VisualTheme]}\")\nprint(f\"ğŸ¨ æ”¯æŒçš„è§†é¢‘é£æ ¼: {[style.value for style in VideoStyle]}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. è§†é¢‘ç”Ÿæˆç³»ç»Ÿæ¶æ„",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# === 1. ç³»ç»Ÿåˆå§‹åŒ–å’Œç¯å¢ƒé…ç½® ===\nimport sys\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport json\nimport time\nfrom datetime import datetime\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom enum import Enum\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# æ·»åŠ é¡¹ç›®è·¯å¾„\nproject_root = Path(__file__).parent.parent\nsys.path.append(str(project_root))\n\n# è®¾ç½®ä¸­æ–‡å­—ä½“\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\nprint(\"ğŸ“º ã€Šå¿ƒå¢ƒæµè½¬ã€‹è§†é¢‘ç”Ÿæˆå·¥ä½œå®¤ - ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\")\nprint(f\"ğŸ•’ å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(\"ğŸ¯ ç›®æ ‡: åŸºäºæƒ…ç»ªçš„æ²»ç–—è§†é¢‘ç”Ÿæˆä¸è´¨é‡è¯„ä¼°\")\nprint(\"=\"*60)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# ã€Šå¿ƒå¢ƒæµè½¬ã€‹è§†é¢‘ç”Ÿæˆå·¥ä½œå®¤\n## 07_video_generation_workshop.ipynb\n\n### å®éªŒç›®æ ‡\n- æµ‹è¯•åŸºäºæƒ…ç»ªçŠ¶æ€çš„æ²»ç–—è§†é¢‘ç”Ÿæˆ\n- éªŒè¯è§†è§‰å†…å®¹å¯¹ç¡çœ è¯±å¯¼çš„å½±å“\n- è¯„ä¼°è§†é¢‘è´¨é‡å’Œæ²»ç–—æ•ˆæœ\n- ä¼˜åŒ–è§†é¢‘ç”Ÿæˆå‚æ•°\n\n### æ ¸å¿ƒæŠ€æœ¯\n- HunyuanVideoæ¨¡å‹é€‚é…\n- æƒ…ç»ªé©±åŠ¨è§†è§‰å†…å®¹ç”Ÿæˆ\n- ç¡çœ ä¼˜åŒ–è§†è§‰æ¨¡å¼\n- å¤šç»´åº¦è§†é¢‘è´¨é‡è¯„ä¼°\n\n---\n\n**å®éªŒç¯å¢ƒ**: JupyterHub GPU ç¯å¢ƒ  \n**GPUè¦æ±‚**: 40-80GBæ˜¾å­˜  \n**æµ‹è¯•æ¨¡å¼**: æ¨¡æ‹Ÿç¯å¢ƒ (é¿å…å¤§æ¨¡å‹åŠ è½½)",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}