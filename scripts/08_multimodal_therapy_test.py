#!/usr/bin/env python3
"""
08 - å¤šæ¨¡æ€æ²»ç–—æµ‹è¯•
æµ‹è¯•éŸ³è§†é¢‘èåˆçš„ç¡çœ æ²»ç–—æ•ˆæœ
"""

import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from pathlib import Path
import json
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

class MultimodalSynchronizer:
    """å¤šæ¨¡æ€åŒæ­¥å™¨"""
    
    def __init__(self):
        self.audio_fps = 44100  # éŸ³é¢‘é‡‡æ ·ç‡
        self.video_fps = 30     # è§†é¢‘å¸§ç‡
        self.sync_tolerance = 0.033  # 33mså®¹å·®
    
    def calculate_sync_points(self, duration_seconds):
        """è®¡ç®—åŒæ­¥ç‚¹"""
        # å…³é”®åŒæ­¥æ—¶åˆ»
        sync_points = []
        
        # å¼€å§‹ç‚¹
        sync_points.append({
            "time": 0.0,
            "event": "start",
            "audio": "fade_in",
            "video": "fade_in"
        })
        
        # å‘¼å¸å¼•å¯¼ç‚¹ï¼ˆæ¯20ç§’ï¼‰
        for i in range(1, int(duration_seconds / 20) + 1):
            sync_points.append({
                "time": i * 20.0,
                "event": "breathing_cue",
                "audio": "breathing_sound",
                "video": "breathing_circle"
            })
        
        # ä¸­é—´è¿‡æ¸¡ç‚¹
        if duration_seconds > 60:
            sync_points.append({
                "time": duration_seconds / 2,
                "event": "transition",
                "audio": "key_change",
                "video": "pattern_change"
            })
        
        # ç»“æŸç‚¹
        sync_points.append({
            "time": duration_seconds - 10,
            "event": "ending",
            "audio": "fade_out",
            "video": "fade_out"
        })
        
        return sync_points
    
    def generate_sync_map(self, audio_events, video_events, duration):
        """ç”ŸæˆåŒæ­¥æ˜ å°„"""
        sync_map = []
        
        # å¯¹é½éŸ³è§†é¢‘äº‹ä»¶
        for audio_event in audio_events:
            best_match = None
            min_diff = float('inf')
            
            for video_event in video_events:
                time_diff = abs(audio_event['time'] - video_event['time'])
                if time_diff < min_diff and time_diff < self.sync_tolerance:
                    min_diff = time_diff
                    best_match = video_event
            
            if best_match:
                sync_map.append({
                    "time": audio_event['time'],
                    "audio_event": audio_event['type'],
                    "video_event": best_match['type'],
                    "sync_quality": 1.0 - (min_diff / self.sync_tolerance)
                })
        
        return sync_map

class TherapyEffectAnalyzer:
    """æ²»ç–—æ•ˆæœåˆ†æå™¨"""
    
    def __init__(self):
        self.metrics = {
            "relaxation_score": 0.0,
            "coherence_score": 0.0,
            "engagement_score": 0.0,
            "effectiveness_score": 0.0
        }
    
    def analyze_multimodal_effect(self, audio_features, video_features, sync_quality):
        """åˆ†æå¤šæ¨¡æ€æ²»ç–—æ•ˆæœ"""
        # æ”¾æ¾åº¦è¯„åˆ†
        audio_relaxation = self._calculate_audio_relaxation(audio_features)
        video_relaxation = self._calculate_video_relaxation(video_features)
        self.metrics["relaxation_score"] = (audio_relaxation + video_relaxation) / 2
        
        # åè°ƒæ€§è¯„åˆ†
        self.metrics["coherence_score"] = sync_quality * 0.8 + 0.2
        
        # å‚ä¸åº¦è¯„åˆ†
        variation_score = self._calculate_variation_score(audio_features, video_features)
        self.metrics["engagement_score"] = variation_score
        
        # æ•´ä½“æ•ˆæœè¯„åˆ†
        self.metrics["effectiveness_score"] = (
            self.metrics["relaxation_score"] * 0.4 +
            self.metrics["coherence_score"] * 0.3 +
            self.metrics["engagement_score"] * 0.3
        )
        
        return self.metrics
    
    def _calculate_audio_relaxation(self, features):
        """è®¡ç®—éŸ³é¢‘æ”¾æ¾åº¦"""
        # åŸºäºBPMã€éŸ³é‡å˜åŒ–ç­‰
        bpm_score = 1.0 - min(features.get("avg_bpm", 80) / 120, 1.0)
        volume_score = 1.0 - features.get("volume_variance", 0.5)
        
        return (bpm_score + volume_score) / 2
    
    def _calculate_video_relaxation(self, features):
        """è®¡ç®—è§†é¢‘æ”¾æ¾åº¦"""
        # åŸºäºäº®åº¦ã€è¿åŠ¨ç­‰
        brightness_score = 1.0 - min(features.get("avg_brightness", 50) / 100, 1.0)
        motion_score = 1.0 - min(features.get("motion_intensity", 0.5), 1.0)
        
        return (brightness_score + motion_score) / 2
    
    def _calculate_variation_score(self, audio_features, video_features):
        """è®¡ç®—å˜åŒ–åº¦è¯„åˆ†"""
        # é€‚åº¦çš„å˜åŒ–ä¿æŒæ³¨æ„åŠ›
        audio_var = audio_features.get("pattern_changes", 3)
        video_var = video_features.get("scene_changes", 2)
        
        optimal_changes = 4
        score = 1.0 - abs((audio_var + video_var) / 2 - optimal_changes) / optimal_changes
        
        return max(0, min(1, score))

class MultimodalTherapySession:
    """å¤šæ¨¡æ€æ²»ç–—ä¼šè¯"""
    
    def __init__(self, session_id):
        self.session_id = session_id
        self.synchronizer = MultimodalSynchronizer()
        self.analyzer = TherapyEffectAnalyzer()
        self.timeline = []
        
    def design_session(self, user_profile, duration_minutes=30):
        """è®¾è®¡æ²»ç–—ä¼šè¯"""
        print(f"\nğŸ¯ è®¾è®¡å¤šæ¨¡æ€æ²»ç–—æ–¹æ¡ˆ")
        print(f"æ—¶é•¿: {duration_minutes}åˆ†é’Ÿ")
        
        # ç”ŸæˆåŒæ­¥ç‚¹
        sync_points = self.synchronizer.calculate_sync_points(duration_minutes * 60)
        
        # è®¾è®¡éŸ³é¢‘è½¨é“
        audio_track = self._design_audio_track(user_profile, sync_points)
        
        # è®¾è®¡è§†é¢‘è½¨é“
        video_track = self._design_video_track(user_profile, sync_points)
        
        # åˆ›å»ºæ—¶é—´è½´
        self.timeline = self._create_timeline(audio_track, video_track, sync_points)
        
        return self.timeline
    
    def _design_audio_track(self, user_profile, sync_points):
        """è®¾è®¡éŸ³é¢‘è½¨é“"""
        track = {
            "type": "audio",
            "segments": []
        }
        
        # æ ¹æ®ç”¨æˆ·é—®é¢˜é€‰æ‹©éŸ³ä¹é£æ ¼
        if "å…¥ç¡å›°éš¾" in user_profile.get("issues", []):
            base_style = "ambient_slow"
            base_bpm = 50
        else:
            base_style = "nature_sounds"
            base_bpm = 60
        
        # åˆ›å»ºéŸ³é¢‘æ®µ
        for i in range(len(sync_points) - 1):
            start = sync_points[i]
            end = sync_points[i + 1]
            
            segment = {
                "start_time": start["time"],
                "end_time": end["time"],
                "style": base_style,
                "bpm": base_bpm - i * 5,  # é€æ¸å‡æ…¢
                "volume": 0.8 - i * 0.1,   # é€æ¸å‡å¼±
                "instruments": ["piano", "strings", "nature"],
                "effects": ["reverb", "gentle_eq"]
            }
            
            track["segments"].append(segment)
        
        return track
    
    def _design_video_track(self, user_profile, sync_points):
        """è®¾è®¡è§†é¢‘è½¨é“"""
        track = {
            "type": "video",
            "segments": []
        }
        
        # æ ¹æ®ç”¨æˆ·åå¥½é€‰æ‹©è§†è§‰é£æ ¼
        if user_profile.get("preferences", {}).get("visual") == "abstract":
            base_pattern = "mandala"
        else:
            base_pattern = "nature_scene"
        
        # åˆ›å»ºè§†é¢‘æ®µ
        patterns = ["breathing_circle", "gradient_flow", "wave_pattern", "mandala"]
        colors = ["ocean", "sunset", "forest", "lavender"]
        
        for i in range(len(sync_points) - 1):
            start = sync_points[i]
            end = sync_points[i + 1]
            
            segment = {
                "start_time": start["time"],
                "end_time": end["time"],
                "pattern": patterns[i % len(patterns)],
                "color_palette": colors[i % len(colors)],
                "brightness": 0.5 - i * 0.05,  # é€æ¸å˜æš—
                "motion_speed": "slow",
                "transitions": ["smooth_fade"]
            }
            
            track["segments"].append(segment)
        
        return track
    
    def _create_timeline(self, audio_track, video_track, sync_points):
        """åˆ›å»ºç»Ÿä¸€æ—¶é—´è½´"""
        timeline = {
            "total_duration": sync_points[-1]["time"],
            "sync_points": sync_points,
            "audio_track": audio_track,
            "video_track": video_track,
            "coordination": []
        }
        
        # æ·»åŠ åè°ƒäº‹ä»¶
        for point in sync_points:
            if point["event"] == "breathing_cue":
                timeline["coordination"].append({
                    "time": point["time"],
                    "type": "synchronized_breathing",
                    "audio_action": "breathing_rhythm",
                    "video_action": "breathing_visual"
                })
        
        return timeline
    
    def simulate_playback(self):
        """æ¨¡æ‹Ÿæ’­æ”¾è¿‡ç¨‹"""
        print("\nâ–¶ï¸ æ¨¡æ‹Ÿå¤šæ¨¡æ€æ’­æ”¾")
        print("-" * 40)
        
        # æ¨¡æ‹Ÿç‰¹å¾æå–
        audio_features = {
            "avg_bpm": 55,
            "volume_variance": 0.2,
            "pattern_changes": 3
        }
        
        video_features = {
            "avg_brightness": 30,
            "motion_intensity": 0.2,
            "scene_changes": 4
        }
        
        # æ¨¡æ‹ŸåŒæ­¥è´¨é‡
        sync_quality = 0.95
        
        # åˆ†ææ•ˆæœ
        effects = self.analyzer.analyze_multimodal_effect(
            audio_features, video_features, sync_quality
        )
        
        return effects

def run_multimodal_test():
    """è¿è¡Œå¤šæ¨¡æ€æ²»ç–—æµ‹è¯•"""
    print("ã€Šå¿ƒå¢ƒæµè½¬ã€‹å¤šæ¨¡æ€æ²»ç–—æµ‹è¯•")
    print("=" * 50)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
    
    # æµ‹è¯•ç”¨æˆ·é…ç½®
    test_users = [
        {
            "id": "user_001",
            "name": "è½»åº¦å¤±çœ ç”¨æˆ·",
            "issues": ["å…¥ç¡å›°éš¾"],
            "preferences": {"visual": "nature", "audio": "ambient"}
        },
        {
            "id": "user_002", 
            "name": "ç„¦è™‘å¤±çœ ç”¨æˆ·",
            "issues": ["å…¥ç¡å›°éš¾", "é¢‘ç¹é†’æ¥"],
            "preferences": {"visual": "abstract", "audio": "music"}
        },
        {
            "id": "user_003",
            "name": "é‡åº¦å¤±çœ ç”¨æˆ·",
            "issues": ["å¤±çœ ", "æ—©é†’", "å™©æ¢¦"],
            "preferences": {"visual": "mixed", "audio": "mixed"}
        }
    ]
    
    results = []
    
    for user in test_users:
        print(f"\n{'='*40}")
        print(f"ğŸ‘¤ æµ‹è¯•ç”¨æˆ·: {user['name']}")
        print(f"é—®é¢˜: {', '.join(user['issues'])}")
        
        # åˆ›å»ºä¼šè¯
        session = MultimodalTherapySession(f"session_{user['id']}")
        
        # è®¾è®¡æ–¹æ¡ˆ
        timeline = session.design_session(user, duration_minutes=20)
        
        # æ˜¾ç¤ºæ–¹æ¡ˆæ¦‚è§ˆ
        display_session_plan(timeline)
        
        # æ¨¡æ‹Ÿæ’­æ”¾
        effects = session.simulate_playback()
        
        # æ˜¾ç¤ºæ•ˆæœè¯„ä¼°
        display_effectiveness(effects)
        
        # è®°å½•ç»“æœ
        results.append({
            "user": user,
            "timeline": timeline,
            "effectiveness": effects
        })
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    generate_comparison_report(results)
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    save_test_results(results)
    
    return results

def display_session_plan(timeline):
    """æ˜¾ç¤ºä¼šè¯è®¡åˆ’"""
    print(f"\nğŸ“‹ æ²»ç–—æ–¹æ¡ˆæ¦‚è§ˆ")
    print(f"æ€»æ—¶é•¿: {timeline['total_duration']/60:.1f}åˆ†é’Ÿ")
    print(f"åŒæ­¥ç‚¹: {len(timeline['sync_points'])}ä¸ª")
    
    print("\néŸ³é¢‘è½¨é“:")
    for i, seg in enumerate(timeline['audio_track']['segments'][:3]):
        print(f"  æ®µ{i+1}: {seg['style']} @ {seg['bpm']}BPM")
    
    print("\nè§†é¢‘è½¨é“:")
    for i, seg in enumerate(timeline['video_track']['segments'][:3]):
        print(f"  æ®µ{i+1}: {seg['pattern']} - {seg['color_palette']}")

def display_effectiveness(effects):
    """æ˜¾ç¤ºæ•ˆæœè¯„ä¼°"""
    print(f"\nğŸ“Š æ²»ç–—æ•ˆæœè¯„ä¼°")
    print(f"æ”¾æ¾åº¦: {effects['relaxation_score']:.1%}")
    print(f"åè°ƒæ€§: {effects['coherence_score']:.1%}")
    print(f"å‚ä¸åº¦: {effects['engagement_score']:.1%}")
    print(f"æ•´ä½“æ•ˆæœ: {effects['effectiveness_score']:.1%}")

def generate_comparison_report(results):
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    print(f"\n{'='*50}")
    print("ğŸ“Š å¤šæ¨¡æ€æ²»ç–—æ•ˆæœå¯¹æ¯”")
    print("=" * 50)
    
    # åˆ›å»ºå¯¹æ¯”å›¾
    fig, ax = plt.subplots(figsize=(10, 6))
    
    users = [r['user']['name'] for r in results]
    metrics = ['relaxation_score', 'coherence_score', 'engagement_score', 'effectiveness_score']
    metric_names = ['æ”¾æ¾åº¦', 'åè°ƒæ€§', 'å‚ä¸åº¦', 'æ•´ä½“æ•ˆæœ']
    
    x = np.arange(len(users))
    width = 0.2
    
    for i, (metric, name) in enumerate(zip(metrics, metric_names)):
        values = [r['effectiveness'][metric] for r in results]
        ax.bar(x + i * width, values, width, label=name)
    
    ax.set_xlabel('ç”¨æˆ·ç±»å‹')
    ax.set_ylabel('è¯„åˆ†')
    ax.set_title('å¤šæ¨¡æ€æ²»ç–—æ•ˆæœå¯¹æ¯”')
    ax.set_xticks(x + width * 1.5)
    ax.set_xticklabels(users)
    ax.legend()
    ax.set_ylim(0, 1)
    
    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, alpha=0.3, axis='y')
    
    # ä¿å­˜å›¾è¡¨
    output_dir = Path("outputs/multimodal")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    chart_path = output_dir / "effectiveness_comparison.png"
    plt.tight_layout()
    plt.savefig(chart_path, dpi=150)
    plt.close()
    
    print(f"\nâœ… å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {chart_path}")

def save_test_results(results):
    """ä¿å­˜æµ‹è¯•ç»“æœ"""
    output_dir = Path("outputs/multimodal")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # å‡†å¤‡æ•°æ®
    test_data = {
        "timestamp": datetime.now().isoformat(),
        "test_type": "multimodal_therapy",
        "total_users": len(results),
        "results": []
    }
    
    for r in results:
        test_data["results"].append({
            "user": r["user"],
            "effectiveness": r["effectiveness"],
            "timeline_summary": {
                "duration": r["timeline"]["total_duration"],
                "sync_points": len(r["timeline"]["sync_points"]),
                "audio_segments": len(r["timeline"]["audio_track"]["segments"]),
                "video_segments": len(r["timeline"]["video_track"]["segments"])
            }
        })
    
    # ä¿å­˜æ–‡ä»¶
    output_file = output_dir / f"test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(test_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜: {output_file}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        # è¿è¡Œæµ‹è¯•
        results = run_multimodal_test()
        
        # ç”Ÿæˆå»ºè®®
        print("\nğŸ’¡ å¤šæ¨¡æ€æ²»ç–—ä¼˜åŒ–å»ºè®®")
        print("-" * 40)
        print("1. ç²¾ç¡®åŒæ­¥ï¼šç¡®ä¿éŸ³è§†é¢‘åœ¨å…³é”®æ—¶åˆ»å®Œç¾é…åˆ")
        print("2. ä¸ªæ€§åŒ–è°ƒæ•´ï¼šæ ¹æ®å®æ—¶åé¦ˆåŠ¨æ€è°ƒæ•´å‚æ•°")
        print("3. æ¸è¿›å¼è®¾è®¡ï¼šä»æ´»è·ƒåˆ°å¹³é™çš„å¹³æ»‘è¿‡æ¸¡")
        print("4. æ„Ÿå®˜å¹³è¡¡ï¼šé¿å…æŸä¸€æ¨¡æ€è¿‡äºçªå‡º")
        
        print("\nğŸ”¬ æŠ€æœ¯å®ç°è¦ç‚¹")
        print("-" * 40)
        print("1. ä½¿ç”¨æ—¶é—´ç ç¡®ä¿ç²¾ç¡®åŒæ­¥")
        print("2. å®ç°è·¨æ¨¡æ€ç‰¹å¾æå–å’Œåˆ†æ")
        print("3. å»ºç«‹ç”¨æˆ·åé¦ˆæœºåˆ¶")
        print("4. ä¼˜åŒ–èµ„æºä½¿ç”¨é¿å…å»¶è¿Ÿ")
        
        print("\n" + "=" * 50)
        print("å¤šæ¨¡æ€æ²»ç–—æµ‹è¯•å®Œæˆ")
        print("=" * 50)
        print(f"å®Œæˆæ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
        print("\nğŸš€ ä¸‹ä¸€æ­¥: è¿è¡Œ 09_performance_optimization.py")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()