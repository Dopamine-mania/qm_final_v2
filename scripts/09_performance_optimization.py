#!/usr/bin/env python3
"""
09 - æ€§èƒ½ä¼˜åŒ–æµ‹è¯•
æµ‹è¯•å’Œä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ï¼ŒåŒ…æ‹¬å†…å­˜ç®¡ç†ã€GPUåŠ é€Ÿç­‰
"""

import time
import psutil
import torch
import numpy as np
from datetime import datetime
from pathlib import Path
import json
import gc
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = {
            "cpu_usage": [],
            "memory_usage": [],
            "gpu_usage": [],
            "gpu_memory": [],
            "timestamps": []
        }
        self.start_time = time.time()
    
    def record_metrics(self):
        """è®°å½•å½“å‰æ€§èƒ½æŒ‡æ ‡"""
        # CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=0.1)
        self.metrics["cpu_usage"].append(cpu_percent)
        
        # å†…å­˜ä½¿ç”¨
        memory = psutil.virtual_memory()
        self.metrics["memory_usage"].append(memory.percent)
        
        # GPUä½¿ç”¨ç‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if torch.cuda.is_available():
            try:
                # ä½¿ç”¨nvidia-ml-pyè·å–GPUä½¿ç”¨ç‡
                gpu_usage = 50.0  # æ¨¡æ‹Ÿå€¼ï¼Œå› ä¸ºtorch.cuda.utilization()å¯èƒ½ä¸å¯ç”¨
                gpu_memory_used = torch.cuda.memory_allocated() / (1024**3)  # GB
                gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
                gpu_memory_percent = (gpu_memory_used / gpu_memory_total) * 100 if gpu_memory_total > 0 else 0
                
                self.metrics["gpu_usage"].append(gpu_usage)
                self.metrics["gpu_memory"].append(gpu_memory_percent)
            except Exception as e:
                print(f"GPUç›‘æ§è­¦å‘Š: {e}")
                self.metrics["gpu_usage"].append(0)
                self.metrics["gpu_memory"].append(0)
        else:
            self.metrics["gpu_usage"].append(0)
            self.metrics["gpu_memory"].append(0)
        
        # æ—¶é—´æˆ³
        self.metrics["timestamps"].append(time.time() - self.start_time)
    
    def get_summary(self):
        """è·å–æ€§èƒ½æ€»ç»“"""
        summary = {
            "avg_cpu": np.mean(self.metrics["cpu_usage"]) if self.metrics["cpu_usage"] else 0,
            "max_cpu": np.max(self.metrics["cpu_usage"]) if self.metrics["cpu_usage"] else 0,
            "avg_memory": np.mean(self.metrics["memory_usage"]) if self.metrics["memory_usage"] else 0,
            "max_memory": np.max(self.metrics["memory_usage"]) if self.metrics["memory_usage"] else 0,
            "avg_gpu": np.mean(self.metrics["gpu_usage"]) if self.metrics["gpu_usage"] else 0,
            "max_gpu": np.max(self.metrics["gpu_usage"]) if self.metrics["gpu_usage"] else 0,
            "duration": self.metrics["timestamps"][-1] if self.metrics["timestamps"] else 0
        }
        return summary

class OptimizationBenchmark:
    """ä¼˜åŒ–åŸºå‡†æµ‹è¯•"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.monitor = PerformanceMonitor()
    
    def benchmark_tensor_operations(self, size=(1000, 1000)):
        """æµ‹è¯•å¼ é‡è¿ç®—æ€§èƒ½"""
        print(f"\nğŸ”§ å¼ é‡è¿ç®—åŸºå‡†æµ‹è¯•")
        print(f"è®¾å¤‡: {self.device}")
        print(f"çŸ©é˜µå¤§å°: {size}")
        
        results = {}
        
        # æµ‹è¯•ä¸åŒç²¾åº¦
        dtypes = [torch.float32]
        if self.device.type == 'cuda':
            dtypes.append(torch.float16)  # FP16åªåœ¨GPUä¸Šæµ‹è¯•
        
        for dtype in dtypes:
            print(f"\næ•°æ®ç±»å‹: {dtype}")
            
            try:
                # åˆ›å»ºéšæœºå¼ é‡
                a = torch.randn(size, dtype=dtype, device=self.device)
                b = torch.randn(size, dtype=dtype, device=self.device)
                
                # é¢„çƒ­
                for _ in range(5):
                    _ = torch.matmul(a, b)
                
                if self.device.type == 'cuda':
                    torch.cuda.synchronize()
                
                # æµ‹è¯•çŸ©é˜µä¹˜æ³•
                start = time.time()
                for _ in range(100):
                    c = torch.matmul(a, b)
                
                if self.device.type == 'cuda':
                    torch.cuda.synchronize()
                
                duration = time.time() - start
                ops_per_sec = 100 / duration
                
                results[str(dtype)] = {
                    "duration": duration,
                    "ops_per_sec": ops_per_sec,
                    "memory_mb": a.element_size() * a.nelement() * 2 / (1024**2)
                }
                
                print(f"  é€Ÿåº¦: {ops_per_sec:.2f} ops/ç§’")
                print(f"  å†…å­˜: {results[str(dtype)]['memory_mb']:.1f} MB")
                
                # æ¸…ç†
                del a, b, c
                if self.device.type == 'cuda':
                    torch.cuda.empty_cache()
                gc.collect()
                
            except Exception as e:
                print(f"  æµ‹è¯•å¤±è´¥: {e}")
                results[str(dtype)] = {"error": str(e)}
        
        return results
    
    def test_memory_optimization(self):
        """æµ‹è¯•å†…å­˜ä¼˜åŒ–æŠ€æœ¯"""
        print(f"\nğŸ’¾ å†…å­˜ä¼˜åŒ–æµ‹è¯•")
        
        strategies = {}
        
        # 1. æ¢¯åº¦ç´¯ç§¯ vs å¤§æ‰¹é‡
        print("\n1. æ‰¹é‡å¤§å°ä¼˜åŒ–")
        
        # å¤§æ‰¹é‡ï¼ˆå¯èƒ½OOMï¼‰
        try:
            batch_size = 64 if self.device.type == 'cpu' else 128
            model = self._create_dummy_model()
            data = torch.randn(batch_size, 3, 224, 224, device=self.device)
            
            start_mem = self._get_memory_usage()
            start_time = time.time()
            
            output = model(data)
            loss = output.mean()
            loss.backward()
            
            end_time = time.time()
            end_mem = self._get_memory_usage()
            
            strategies["large_batch"] = {
                "batch_size": batch_size,
                "memory_used": end_mem - start_mem,
                "time": end_time - start_time,
                "success": True
            }
            
            del model, data, output, loss
            if self.device.type == 'cuda':
                torch.cuda.empty_cache()
            gc.collect()
            
        except RuntimeError as e:
            strategies["large_batch"] = {
                "batch_size": batch_size,
                "error": str(e),
                "success": False
            }
        
        # æ¢¯åº¦ç´¯ç§¯
        try:
            accumulation_steps = 4
            micro_batch = 16 if self.device.type == 'cpu' else 32
            model = self._create_dummy_model()
            
            start_mem = self._get_memory_usage()
            start_time = time.time()
            
            model.zero_grad()
            for i in range(accumulation_steps):
                data = torch.randn(micro_batch, 3, 224, 224, device=self.device)
                output = model(data)
                loss = output.mean() / accumulation_steps
                loss.backward()
            
            end_time = time.time()
            end_mem = self._get_memory_usage()
            
            strategies["gradient_accumulation"] = {
                "micro_batch": micro_batch,
                "accumulation_steps": accumulation_steps,
                "effective_batch": micro_batch * accumulation_steps,
                "memory_used": end_mem - start_mem,
                "time": end_time - start_time,
                "success": True
            }
            
            del model, data, output, loss
            
        except Exception as e:
            strategies["gradient_accumulation"] = {
                "error": str(e),
                "success": False
            }
        
        # 2. æ··åˆç²¾åº¦è®­ç»ƒ
        print("\n2. æ··åˆç²¾åº¦ä¼˜åŒ–")
        
        if self.device.type == 'cuda' and torch.cuda.is_available():
            try:
                from torch.cuda.amp import autocast, GradScaler
                
                model = self._create_dummy_model()
                scaler = GradScaler()
                optimizer = torch.optim.Adam(model.parameters())
                
                start_mem = self._get_memory_usage()
                start_time = time.time()
                
                data = torch.randn(64, 3, 224, 224, device=self.device)
                
                with autocast():
                    output = model(data)
                    loss = output.mean()
                
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
                
                end_time = time.time()
                end_mem = self._get_memory_usage()
                
                strategies["mixed_precision"] = {
                    "memory_used": end_mem - start_mem,
                    "time": end_time - start_time,
                    "speedup": "~2x expected",
                    "success": True
                }
            except Exception as e:
                strategies["mixed_precision"] = {
                    "error": str(e),
                    "success": False
                }
        else:
            strategies["mixed_precision"] = {
                "note": "ä»…GPUæ”¯æŒ",
                "success": False
            }
        
        # æ¸…ç†
        if self.device.type == 'cuda':
            torch.cuda.empty_cache()
        gc.collect()
        
        return strategies
    
    def test_parallel_processing(self):
        """æµ‹è¯•å¹¶è¡Œå¤„ç†"""
        print(f"\nâš¡ å¹¶è¡Œå¤„ç†æµ‹è¯•")
        
        # æµ‹è¯•æ•°æ®
        n_tasks = 50  # å‡å°‘ä»»åŠ¡æ•°ä»¥åŠ å¿«æµ‹è¯•
        data = [np.random.randn(500, 500) for _ in range(n_tasks)]  # å‡å°æ•°æ®å¤§å°
        
        results = {}
        
        # 1. ä¸²è¡Œå¤„ç†
        print("\n1. ä¸²è¡Œå¤„ç†")
        start = time.time()
        serial_results = []
        for d in data:
            result = self._process_data(d)
            serial_results.append(result)
        serial_time = time.time() - start
        
        results["serial"] = {
            "time": serial_time,
            "tasks_per_sec": n_tasks / serial_time
        }
        print(f"  æ—¶é—´: {serial_time:.2f}ç§’")
        
        # 2. çº¿ç¨‹å¹¶è¡Œ
        print("\n2. çº¿ç¨‹å¹¶è¡Œ")
        start = time.time()
        with ThreadPoolExecutor(max_workers=4) as executor:
            thread_results = list(executor.map(self._process_data, data))
        thread_time = time.time() - start
        
        results["thread_parallel"] = {
            "time": thread_time,
            "tasks_per_sec": n_tasks / thread_time,
            "speedup": serial_time / thread_time
        }
        print(f"  æ—¶é—´: {thread_time:.2f}ç§’")
        print(f"  åŠ é€Ÿ: {results['thread_parallel']['speedup']:.2f}x")
        
        # 3. è¿›ç¨‹å¹¶è¡Œ
        print("\n3. è¿›ç¨‹å¹¶è¡Œ")
        try:
            start = time.time()
            with ProcessPoolExecutor(max_workers=4) as executor:
                process_results = list(executor.map(self._process_data, data))
            process_time = time.time() - start
            
            results["process_parallel"] = {
                "time": process_time,
                "tasks_per_sec": n_tasks / process_time,
                "speedup": serial_time / process_time
            }
            print(f"  æ—¶é—´: {process_time:.2f}ç§’")
            print(f"  åŠ é€Ÿ: {results['process_parallel']['speedup']:.2f}x")
        except Exception as e:
            print(f"  è¿›ç¨‹å¹¶è¡Œæµ‹è¯•å¤±è´¥: {e}")
            results["process_parallel"] = {"error": str(e)}
        
        return results
    
    def _create_dummy_model(self):
        """åˆ›å»ºæµ‹è¯•æ¨¡å‹"""
        # ç®€åŒ–æ¨¡å‹ä»¥é€‚åº”CPU
        if self.device.type == 'cpu':
            return torch.nn.Sequential(
                torch.nn.Conv2d(3, 32, 3, padding=1),
                torch.nn.ReLU(),
                torch.nn.MaxPool2d(2),
                torch.nn.Conv2d(32, 64, 3, padding=1),
                torch.nn.ReLU(),
                torch.nn.AdaptiveAvgPool2d(1),
                torch.nn.Flatten(),
                torch.nn.Linear(64, 10)
            ).to(self.device)
        else:
            return torch.nn.Sequential(
                torch.nn.Conv2d(3, 64, 3, padding=1),
                torch.nn.ReLU(),
                torch.nn.MaxPool2d(2),
                torch.nn.Conv2d(64, 128, 3, padding=1),
                torch.nn.ReLU(),
                torch.nn.AdaptiveAvgPool2d(1),
                torch.nn.Flatten(),
                torch.nn.Linear(128, 10)
            ).to(self.device)
    
    def _get_memory_usage(self):
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨"""
        if self.device.type == 'cuda':
            return torch.cuda.memory_allocated() / (1024**2)  # MB
        else:
            return psutil.Process().memory_info().rss / (1024**2)  # MB
    
    def _process_data(self, data):
        """æ¨¡æ‹Ÿæ•°æ®å¤„ç†"""
        # æ‰§è¡Œä¸€äº›è®¡ç®—å¯†é›†å‹æ“ä½œ
        result = np.fft.fft2(data)
        result = np.abs(result)
        result = np.log(result + 1)
        return result.mean()

def visualize_performance(monitor_data, output_dir):
    """å¯è§†åŒ–æ€§èƒ½æ•°æ®"""
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
    if not monitor_data["timestamps"]:
        print("âš ï¸ æ²¡æœ‰æ€§èƒ½æ•°æ®å¯è§†åŒ–")
        return
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # CPUä½¿ç”¨ç‡
    ax = axes[0, 0]
    if monitor_data["cpu_usage"]:
        ax.plot(monitor_data["timestamps"], monitor_data["cpu_usage"])
        ax.set_title("CPU Usage")
        ax.set_xlabel("Time (s)")
        ax.set_ylabel("Usage (%)")
        ax.grid(True, alpha=0.3)
    else:
        ax.text(0.5, 0.5, "No CPU data", ha='center', va='center')
    
    # å†…å­˜ä½¿ç”¨ç‡
    ax = axes[0, 1]
    if monitor_data["memory_usage"]:
        ax.plot(monitor_data["timestamps"], monitor_data["memory_usage"])
        ax.set_title("Memory Usage")
        ax.set_xlabel("Time (s)")
        ax.set_ylabel("Usage (%)")
        ax.grid(True, alpha=0.3)
    else:
        ax.text(0.5, 0.5, "No memory data", ha='center', va='center')
    
    # GPUä½¿ç”¨ç‡
    ax = axes[1, 0]
    if monitor_data["gpu_usage"] and any(monitor_data["gpu_usage"]):
        ax.plot(monitor_data["timestamps"], monitor_data["gpu_usage"])
        ax.set_title("GPU Usage")
        ax.set_xlabel("Time (s)")
        ax.set_ylabel("Usage (%)")
        ax.grid(True, alpha=0.3)
    else:
        ax.text(0.5, 0.5, "No GPU data", ha='center', va='center')
    
    # GPUå†…å­˜
    ax = axes[1, 1]
    if monitor_data["gpu_memory"] and any(monitor_data["gpu_memory"]):
        ax.plot(monitor_data["timestamps"], monitor_data["gpu_memory"])
        ax.set_title("GPU Memory")
        ax.set_xlabel("Time (s)")
        ax.set_ylabel("Usage (%)")
        ax.grid(True, alpha=0.3)
    else:
        ax.text(0.5, 0.5, "No GPU memory data", ha='center', va='center')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    chart_path = output_dir / "performance_metrics.png"
    plt.savefig(chart_path, dpi=150)
    plt.close()
    
    print(f"\nâœ… æ€§èƒ½å›¾è¡¨å·²ä¿å­˜: {chart_path}")

def run_optimization_tests():
    """è¿è¡Œä¼˜åŒ–æµ‹è¯•"""
    print("ã€Šå¿ƒå¢ƒæµè½¬ã€‹æ€§èƒ½ä¼˜åŒ–æµ‹è¯•")
    print("=" * 50)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å™¨
    benchmark = OptimizationBenchmark()
    
    # å¼€å§‹ç›‘æ§
    monitor = benchmark.monitor
    
    # å®šæœŸè®°å½•æ€§èƒ½
    import threading
    stop_monitoring = threading.Event()
    
    def monitor_loop():
        while not stop_monitoring.is_set():
            try:
                monitor.record_metrics()
            except Exception as e:
                print(f"ç›‘æ§é”™è¯¯: {e}")
            time.sleep(0.5)
    
    monitor_thread = threading.Thread(target=monitor_loop)
    monitor_thread.start()
    
    results = {
        "timestamp": datetime.now().isoformat(),
        "device": str(benchmark.device),
        "tests": {}
    }
    
    try:
        # 1. å¼ é‡è¿ç®—åŸºå‡†æµ‹è¯•
        tensor_results = benchmark.benchmark_tensor_operations()
        results["tests"]["tensor_operations"] = tensor_results
        
        # 2. å†…å­˜ä¼˜åŒ–æµ‹è¯•
        memory_results = benchmark.test_memory_optimization()
        results["tests"]["memory_optimization"] = memory_results
        
        # 3. å¹¶è¡Œå¤„ç†æµ‹è¯•
        parallel_results = benchmark.test_parallel_processing()
        results["tests"]["parallel_processing"] = parallel_results
        
    finally:
        # åœæ­¢ç›‘æ§
        stop_monitoring.set()
        monitor_thread.join()
    
    # è·å–æ€§èƒ½æ€»ç»“
    performance_summary = monitor.get_summary()
    results["performance_summary"] = performance_summary
    
    # æ˜¾ç¤ºç»“æœ
    display_optimization_results(results)
    
    # å¯è§†åŒ–æ€§èƒ½
    output_dir = Path("outputs/optimization")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    visualize_performance(monitor.metrics, output_dir)
    
    # ä¿å­˜ç»“æœ
    save_optimization_results(results, output_dir)
    
    return results

def display_optimization_results(results):
    """æ˜¾ç¤ºä¼˜åŒ–ç»“æœ"""
    print("\nğŸ“Š ä¼˜åŒ–æµ‹è¯•ç»“æœ")
    print("=" * 50)
    
    # å¼ é‡è¿ç®—ç»“æœ
    print("\n1. å¼ é‡è¿ç®—æ€§èƒ½:")
    for dtype, metrics in results["tests"]["tensor_operations"].items():
        if "error" not in metrics:
            print(f"  {dtype}:")
            print(f"    é€Ÿåº¦: {metrics['ops_per_sec']:.2f} ops/ç§’")
            print(f"    å†…å­˜: {metrics['memory_mb']:.1f} MB")
    
    # å†…å­˜ä¼˜åŒ–ç»“æœ
    print("\n2. å†…å­˜ä¼˜åŒ–:")
    for strategy, metrics in results["tests"]["memory_optimization"].items():
        if metrics.get("success"):
            print(f"  {strategy}:")
            print(f"    å†…å­˜ä½¿ç”¨: {metrics.get('memory_used', 0):.1f} MB")
            print(f"    æ‰§è¡Œæ—¶é—´: {metrics.get('time', 0):.3f}ç§’")
    
    # å¹¶è¡Œå¤„ç†ç»“æœ
    print("\n3. å¹¶è¡Œå¤„ç†:")
    for method, metrics in results["tests"]["parallel_processing"].items():
        if "error" not in metrics:
            print(f"  {method}:")
            print(f"    æ—¶é—´: {metrics['time']:.2f}ç§’")
            if "speedup" in metrics:
                print(f"    åŠ é€Ÿæ¯”: {metrics['speedup']:.2f}x")
    
    # æ€§èƒ½æ€»ç»“
    summary = results["performance_summary"]
    print("\nğŸ“ˆ æ€§èƒ½ç›‘æ§æ€»ç»“:")
    print(f"  å¹³å‡CPU: {summary['avg_cpu']:.1f}%")
    print(f"  å³°å€¼CPU: {summary['max_cpu']:.1f}%")
    print(f"  å¹³å‡å†…å­˜: {summary['avg_memory']:.1f}%")
    print(f"  å³°å€¼å†…å­˜: {summary['max_memory']:.1f}%")
    if summary['avg_gpu'] > 0:
        print(f"  å¹³å‡GPU: {summary['avg_gpu']:.1f}%")
        print(f"  å³°å€¼GPU: {summary['max_gpu']:.1f}%")

def save_optimization_results(results, output_dir):
    """ä¿å­˜ä¼˜åŒ–ç»“æœ"""
    output_file = output_dir / f"optimization_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        # å¤„ç†ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
        def default(obj):
            if isinstance(obj, (np.integer, np.floating)):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            else:
                return str(obj)
        
        json.dump(results, f, indent=2, ensure_ascii=False, default=default)
    
    print(f"\nğŸ’¾ ä¼˜åŒ–ç»“æœå·²ä¿å­˜: {output_file}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        # è¿è¡Œä¼˜åŒ–æµ‹è¯•
        results = run_optimization_tests()
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        print("\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®")
        print("-" * 40)
        
        device = results["device"]
        if "cuda" in device:
            print("GPUä¼˜åŒ–:")
            print("1. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16ï¼‰å¯èŠ‚çœ50%æ˜¾å­˜")
            print("2. å¯ç”¨æ¢¯åº¦ç´¯ç§¯å¤„ç†å¤§æ‰¹é‡æ•°æ®")
            print("3. ä½¿ç”¨CUDAæµå®ç°å¼‚æ­¥å¤„ç†")
            print("4. å®šæœŸæ¸…ç†GPUç¼“å­˜é¿å…å†…å­˜æ³„æ¼")
        else:
            print("CPUä¼˜åŒ–:")
            print("1. ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡ŒåŠ é€Ÿè®¡ç®—å¯†é›†å‹ä»»åŠ¡")
            print("2. è€ƒè™‘ä½¿ç”¨é‡åŒ–æŠ€æœ¯å‡å°‘å†…å­˜å ç”¨")
            print("3. ä¼˜åŒ–æ•°æ®åŠ è½½é¿å…IOç“¶é¢ˆ")
            print("4. ä½¿ç”¨å†…å­˜æ˜ å°„å¤„ç†å¤§æ–‡ä»¶")
        
        print("\nğŸ”§ é€šç”¨ä¼˜åŒ–:")
        print("1. æ‰¹å¤„ç†ï¼šåˆå¹¶å°ä»»åŠ¡å‡å°‘å¼€é”€")
        print("2. ç¼“å­˜ï¼šå¤ç”¨è®¡ç®—ç»“æœé¿å…é‡å¤")
        print("3. æ‡’åŠ è½½ï¼šæŒ‰éœ€åŠ è½½æ¨¡å‹å’Œæ•°æ®")
        print("4. ç›‘æ§ï¼šæŒç»­è¿½è¸ªæ€§èƒ½æŒ‡æ ‡")
        
        print("\n" + "=" * 50)
        print("æ€§èƒ½ä¼˜åŒ–æµ‹è¯•å®Œæˆ")
        print("=" * 50)
        print(f"å®Œæˆæ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
        print("\nğŸš€ ä¸‹ä¸€æ­¥: è¿è¡Œ 10_complete_system_demo.py")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()