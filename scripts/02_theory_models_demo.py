#!/usr/bin/env python3
"""
02 - ç†è®ºæ¨¡å‹æ¼”ç¤º
æ¼”ç¤ºã€Šå¿ƒå¢ƒæµè½¬ã€‹ç³»ç»Ÿçš„æ ¸å¿ƒç†è®º:
- ISOä¸‰é˜¶æ®µæ²»ç–—åŸåˆ™
- Valence-Arousalæƒ…ç»ªæ¨¡å‹
- éŸ³ä¹æ²»ç–—å‚æ•°æ¨è
"""

import numpy as np
import matplotlib.pyplot as plt
from dataclasses import dataclass
from enum import Enum
from datetime import datetime
import json
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# é…ç½®matplotlibæ”¯æŒä¸­æ–‡
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

@dataclass
class EmotionState:
    """æƒ…ç»ªçŠ¶æ€æ•°æ®ç±»"""
    valence: float  # -1åˆ°1
    arousal: float  # -1åˆ°1
    confidence: float = 0.8
    
    def distance_to(self, other):
        """è®¡ç®—åˆ°å¦ä¸€ä¸ªæƒ…ç»ªçŠ¶æ€çš„æ¬§æ°è·ç¦»"""
        return ((self.valence - other.valence)**2 + (self.arousal - other.arousal)**2)**0.5

class ISOStage(Enum):
    """ISOæ²»ç–—é˜¶æ®µæšä¸¾"""
    SYNC = "åŒæ­¥åŒ–"
    GUIDE = "å¼•å¯¼åŒ–" 
    CONSOLIDATE = "å·©å›ºåŒ–"

class ISOModel:
    """ISOä¸‰é˜¶æ®µæ²»ç–—æ¨¡å‹"""
    
    def __init__(self):
        print("ğŸµ ISOä¸‰é˜¶æ®µæ²»ç–—æ¨¡å‹åˆå§‹åŒ–")
    
    def plan_stages(self, current, target, duration):
        """è§„åˆ’ä¸‰ä¸ªæ²»ç–—é˜¶æ®µ"""
        return [
            {'stage': ISOStage.SYNC, 'duration': duration * 0.25, 'emotion': current},
            {'stage': ISOStage.GUIDE, 'duration': duration * 0.50, 
             'emotion': EmotionState((current.valence + target.valence)/2, 
                                   (current.arousal + target.arousal)/2)},
            {'stage': ISOStage.CONSOLIDATE, 'duration': duration * 0.25, 'emotion': target}
        ]
    
    def generate_trajectory(self, current, target, duration, points=50):
        """ç”Ÿæˆæƒ…ç»ªå˜åŒ–è½¨è¿¹"""
        trajectory = []
        for i in range(points):
            progress = i / (points - 1)
            # Så‹å¹³æ»‘æ›²çº¿
            smooth = 3 * progress**2 - 2 * progress**3
            
            valence = current.valence + (target.valence - current.valence) * smooth
            arousal = current.arousal + (target.arousal - current.arousal) * smooth
            
            if progress < 0.25:
                stage = ISOStage.SYNC
            elif progress < 0.75:
                stage = ISOStage.GUIDE
            else:
                stage = ISOStage.CONSOLIDATE
            
            trajectory.append({
                'time': progress * duration,
                'emotion': EmotionState(valence, arousal),
                'stage': stage
            })
        return trajectory

class MusicModel:
    """éŸ³ä¹æ²»ç–—å‚æ•°æ¨¡å‹"""
    
    def __init__(self):
        print("ğŸ¼ éŸ³ä¹æ²»ç–—æ¨¡å‹åˆå§‹åŒ–")
    
    def calc_bpm(self, arousal):
        """æ ¹æ®å”¤é†’åº¦è®¡ç®—BPM"""
        # å”¤é†’åº¦æ˜ å°„åˆ°BPM: -1â†’40, 0â†’80, 1â†’120
        return 80 + (arousal * 40)
    
    def recommend_music(self, emotion, stage):
        """æ¨èéŸ³ä¹å‚æ•°"""
        bpm = self.calc_bpm(emotion.arousal)
        
        if emotion.valence > 0.2:
            key_type = "å¤§è°ƒ"
        elif emotion.valence < -0.2:
            key_type = "å°è°ƒ"
        else:
            key_type = "ä¸­æ€§è°ƒ"
        
        if stage == ISOStage.SYNC:
            instruments = ["å°æç´", "é’¢ç´"]
        elif stage == ISOStage.GUIDE:
            instruments = ["é•¿ç¬›", "å¼¦ä¹"]
        else:
            instruments = ["å¤§æç´", "ç«–ç´"]
        
        return {
            'bpm': round(bpm),
            'key': key_type,
            'instruments': instruments,
            'volume': 'soft' if emotion.arousal < 0 else 'moderate'
        }

def plot_emotion_trajectory(trajectory, current_emotion, target_emotion, 
                          output_dir="outputs/figures"):
    """ç»˜åˆ¶æƒ…ç»ªè½¨è¿¹å›¾"""
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    times = [p['time'] for p in trajectory]
    valences = [p['emotion'].valence for p in trajectory]
    arousals = [p['emotion'].arousal for p in trajectory]
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # æ—¶é—´åºåˆ—å›¾
    ax1.plot(times, valences, 'b-', linewidth=3, label='Valence')
    ax1.plot(times, arousals, 'r-', linewidth=3, label='Arousal')
    ax1.axhline(y=0, color='k', linestyle='--', alpha=0.3)
    
    # é˜¶æ®µåˆ†ç•Œ
    ax1.axvspan(0, 5, alpha=0.2, color='lightblue', label='Sync')
    ax1.axvspan(5, 15, alpha=0.2, color='lightgreen', label='Guide')
    ax1.axvspan(15, 20, alpha=0.2, color='lightcoral', label='Consolidate')
    
    ax1.set_xlabel('Time (minutes)')
    ax1.set_ylabel('Emotion Value')
    ax1.set_title('ISO Three-Stage Emotion Trajectory')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # VAç©ºé—´å›¾
    ax2.plot(valences, arousals, 'purple', linewidth=3)
    ax2.scatter(current_emotion.valence, current_emotion.arousal, 
                c='red', s=100, label='Start')
    ax2.scatter(target_emotion.valence, target_emotion.arousal, 
                c='green', s=100, label='Target')
    
    ax2.axhline(y=0, color='black', linewidth=1)
    ax2.axvline(x=0, color='black', linewidth=1)
    
    ax2.set_xlim(-1, 1)
    ax2.set_ylim(-1, 1)
    ax2.set_xlabel('Valence')
    ax2.set_ylabel('Arousal')
    ax2.set_title('Emotion Trajectory in V-A Space')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    output_path = Path(output_dir) / "emotion_trajectory.png"
    plt.savefig(output_path)
    plt.close()
    
    print(f"âœ… æƒ…ç»ªè½¨è¿¹å›¾å·²ä¿å­˜: {output_path}")

def plot_music_parameters(trajectory, music_model, output_dir="outputs/figures"):
    """ç»˜åˆ¶éŸ³ä¹å‚æ•°å˜åŒ–å›¾"""
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    times = [p['time'] for p in trajectory]
    bpms = [music_model.calc_bpm(p['emotion'].arousal) for p in trajectory]
    volumes = [50 + (p['emotion'].arousal * 25) for p in trajectory]
    
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
    
    # BPMå˜åŒ–
    ax1.plot(times, bpms, 'orange', linewidth=3)
    ax1.fill_between(times, bpms, alpha=0.3, color='orange')
    ax1.set_ylabel('BPM')
    ax1.set_title('Music Tempo Changes')
    ax1.grid(True, alpha=0.3)
    
    # éŸ³é‡å˜åŒ–
    ax2.plot(times, volumes, 'green', linewidth=3)
    ax2.fill_between(times, volumes, alpha=0.3, color='green')
    ax2.set_xlabel('Time (minutes)')
    ax2.set_ylabel('Volume')
    ax2.set_title('Volume Changes')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    output_path = Path(output_dir) / "music_parameters.png"
    plt.savefig(output_path)
    plt.close()
    
    print(f"âœ… éŸ³ä¹å‚æ•°å›¾å·²ä¿å­˜: {output_path}")
    
    return bpms, volumes

def main():
    """ä¸»å‡½æ•°"""
    print("ã€Šå¿ƒå¢ƒæµè½¬ã€‹ç†è®ºæ¨¡å‹æ¼”ç¤º")
    print("=" * 40)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
    
    # 1. åˆå§‹åŒ–æ¨¡å‹
    iso_model = ISOModel()
    music_model = MusicModel()
    print("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    # 2. å®šä¹‰æƒ…ç»ªçŠ¶æ€
    # å½“å‰ï¼šç„¦è™‘çŠ¶æ€ï¼ˆè´Ÿæ•ˆä»·ï¼Œé«˜å”¤é†’ï¼‰
    current_emotion = EmotionState(valence=-0.6, arousal=0.8)
    # ç›®æ ‡ï¼šå¹³é™çŠ¶æ€ï¼ˆæ­£æ•ˆä»·ï¼Œä½å”¤é†’ï¼‰  
    target_emotion = EmotionState(valence=0.5, arousal=-0.7)
    
    distance = current_emotion.distance_to(target_emotion)
    
    print(f"\nå½“å‰æƒ…ç»ª: V={current_emotion.valence:.2f}, A={current_emotion.arousal:.2f} (ç„¦è™‘)")
    print(f"ç›®æ ‡æƒ…ç»ª: V={target_emotion.valence:.2f}, A={target_emotion.arousal:.2f} (å¹³é™)")
    print(f"æƒ…ç»ªè·ç¦»: {distance:.3f}")
    
    # 3. ISOä¸‰é˜¶æ®µè§„åˆ’
    duration = 20.0  # 20åˆ†é’Ÿ
    stages = iso_model.plan_stages(current_emotion, target_emotion, duration)
    
    print(f"\nISOä¸‰é˜¶æ®µæ²»ç–—è§„åˆ’ (æ€»æ—¶é•¿: {duration}åˆ†é’Ÿ)")
    print("=" * 45)
    
    for i, stage in enumerate(stages, 1):
        emotion = stage['emotion']
        print(f"ç¬¬{i}é˜¶æ®µ: {stage['stage'].value}")
        print(f"  æ—¶é•¿: {stage['duration']:.1f}åˆ†é’Ÿ")
        print(f"  ç›®æ ‡: V={emotion.valence:.2f}, A={emotion.arousal:.2f}")
        print()
    
    # 4. ç”Ÿæˆè½¨è¿¹
    trajectory = iso_model.generate_trajectory(current_emotion, target_emotion, duration)
    print(f"è½¨è¿¹ç”Ÿæˆå®Œæˆ: {len(trajectory)}ä¸ªæ—¶é—´ç‚¹")
    
    # 5. å¯è§†åŒ–
    plot_emotion_trajectory(trajectory, current_emotion, target_emotion)
    bpms, volumes = plot_music_parameters(trajectory, music_model)
    
    # 6. éŸ³ä¹æ¨è
    print("\néŸ³ä¹æ²»ç–—æ–¹æ¡ˆ:")
    print("=" * 30)
    
    for stage in stages:
        music = music_model.recommend_music(stage['emotion'], stage['stage'])
        print(f"{stage['stage'].value}:")
        print(f"  BPM: {music['bpm']}")
        print(f"  è°ƒæ€§: {music['key']}")
        print(f"  ä¹å™¨: {', '.join(music['instruments'])}")
        print(f"  éŸ³é‡: {music['volume']}")
        print()
    
    # 7. éªŒè¯ç»“æœ
    print("\nğŸ”¬ ç†è®ºæ¨¡å‹éªŒè¯:")
    print("=" * 25)
    
    checks = [
        ("æƒ…ç»ªå˜åŒ–åˆç†", distance < 2.5),
        ("å¼•å¯¼é˜¶æ®µæœ€é•¿", stages[1]['duration'] >= stages[0]['duration']),
        ("BPMé€’å‡", bpms[0] > bpms[-1]),
        ("éŸ³é‡é€’å‡", volumes[0] > volumes[-1]),
        ("è¾¾åˆ°ç›®æ ‡", trajectory[-1]['emotion'].distance_to(target_emotion) < 0.1)
    ]
    
    passed = 0
    for name, result in checks:
        status = "âœ…" if result else "âŒ"
        print(f"{name}: {status}")
        if result:
            passed += 1
    
    score = passed / len(checks)
    print(f"\néªŒè¯å¾—åˆ†: {score:.1%} ({passed}/{len(checks)})")
    
    # 8. ä¿å­˜ç»“æœ
    results = {
        'timestamp': datetime.now().isoformat(),
        'scenario': {
            'current': {'valence': current_emotion.valence, 'arousal': current_emotion.arousal},
            'target': {'valence': target_emotion.valence, 'arousal': target_emotion.arousal},
            'distance': distance
        },
        'iso_planning': {
            'duration': duration,
            'stages': len(stages),
            'trajectory_points': len(trajectory)
        },
        'music_params': {
            'bpm_start': bpms[0],
            'bpm_end': bpms[-1],
            'bpm_reduction': bpms[0] - bpms[-1]
        },
        'validation': {
            'score': score,
            'checks_passed': passed,
            'total_checks': len(checks)
        }
    }
    
    output_dir = Path('outputs/validation')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    result_file = output_dir / 'theory_demo_results.json'
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {result_file}")
    
    # æ€»ç»“
    print("\n" + "=" * 50)
    print("ã€Šå¿ƒå¢ƒæµè½¬ã€‹ç†è®ºæ¨¡å‹æ¼”ç¤ºå®Œæˆ")
    print("=" * 50)
    print(f"âœ… ISOä¸‰é˜¶æ®µè§„åˆ’: {len(stages)}é˜¶æ®µ")
    print(f"âœ… æƒ…ç»ªè½¨è¿¹: {len(trajectory)}ä¸ªæ—¶é—´ç‚¹")
    print(f"âœ… éŸ³ä¹å‚æ•°: BPM {bpms[0]:.0f}â†’{bpms[-1]:.0f}")
    print(f"âœ… éªŒè¯å¾—åˆ†: {score:.1%}")
    print(f"\nğŸš€ ä¸‹ä¸€æ­¥: è¿è¡Œ 03_model_adapters_test.py")
    print(f"å®Œæˆæ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")

if __name__ == "__main__":
    main()