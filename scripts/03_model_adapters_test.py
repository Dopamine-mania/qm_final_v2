#!/usr/bin/env python3
"""
03 - æ¨¡å‹é€‚é…å™¨æµ‹è¯•
æµ‹è¯•å„ç§AIæ¨¡å‹çš„ç¡¬ä»¶é€‚é…èƒ½åŠ›å’Œå†…å­˜ç®¡ç†
"""

import torch
import psutil
import GPUtil
from datetime import datetime
from pathlib import Path
import json
import time
import gc
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

class HardwareMonitor:
    """ç¡¬ä»¶èµ„æºç›‘æ§å™¨"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.gpu_available = torch.cuda.is_available()
        
    def get_gpu_info(self) -> Dict:
        """è·å–GPUä¿¡æ¯"""
        if not self.gpu_available:
            return {"available": False}
        
        gpu = GPUtil.getGPUs()[0] if GPUtil.getGPUs() else None
        if gpu:
            return {
                "available": True,
                "name": gpu.name,
                "memory_total": gpu.memoryTotal,
                "memory_used": gpu.memoryUsed,
                "memory_free": gpu.memoryFree,
                "utilization": gpu.load * 100
            }
        return {"available": False}
    
    def get_memory_info(self) -> Dict:
        """è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯"""
        mem = psutil.virtual_memory()
        return {
            "total": mem.total / (1024**3),  # GB
            "used": mem.used / (1024**3),
            "available": mem.available / (1024**3),
            "percent": mem.percent
        }
    
    def get_current_usage(self) -> Dict:
        """è·å–å½“å‰èµ„æºä½¿ç”¨æƒ…å†µ"""
        return {
            "gpu": self.get_gpu_info(),
            "memory": self.get_memory_info(),
            "timestamp": datetime.now().isoformat()
        }

class ModelAdapter:
    """æ¨¡å‹é€‚é…å™¨åŸºç±»"""
    
    def __init__(self, model_name: str, model_size_gb: float):
        self.model_name = model_name
        self.model_size_gb = model_size_gb
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.monitor = HardwareMonitor()
        
    def check_compatibility(self) -> Tuple[bool, str]:
        """æ£€æŸ¥æ¨¡å‹å…¼å®¹æ€§"""
        gpu_info = self.monitor.get_gpu_info()
        mem_info = self.monitor.get_memory_info()
        
        # æ£€æŸ¥GPUå†…å­˜
        if gpu_info["available"]:
            gpu_free_gb = gpu_info["memory_free"] / 1024
            if gpu_free_gb >= self.model_size_gb:
                return True, f"GPUæ¨¡å¼å¯ç”¨ ({gpu_free_gb:.1f}GBå¯ç”¨)"
            
        # æ£€æŸ¥ç³»ç»Ÿå†…å­˜
        if mem_info["available"] >= self.model_size_gb * 2:  # éœ€è¦2å€å†…å­˜
            return True, f"CPUæ¨¡å¼å¯ç”¨ ({mem_info['available']:.1f}GBå¯ç”¨)"
        
        return False, "å†…å­˜ä¸è¶³"
    
    def simulate_loading(self) -> Dict:
        """æ¨¡æ‹Ÿæ¨¡å‹åŠ è½½"""
        print(f"\nğŸ“Š æµ‹è¯• {self.model_name}")
        print("-" * 40)
        
        # æ£€æŸ¥å…¼å®¹æ€§
        compatible, message = self.check_compatibility()
        print(f"å…¼å®¹æ€§: {'âœ…' if compatible else 'âŒ'} {message}")
        
        if not compatible:
            return {
                "model": self.model_name,
                "status": "failed",
                "reason": message
            }
        
        # æ¨¡æ‹ŸåŠ è½½è¿‡ç¨‹
        start_time = time.time()
        start_usage = self.monitor.get_current_usage()
        
        # åˆ›å»ºæ¨¡æ‹Ÿå¼ é‡
        try:
            # æ¨¡æ‹Ÿå ç”¨å†…å­˜ (ä½¿ç”¨è¾ƒå°çš„å¼ é‡é¿å…çœŸçš„è€—å°½å†…å­˜)
            tensor_size = min(int(self.model_size_gb * 100), 1000)  # MB
            dummy_tensors = []
            
            for i in range(5):
                tensor = torch.randn(tensor_size, 1024, 1024, device=self.device)
                dummy_tensors.append(tensor)
                time.sleep(0.1)
            
            # è·å–å³°å€¼ä½¿ç”¨
            peak_usage = self.monitor.get_current_usage()
            
            # æ¸…ç†
            del dummy_tensors
            if self.device.type == 'cuda':
                torch.cuda.empty_cache()
            gc.collect()
            
            load_time = time.time() - start_time
            
            print(f"âœ… åŠ è½½æˆåŠŸ")
            print(f"  è®¾å¤‡: {self.device}")
            print(f"  åŠ è½½æ—¶é—´: {load_time:.2f}ç§’")
            
            return {
                "model": self.model_name,
                "status": "success",
                "device": str(self.device),
                "load_time": load_time,
                "memory_usage": {
                    "start": start_usage,
                    "peak": peak_usage
                }
            }
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {str(e)}")
            return {
                "model": self.model_name,
                "status": "failed",
                "reason": str(e)
            }

class AdapterTester:
    """é€‚é…å™¨æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.monitor = HardwareMonitor()
        self.results = []
        
    def run_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ã€Šå¿ƒå¢ƒæµè½¬ã€‹æ¨¡å‹é€‚é…å™¨æµ‹è¯•")
        print("=" * 50)
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
        
        # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
        self.show_system_info()
        
        # å®šä¹‰æµ‹è¯•æ¨¡å‹
        test_models = [
            ("EmotionNet-Small", 0.5),    # 0.5GB
            ("EmotionNet-Base", 2.0),      # 2GB
            ("MusicGen-Small", 4.0),       # 4GB
            ("MusicGen-Large", 8.0),       # 8GB
            ("VideoGen-Base", 10.0),       # 10GB
            ("VideoGen-Pro", 20.0),        # 20GB
            ("MultiModal-Fusion", 40.0)    # 40GB
        ]
        
        # æµ‹è¯•æ¯ä¸ªæ¨¡å‹
        for model_name, size_gb in test_models:
            adapter = ModelAdapter(model_name, size_gb)
            result = adapter.simulate_loading()
            self.results.append(result)
            
            # æ¸…ç†å†…å­˜
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            time.sleep(1)
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
        
    def show_system_info(self):
        """æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯"""
        print("\nğŸ–¥ï¸ ç³»ç»Ÿé…ç½®")
        print("-" * 40)
        
        # GPUä¿¡æ¯
        gpu_info = self.monitor.get_gpu_info()
        if gpu_info["available"]:
            print(f"GPU: {gpu_info['name']}")
            print(f"æ˜¾å­˜: {gpu_info['memory_total']/1024:.1f}GB")
            print(f"å¯ç”¨: {gpu_info['memory_free']/1024:.1f}GB")
        else:
            print("GPU: ä¸å¯ç”¨")
        
        # å†…å­˜ä¿¡æ¯
        mem_info = self.monitor.get_memory_info()
        print(f"å†…å­˜: {mem_info['total']:.1f}GB")
        print(f"å¯ç”¨: {mem_info['available']:.1f}GB")
        
    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\nğŸ“Š æµ‹è¯•æŠ¥å‘Š")
        print("=" * 50)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in self.results if r['status'] == 'success')
        total_count = len(self.results)
        
        print(f"æ€»æµ‹è¯•: {total_count}")
        print(f"æˆåŠŸ: {success_count}")
        print(f"å¤±è´¥: {total_count - success_count}")
        print(f"æˆåŠŸç‡: {success_count/total_count*100:.1f}%")
        
        # æ¨èé…ç½®
        print("\nğŸ’¡ æ¨èé…ç½®")
        print("-" * 40)
        
        gpu_info = self.monitor.get_gpu_info()
        if gpu_info["available"]:
            gpu_memory_gb = gpu_info['memory_total'] / 1024
            
            if gpu_memory_gb >= 40:
                print("âœ… æ——èˆ°é…ç½®: å¯è¿è¡Œæ‰€æœ‰æ¨¡å‹")
                print("  - æ”¯æŒMultiModal-Fusion")
                print("  - æ”¯æŒå¹¶è¡Œæ¨ç†")
            elif gpu_memory_gb >= 20:
                print("âœ… ä¸“ä¸šé…ç½®: å¯è¿è¡Œå¤§éƒ¨åˆ†æ¨¡å‹")
                print("  - æ”¯æŒVideoGen-Base")
                print("  - éœ€è¦æ˜¾å­˜ä¼˜åŒ–")
            elif gpu_memory_gb >= 8:
                print("âš ï¸ æ ‡å‡†é…ç½®: å¯è¿è¡ŒåŸºç¡€æ¨¡å‹")
                print("  - æ”¯æŒMusicGen-Small")
                print("  - å»ºè®®ä½¿ç”¨é‡åŒ–")
            else:
                print("âŒ å…¥é—¨é…ç½®: ä»…æ”¯æŒå°æ¨¡å‹")
                print("  - ä»…EmotionNetç³»åˆ—")
                print("  - å¼ºçƒˆå»ºè®®å‡çº§")
        
        # ä¿å­˜ç»“æœ
        self.save_results()
        
    def save_results(self):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        output_dir = Path("outputs/validation")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "system_info": {
                "gpu": self.monitor.get_gpu_info(),
                "memory": self.monitor.get_memory_info()
            },
            "test_results": self.results,
            "summary": {
                "total": len(self.results),
                "success": sum(1 for r in self.results if r['status'] == 'success'),
                "failed": sum(1 for r in self.results if r['status'] == 'failed')
            }
        }
        
        output_file = output_dir / "adapter_test_results.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        tester = AdapterTester()
        tester.run_tests()
        
        print("\n" + "=" * 50)
        print("æ¨¡å‹é€‚é…å™¨æµ‹è¯•å®Œæˆ")
        print("=" * 50)
        print(f"å®Œæˆæ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
        print("\nğŸš€ ä¸‹ä¸€æ­¥: è¿è¡Œ 04_therapy_session_demo.py")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()