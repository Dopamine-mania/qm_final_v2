#!/usr/bin/env python3
"""
ã€Šå¿ƒå¢ƒæµè½¬ã€‹ç¡çœ æ²»ç–—ç³»ç»Ÿ - äº¤äº’å¼æ¼”ç¤ºåº”ç”¨
ç”¨æˆ·è¾“å…¥æ–‡å­—/è¯­éŸ³ â†’ æƒ…ç»ªè¯†åˆ« â†’ ç”Ÿæˆä¸‰é˜¶æ®µéŸ³è§†é¢‘ â†’ å¼•å¯¼å…¥ç¡
"""

import os
import sys
import time
import json
import numpy as np
from datetime import datetime
from pathlib import Path
import matplotlib.pyplot as plt
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional

# æ·»åŠ scriptsç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'scripts'))

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
import importlib.util
import sys

def import_from_file(file_path, module_name):
    spec = importlib.util.spec_from_file_location(module_name, file_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module

# åŠ¨æ€å¯¼å…¥scriptsæ¨¡å—
theory_module = import_from_file("scripts/02_theory_models_demo.py", "theory_models")
music_module = import_from_file("scripts/06_music_generation_workshop.py", "music_workshop")
video_module = import_from_file("scripts/07_video_generation_workshop.py", "video_workshop")

ISOModel = theory_module.ISOModel
EmotionState = theory_module.EmotionState
MusicModel = theory_module.MusicModel
SleepMusicGenerator = music_module.SleepMusicGenerator
SleepVideoGenerator = video_module.SleepVideoGenerator

@dataclass
class TherapySession:
    """æ²»ç–—ä¼šè¯æ•°æ®"""
    user_input: str
    detected_emotion: EmotionState
    iso_stages: List[Dict]
    music_file: str
    video_files: List[str]
    start_time: datetime
    
class MoodFlowApp:
    """å¿ƒå¢ƒæµè½¬åº”ç”¨ä¸»ç±»"""
    
    def __init__(self):
        print("\n" + "="*60)
        print("ğŸŒ™ ã€Šå¿ƒå¢ƒæµè½¬ã€‹ç¡çœ æ²»ç–—ç³»ç»Ÿ å¯åŠ¨ä¸­...")
        print("="*60)
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.iso_model = ISOModel()
        self.music_model = MusicModel()
        self.music_generator = SleepMusicGenerator(sample_rate=44100)
        self.video_generator = SleepVideoGenerator(width=960, height=540, fps=24)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("outputs/demo_sessions")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æƒ…ç»ªå…³é”®è¯æ˜ å°„
        self.emotion_keywords = {
            "ç„¦è™‘": ["ç„¦è™‘", "ç´§å¼ ", "æ‹…å¿ƒ", "å®³æ€•", "ä¸å®‰", "ææ…Œ", "å¿§è™‘"],
            "å‹åŠ›": ["å‹åŠ›", "ç´¯", "ç–²æƒ«", "å·¥ä½œ", "å¿™", "çƒ¦", "å¤´ç–¼"],
            "æŠ‘éƒ": ["éš¾è¿‡", "æ‚²ä¼¤", "å¤±æœ›", "æ²®ä¸§", "ä½è½", "å“­", "ç»æœ›"],
            "æ„¤æ€’": ["ç”Ÿæ°”", "æ„¤æ€’", "æ¼ç«", "è®¨åŒ", "çƒ¦èº", "æ°”", "æ¨"],
            "å…´å¥‹": ["å…´å¥‹", "æ¿€åŠ¨", "å¼€å¿ƒ", "é«˜å…´", "åˆºæ¿€"],
            "å¹³é™": ["å¹³é™", "æ”¾æ¾", "èˆ’é€‚", "å®‰é™", "å®é™"]
        }
        
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼\n")
    
    def safe_progress_update(self, progress_callback, value, desc=""):
        """Safely update progress bar to avoid Gradio version compatibility issues"""
        try:
            if progress_callback is not None:
                progress_callback(value, desc=desc)
        except Exception as e:
            print(f"Progress update warning: {str(e)}")
            pass
    
    def analyze_emotion_from_text(self, text: str) -> EmotionState:
        """ä»æ–‡æœ¬åˆ†ææƒ…ç»ªçŠ¶æ€"""
        print("ğŸ” åˆ†ææƒ…ç»ªçŠ¶æ€...")
        
        # è®¡ç®—å„æƒ…ç»ªå¾—åˆ†
        emotion_scores = {}
        for emotion, keywords in self.emotion_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text)
            if score > 0:
                emotion_scores[emotion] = score
        
        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°å…³é”®è¯ï¼Œé»˜è®¤ä¸ºè½»åº¦ç„¦è™‘
        if not emotion_scores:
            emotion_scores["ç„¦è™‘"] = 1
        
        # æ‰¾å‡ºä¸»è¦æƒ…ç»ª
        primary_emotion = max(emotion_scores.items(), key=lambda x: x[1])[0]
        
        # æ˜ å°„åˆ°V-Aç©ºé—´
        va_mapping = {
            "ç„¦è™‘": (-0.6, 0.8),    # è´Ÿé¢é«˜å”¤é†’
            "å‹åŠ›": (-0.5, 0.6),    # è´Ÿé¢ä¸­é«˜å”¤é†’
            "æŠ‘éƒ": (-0.8, -0.3),   # è´Ÿé¢ä½å”¤é†’
            "æ„¤æ€’": (-0.7, 0.9),    # è´Ÿé¢é«˜å”¤é†’
            "å…´å¥‹": (0.6, 0.8),     # æ­£é¢é«˜å”¤é†’
            "å¹³é™": (0.5, -0.5)     # æ­£é¢ä½å”¤é†’
        }
        
        valence, arousal = va_mapping.get(primary_emotion, (-0.5, 0.5))
        
        # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
        print(f"  æ£€æµ‹åˆ°çš„ä¸»è¦æƒ…ç»ª: {primary_emotion}")
        print(f"  æƒ…ç»ªå‚æ•°: Valence={valence:.2f}, Arousal={arousal:.2f}")
        
        return EmotionState(valence=valence, arousal=arousal)
    
    def plan_therapy_stages(self, current_emotion: EmotionState, duration: int = 20) -> List[Dict]:
        """è§„åˆ’ä¸‰é˜¶æ®µæ²»ç–—"""
        print("\nğŸ“‹ è§„åˆ’æ²»ç–—æ–¹æ¡ˆ...")
        
        # ç›®æ ‡æƒ…ç»ªï¼šå¹³é™å…¥ç¡çŠ¶æ€
        target_emotion = EmotionState(valence=0.3, arousal=-0.8)
        
        # ä½¿ç”¨ISOæ¨¡å‹è§„åˆ’
        stages = self.iso_model.plan_stages(current_emotion, target_emotion, duration)
        
        # éªŒè¯é˜¶æ®µè§„åˆ’ç»“æœ
        if not stages:
            raise ValueError("ISOæ¨¡å‹æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„æ²»ç–—é˜¶æ®µ")
        
        print(f"  æ²»ç–—æ€»æ—¶é•¿: {duration} åˆ†é’Ÿ")
        print(f"  ç”Ÿæˆäº† {len(stages)} ä¸ªæ²»ç–—é˜¶æ®µ")
        for i, stage in enumerate(stages, 1):
            print(f"  ç¬¬{i}é˜¶æ®µ - {stage['stage'].value}: {stage['duration']:.0f}åˆ†é’Ÿ")
        
        return stages
    
    def generate_stage_music(self, stages: List[Dict], session_name: str) -> str:
        """ä¸ºå„é˜¶æ®µç”ŸæˆéŸ³ä¹"""
        print("\nğŸµ ç”Ÿæˆæ²»ç–—éŸ³ä¹...")
        
        if not stages:
            raise ValueError("æ— æ³•ä¸ºç©ºçš„æ²»ç–—é˜¶æ®µç”ŸæˆéŸ³ä¹")
        
        # éŸ³é¢‘å‚æ•°
        total_duration = sum(stage['duration'] for stage in stages)
        sample_rate = self.music_generator.sample_rate
        total_samples = int(total_duration * 60 * sample_rate)
        
        # åˆ›å»ºå®Œæ•´éŸ³è½¨
        full_track = np.zeros(total_samples)
        current_pos = 0
        
        for i, stage in enumerate(stages):
            stage_duration = stage['duration'] * 60  # è½¬æ¢ä¸ºç§’
            stage_samples = int(stage_duration * sample_rate)
            
            # è·å–é˜¶æ®µæƒ…ç»ª
            emotion = stage['emotion']
            
            # è®¡ç®—éŸ³ä¹å‚æ•°
            bpm = self.music_model.calc_bpm(emotion.arousal)
            
            # é€‰æ‹©è°ƒæ€§
            if emotion.valence > 0:
                key = 'C'
                mode = 'major'
            else:
                key = 'A'
                mode = 'minor'
            
            print(f"  ç¬¬{i+1}é˜¶æ®µ: BPM={bpm:.0f}, è°ƒæ€§={key} {mode}")
            
            # ç”Ÿæˆè¯¥é˜¶æ®µçš„éŸ³ä¹
            stage_track = self._generate_simple_music(
                duration_seconds=stage_duration,
                bpm=bpm,
                key=key,
                stage_index=i
            )
            
            # æ·»åŠ åˆ°å®Œæ•´éŸ³è½¨
            end_pos = min(current_pos + len(stage_track), total_samples)
            full_track[current_pos:end_pos] = stage_track[:end_pos-current_pos]
            current_pos = end_pos
        
        # ä¿å­˜éŸ³é¢‘
        audio_file = self.output_dir / f"{session_name}_therapy_music.wav"
        self.music_generator.save_audio(full_track, str(audio_file))
        
        print(f"âœ… éŸ³ä¹ç”Ÿæˆå®Œæˆ: {audio_file.name}")
        
        return str(audio_file)
    
    def _generate_simple_music(self, duration_seconds: float, bpm: float, 
                              key: str, stage_index: int) -> np.ndarray:
        """ç”Ÿæˆç®€å•çš„é˜¶æ®µéŸ³ä¹"""
        sample_rate = self.music_generator.sample_rate
        samples = int(duration_seconds * sample_rate)
        track = np.zeros(samples)
        
        # åŸºç¡€é¢‘ç‡ï¼ˆæ ¹æ®è°ƒæ€§ï¼‰
        base_frequencies = {
            'C': 261.63,  # C4
            'A': 440.00   # A4
        }
        base_freq = base_frequencies.get(key, 261.63)
        
        # æ ¹æ®é˜¶æ®µè°ƒæ•´éŸ³è‰²
        if stage_index == 0:  # åŒæ­¥åŒ–
            # ä½¿ç”¨å½“å‰æƒ…ç»ªçš„èŠ‚å¥ï¼Œè¾ƒä¸ºæ´»è·ƒ
            frequencies = [base_freq, base_freq * 1.5, base_freq * 2]
            amplitudes = [0.5, 0.3, 0.2]
        elif stage_index == 1:  # å¼•å¯¼åŒ–
            # é€æ¸æ”¾ç¼“ï¼Œæ·»åŠ å’Œè°éŸ³
            frequencies = [base_freq * 0.5, base_freq, base_freq * 1.5]
            amplitudes = [0.4, 0.4, 0.2]
        else:  # å·©å›ºåŒ–
            # ä½é¢‘ä¸ºä¸»ï¼Œè¥é€ ç¡çœ æ°›å›´
            frequencies = [base_freq * 0.25, base_freq * 0.5, base_freq]
            amplitudes = [0.5, 0.3, 0.2]
        
        # ç”Ÿæˆå’Œå¼¦
        beat_duration = 60.0 / bpm
        beat_samples = int(beat_duration * sample_rate)
        
        for beat_idx in range(int(duration_seconds / beat_duration)):
            start_idx = beat_idx * beat_samples
            end_idx = min(start_idx + beat_samples, samples)
            
            # ç”ŸæˆéŸ³ç¬¦
            t = np.linspace(0, beat_duration, end_idx - start_idx)
            note = np.zeros_like(t)
            
            for freq, amp in zip(frequencies, amplitudes):
                # æ·»åŠ è½»å¾®çš„é¢‘ç‡å˜åŒ–ï¼Œä½¿éŸ³è‰²æ›´è‡ªç„¶
                freq_mod = freq * (1 + 0.01 * np.sin(2 * np.pi * 0.1 * beat_idx))
                note += amp * np.sin(2 * np.pi * freq_mod * t)
            
            # åº”ç”¨åŒ…ç»œ
            envelope = np.exp(-t * 2) * (1 - stage_index * 0.2)  # é€é˜¶æ®µå‡å¼±
            note *= envelope
            
            # æ·»åŠ åˆ°éŸ³è½¨
            track[start_idx:end_idx] += note
        
        # åº”ç”¨æ•´ä½“æ·¡å…¥æ·¡å‡º
        fade_samples = int(5 * sample_rate)  # 5ç§’æ·¡å…¥æ·¡å‡º
        if fade_samples < len(track):
            track[:fade_samples] *= np.linspace(0, 1, fade_samples)
            track[-fade_samples:] *= np.linspace(1, 0, fade_samples)
        
        return track
    
    def generate_stage_videos(self, stages: List[Dict], session_name: str, create_full_videos: bool = False) -> List[str]:
        """ä¸ºå„é˜¶æ®µç”Ÿæˆè§†é¢‘"""
        print("\nğŸ¬ ç”Ÿæˆæ²»ç–—è§†é¢‘...")
        
        video_files = []
        
        # è§†è§‰æ¨¡å¼æ˜ å°„
        stage_patterns = {
            0: ("breathing", "ocean"),      # åŒæ­¥åŒ–ï¼šå‘¼å¸å¼•å¯¼
            1: ("gradient", "sunset"),      # å¼•å¯¼åŒ–ï¼šæ¸å˜è¿‡æ¸¡
            2: ("waves", "lavender")        # å·©å›ºåŒ–ï¼šæŸ”å’Œæ³¢æµª
        }
        
        for i, stage in enumerate(stages):
            pattern, palette = stage_patterns.get(i, ("gradient", "ocean"))
            
            print(f"  ç¬¬{i+1}é˜¶æ®µ: {pattern} - {palette}")
            
            stage_dir = self.output_dir / f"{session_name}_stage_{i+1}"
            stage_dir.mkdir(exist_ok=True)
            
            if create_full_videos:
                # ç”Ÿæˆå®Œæ•´è§†é¢‘
                video_file = stage_dir / f"stage_{i+1}_video.mp4"
                frames = self.video_generator.generate_video(
                    duration_seconds=stage['duration'] * 60,
                    pattern_type=pattern,
                    color_palette=palette,
                    output_path=str(video_file),
                    preview_only=False
                )
                video_files.append(str(video_file))
                print(f"  âœ… ä¿å­˜è§†é¢‘: {video_file.name}")
            else:
                # ç”Ÿæˆé¢„è§ˆå¸§
                frames = self.video_generator.generate_video(
                    duration_seconds=stage['duration'] * 60,
                    pattern_type=pattern,
                    color_palette=palette,
                    output_path=None,
                    preview_only=True
                )
                
                # ä¿å­˜ç¬¬ä¸€å¸§ä½œä¸ºé¢„è§ˆ
                if frames:
                    preview_file = stage_dir / "preview.png"
                    # ç¡®ä¿ç›®å½•å­˜åœ¨
                    preview_file.parent.mkdir(parents=True, exist_ok=True)
                    # ä¿å­˜å›¾ç‰‡
                    plt.imsave(str(preview_file), frames[0])
                    # éªŒè¯æ–‡ä»¶æ˜¯å¦æˆåŠŸä¿å­˜
                    if preview_file.exists():
                        video_files.append(str(preview_file))
                        print(f"  âœ… ä¿å­˜é¢„è§ˆ: {preview_file.name} (è·¯å¾„: {preview_file})")
                    else:
                        print(f"  âŒ é¢„è§ˆä¿å­˜å¤±è´¥: {preview_file}")
        
        return video_files
    
    def create_visualization(self, session: TherapySession) -> str:
        """åˆ›å»ºä¼šè¯å¯è§†åŒ–"""
        print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š...")
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # 1. æƒ…ç»ªè½¨è¿¹
        ax = axes[0, 0]
        stages_data = []
        for stage in session.iso_stages:
            emotion = stage['emotion']
            stages_data.append({
                'name': stage['stage'].value,
                'valence': emotion.valence,
                'arousal': emotion.arousal,
                'duration': stage['duration']
            })
        
        # ç»˜åˆ¶V-Aç©ºé—´è½¨è¿¹
        valences = [session.detected_emotion.valence] + [s['valence'] for s in stages_data]
        arousals = [session.detected_emotion.arousal] + [s['arousal'] for s in stages_data]
        
        ax.plot(valences, arousals, 'o-', linewidth=2, markersize=8)
        ax.scatter(valences[0], arousals[0], c='red', s=100, label='Initial Emotion')
        ax.scatter(valences[-1], arousals[-1], c='green', s=100, label='Target Emotion')
        
        # æ·»åŠ é˜¶æ®µæ ‡æ³¨
        for i, stage in enumerate(stages_data):
            ax.annotate(stage['name'], (valences[i+1], arousals[i+1]), 
                       xytext=(5, 5), textcoords='offset points')
        
        ax.set_xlim(-1, 1)
        ax.set_ylim(-1, 1)
        ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)
        ax.axvline(x=0, color='k', linestyle='--', alpha=0.3)
        ax.set_xlabel('Valence')
        ax.set_ylabel('Arousal')
        ax.set_title('Emotion Trajectory Planning')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 2. é˜¶æ®µæ—¶é—´åˆ†é…
        ax = axes[0, 1]
        stage_names = [s['name'] for s in stages_data]
        stage_durations = [s['duration'] for s in stages_data]
        colors = ['#ff9999', '#66b3ff', '#99ff99']
        
        # å°†stage_namesè½¬æ¢ä¸ºè‹±æ–‡
        stage_names_en = []
        for name in stage_names:
            if 'åŒæ­¥åŒ–' in name:
                stage_names_en.append('Synchronization')
            elif 'å¼•å¯¼åŒ–' in name:
                stage_names_en.append('Guidance')
            elif 'å·©å›ºåŒ–' in name:
                stage_names_en.append('Consolidation')
            else:
                stage_names_en.append(name)
        
        ax.pie(stage_durations, labels=stage_names_en, colors=colors, autopct='%1.0f%%')
        ax.set_title('Therapy Stage Duration Distribution')
        
        # 3. BPMå˜åŒ–æ›²çº¿
        ax = axes[1, 0]
        time_points = []
        bpm_values = []
        current_time = 0
        
        for stage in session.iso_stages:
            emotion = stage['emotion']
            bpm = self.music_model.calc_bpm(emotion.arousal)
            time_points.extend([current_time, current_time + stage['duration']])
            bpm_values.extend([bpm, bpm])
            current_time += stage['duration']
        
        ax.plot(time_points, bpm_values, 'b-', linewidth=2)
        ax.fill_between(time_points, bpm_values, alpha=0.3)
        ax.set_xlabel('Time (minutes)')
        ax.set_ylabel('BPM')
        ax.set_title('Music Rhythm Changes')
        ax.grid(True, alpha=0.3)
        
        # 4. æ²»ç–—ä¿¡æ¯
        ax = axes[1, 1]
        ax.axis('off')
        
        info_text = f"""
ç”¨æˆ·è¾“å…¥: {session.user_input[:30]}...

æ£€æµ‹æƒ…ç»ª: V={session.detected_emotion.valence:.2f}, A={session.detected_emotion.arousal:.2f}

æ²»ç–—æ–¹æ¡ˆ:
â€¢ æ€»æ—¶é•¿: {sum(s['duration'] for s in stages_data):.0f} åˆ†é’Ÿ
â€¢ éŸ³ä¹æ–‡ä»¶: {Path(session.music_file).name}
â€¢ è§†é¢‘é¢„è§ˆ: {len(session.video_files)} ä¸ªé˜¶æ®µ

ç”Ÿæˆæ—¶é—´: {session.start_time.strftime('%Y-%m-%d %H:%M:%S')}
        """
        
        ax.text(0.1, 0.9, "Therapy Session Info", fontsize=14, fontweight='bold', 
                transform=ax.transAxes)
        ax.text(0.1, 0.1, info_text, fontsize=10, transform=ax.transAxes,
                verticalalignment='bottom', fontfamily='monospace')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        report_file = self.output_dir / f"{Path(session.music_file).stem}_report.png"
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
        plt.savefig(report_file, dpi=150, bbox_inches='tight')
        plt.close()
        
        # éªŒè¯æ–‡ä»¶æ˜¯å¦æˆåŠŸä¿å­˜
        if report_file.exists():
            print(f"âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {report_file.name} (è·¯å¾„: {report_file})")
        else:
            print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {report_file}")
        
        return str(report_file)
    
    def run_therapy_session(self, user_input: str, duration: int = 20, create_full_videos: bool = False, progress_callback=None) -> TherapySession:
        """è¿è¡Œå®Œæ•´çš„æ²»ç–—ä¼šè¯"""
        start_time = datetime.now()
        session_name = f"session_{start_time.strftime('%Y%m%d_%H%M%S')}"
        
        print(f"\n{'='*60}")
        print(f"ğŸŒ™ å¼€å§‹æ²»ç–—ä¼šè¯: {session_name}")
        print(f"{'='*60}")
        print(f"å‚æ•°: duration={duration}, create_full_videos={create_full_videos}")
        
        # 1. æƒ…ç»ªåˆ†æ
        self.safe_progress_update(progress_callback, 0.2, "Analyzing emotions...")
        detected_emotion = self.analyze_emotion_from_text(user_input)
        
        # 2. è§„åˆ’æ²»ç–—é˜¶æ®µ
        self.safe_progress_update(progress_callback, 0.3, "Planning therapy stages...")
        iso_stages = self.plan_therapy_stages(detected_emotion, duration)
        
        # 3. ç”ŸæˆéŸ³ä¹
        self.safe_progress_update(progress_callback, 0.4, "Generating therapy music...")
        music_file = self.generate_stage_music(iso_stages, session_name)
        
        # 4. ç”Ÿæˆè§†é¢‘
        self.safe_progress_update(progress_callback, 0.7, "Creating visual guidance...")
        video_files = self.generate_stage_videos(iso_stages, session_name, create_full_videos)
        
        # åˆ›å»ºä¼šè¯å¯¹è±¡
        session = TherapySession(
            user_input=user_input,
            detected_emotion=detected_emotion,
            iso_stages=iso_stages,
            music_file=music_file,
            video_files=video_files,
            start_time=start_time
        )
        
        # 5. ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
        report_file = self.create_visualization(session)
        
        # 6. ä¿å­˜ä¼šè¯æ•°æ®
        session_data = {
            "session_name": session_name,
            "user_input": user_input,
            "detected_emotion": {
                "valence": detected_emotion.valence,
                "arousal": detected_emotion.arousal
            },
            "stages": [
                {
                    "name": stage["stage"].value,
                    "duration": stage["duration"],
                    "emotion": {
                        "valence": stage["emotion"].valence,
                        "arousal": stage["emotion"].arousal
                    }
                }
                for stage in iso_stages
            ],
            "outputs": {
                "music": music_file,
                "videos": video_files,
                "report": report_file
            },
            "timestamp": start_time.isoformat()
        }
        
        session_file = self.output_dir / f"{session_name}_data.json"
        with open(session_file, 'w', encoding='utf-8') as f:
            json.dump(session_data, f, indent=2, ensure_ascii=False)
        
        print(f"\n{'='*60}")
        print("âœ… æ²»ç–—ä¼šè¯å®Œæˆï¼")
        print(f"{'='*60}")
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"  â€¢ éŸ³ä¹: {Path(music_file).name}")
        print(f"  â€¢ è§†é¢‘: {len(video_files)} ä¸ªé˜¶æ®µé¢„è§ˆ")
        print(f"  â€¢ æŠ¥å‘Š: {Path(report_file).name}")
        print(f"  â€¢ æ•°æ®: {session_file.name}")
        
        return session

def main():
    """ä¸»ç¨‹åº"""
    # åˆ›å»ºåº”ç”¨å®ä¾‹
    app = MoodFlowApp()
    
    # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
    print("\n" + "="*60)
    print("æ¬¢è¿ä½¿ç”¨ã€Šå¿ƒå¢ƒæµè½¬ã€‹ç¡çœ æ²»ç–—ç³»ç»Ÿ")
    print("="*60)
    print("\nè¯·æè¿°æ‚¨ç°åœ¨çš„æ„Ÿå—ï¼Œç³»ç»Ÿå°†ä¸ºæ‚¨ç”Ÿæˆä¸ªæ€§åŒ–çš„ç¡çœ æ²»ç–—æ–¹æ¡ˆã€‚")
    print("(è¾“å…¥ 'quit' é€€å‡º)\n")
    
    # æä¾›ç¤ºä¾‹
    examples = [
        "ä»Šå¤©å·¥ä½œå‹åŠ›å¾ˆå¤§ï¼Œèººåœ¨åºŠä¸Šç¿»æ¥è¦†å»ç¡ä¸ç€ï¼Œæ€»æ˜¯æƒ³ç€æ˜å¤©çš„ä¼šè®®",
        "æœ€è¿‘æ€»æ˜¯æ„Ÿåˆ°ç„¦è™‘ï¼Œæ™šä¸Šå¾ˆéš¾å…¥ç¡ï¼Œå³ä½¿ç¡ç€äº†ä¹Ÿå®¹æ˜“é†’",
        "å¿ƒæƒ…æœ‰äº›ä½è½ï¼Œæ„Ÿè§‰å¾ˆç–²æƒ«ä½†å°±æ˜¯ç¡ä¸ç€",
        "æœ‰ç‚¹å…´å¥‹ç¡ä¸ç€ï¼Œè„‘å­é‡Œæƒ³ç€å¾ˆå¤šäº‹æƒ…"
    ]
    
    print("ç¤ºä¾‹è¾“å…¥:")
    for i, example in enumerate(examples, 1):
        print(f"  {i}. {example}")
    
    while True:
        print("\n" + "-"*60)
        user_input = input("è¯·è¾“å…¥æ‚¨çš„æ„Ÿå— (æˆ–è¾“å…¥ç¤ºä¾‹ç¼–å· 1-4): ").strip()
        
        # å¤„ç†é€€å‡º
        if user_input.lower() == 'quit':
            print("\næ„Ÿè°¢ä½¿ç”¨ï¼Œç¥æ‚¨å¥½æ¢¦ï¼æ™šå®‰~ ğŸŒ™")
            break
        
        # å¤„ç†ç¤ºä¾‹é€‰æ‹©
        if user_input.isdigit() and 1 <= int(user_input) <= len(examples):
            user_input = examples[int(user_input) - 1]
            print(f"\næ‚¨é€‰æ‹©äº†: {user_input}")
        
        # æ£€æŸ¥è¾“å…¥
        if len(user_input) < 5:
            print("âŒ è¯·è¾“å…¥æ›´è¯¦ç»†çš„æè¿°ï¼ˆè‡³å°‘5ä¸ªå­—ï¼‰")
            continue
        
        try:
            # è¿è¡Œæ²»ç–—ä¼šè¯
            session = app.run_therapy_session(user_input)
            
            # è¯¢é—®æ˜¯å¦ç»§ç»­
            print("\næ˜¯å¦éœ€è¦ç”Ÿæˆæ–°çš„æ²»ç–—æ–¹æ¡ˆï¼Ÿ(y/n): ", end='')
            if input().strip().lower() != 'y':
                print("\næ„Ÿè°¢ä½¿ç”¨ï¼Œç¥æ‚¨å¥½æ¢¦ï¼æ™šå®‰~ ğŸŒ™")
                break
                
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()